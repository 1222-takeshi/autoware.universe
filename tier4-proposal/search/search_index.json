{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"autoware.universe # This is one of the prototype repositories for Autoware Core/Universe that AWF agreed to create in the TSC meeting on 2021/11/17 . Please see autowarefoundation/autoware for more details.","title":"autoware.universe"},{"location":"#autowareuniverse","text":"This is one of the prototype repositories for Autoware Core/Universe that AWF agreed to create in the TSC meeting on 2021/11/17 . Please see autowarefoundation/autoware for more details.","title":"autoware.universe"},{"location":"common/autoware_auto_perception_rviz_plugin/","text":"autoware_auto_perception_plugin # Purpose # It is an rviz plugin for visualizing the result from perception module. This package is based on the implementation of the rviz plugin developed by Autoware.Auto. See Autoware.Auto design documentation for the original design philosophy. [1] Input Types / Visualization Results # DetectedObjects # Input Types # Name Type Description autoware_auto_perception_msgs::msg::DetectedObjects detection result array Visualization Result # TrackedObjects # Input Types # Name Type Description autoware_auto_perception_msgs::msg::TrackedObjects tracking result array Visualization Result # Overwrite tracking results with detection results. PredictedObjects # Input Types # Name Type Description autoware_auto_perception_msgs::msg::PredictedObjects prediction result array Visualization Result # Overwrite prediction results with tracking results. References/External links # [1] https://gitlab.com/autowarefoundation/autoware.auto/AutowareAuto/-/tree/master/src/tools/visualization/autoware_rviz_plugins Future extensions / Unimplemented parts #","title":"autoware_auto_perception_plugin"},{"location":"common/autoware_auto_perception_rviz_plugin/#autoware_auto_perception_plugin","text":"","title":"autoware_auto_perception_plugin"},{"location":"common/autoware_auto_perception_rviz_plugin/#purpose","text":"It is an rviz plugin for visualizing the result from perception module. This package is based on the implementation of the rviz plugin developed by Autoware.Auto. See Autoware.Auto design documentation for the original design philosophy. [1]","title":"Purpose"},{"location":"common/autoware_auto_perception_rviz_plugin/#input-types-visualization-results","text":"","title":"Input Types / Visualization Results"},{"location":"common/autoware_auto_perception_rviz_plugin/#detectedobjects","text":"","title":"DetectedObjects"},{"location":"common/autoware_auto_perception_rviz_plugin/#input-types","text":"Name Type Description autoware_auto_perception_msgs::msg::DetectedObjects detection result array","title":"Input Types"},{"location":"common/autoware_auto_perception_rviz_plugin/#visualization-result","text":"","title":"Visualization Result"},{"location":"common/autoware_auto_perception_rviz_plugin/#trackedobjects","text":"","title":"TrackedObjects"},{"location":"common/autoware_auto_perception_rviz_plugin/#input-types_1","text":"Name Type Description autoware_auto_perception_msgs::msg::TrackedObjects tracking result array","title":"Input Types"},{"location":"common/autoware_auto_perception_rviz_plugin/#visualization-result_1","text":"Overwrite tracking results with detection results.","title":"Visualization Result"},{"location":"common/autoware_auto_perception_rviz_plugin/#predictedobjects","text":"","title":"PredictedObjects"},{"location":"common/autoware_auto_perception_rviz_plugin/#input-types_2","text":"Name Type Description autoware_auto_perception_msgs::msg::PredictedObjects prediction result array","title":"Input Types"},{"location":"common/autoware_auto_perception_rviz_plugin/#visualization-result_2","text":"Overwrite prediction results with tracking results.","title":"Visualization Result"},{"location":"common/autoware_auto_perception_rviz_plugin/#referencesexternal-links","text":"[1] https://gitlab.com/autowarefoundation/autoware.auto/AutowareAuto/-/tree/master/src/tools/visualization/autoware_rviz_plugins","title":"References/External links"},{"location":"common/autoware_auto_perception_rviz_plugin/#future-extensions-unimplemented-parts","text":"","title":"Future extensions / Unimplemented parts"},{"location":"common/autoware_datetime_rviz_plugin/","text":"autoware_datetime_rviz_plugin # Purpose # This plugin displays the ROS Time and Wall Time in rviz. Assumptions / Known limits # TBD. Usage # Start rviz and select panels/Add new panel. Select autoware_datetime_rviz_plugin/AutowareDateTimePanel and press OK.","title":"autoware_datetime_rviz_plugin"},{"location":"common/autoware_datetime_rviz_plugin/#autoware_datetime_rviz_plugin","text":"","title":"autoware_datetime_rviz_plugin"},{"location":"common/autoware_datetime_rviz_plugin/#purpose","text":"This plugin displays the ROS Time and Wall Time in rviz.","title":"Purpose"},{"location":"common/autoware_datetime_rviz_plugin/#assumptions-known-limits","text":"TBD.","title":"Assumptions / Known limits"},{"location":"common/autoware_datetime_rviz_plugin/#usage","text":"Start rviz and select panels/Add new panel. Select autoware_datetime_rviz_plugin/AutowareDateTimePanel and press OK.","title":"Usage"},{"location":"common/autoware_localization_rviz_plugin/","text":"autoware_localization_rviz_plugin # Purpose # This plugin can display the history of the localization obtained by ekf_localizer or ndt_scan_matching. Inputs / Outputs # Input # Name Type Description input/pose geometry_msgs::msg::PoseStamped In input/pose, put the result of localization calculated by ekf_localizer or ndt_scan_matching Parameters # Core Parameters # Name Type Default Value Description property_buffer_size_ int 100 Buffer size of topic property_line_view_ bool true Use Line property or not property_line_width_ float 0.1 Width of Line property [m] property_line_alpha_ float 1.0 Alpha of Line property property_line_color_ QColor Qt::white Color of Line property Assumptions / Known limits # TBD. Usage # Start rviz and select Add under the Displays panel. Select autoware_localization_rviz_plugin/PoseHistory and press OK. Enter the name of the topic where you want to view the trajectory.","title":"autoware_localization_rviz_plugin"},{"location":"common/autoware_localization_rviz_plugin/#autoware_localization_rviz_plugin","text":"","title":"autoware_localization_rviz_plugin"},{"location":"common/autoware_localization_rviz_plugin/#purpose","text":"This plugin can display the history of the localization obtained by ekf_localizer or ndt_scan_matching.","title":"Purpose"},{"location":"common/autoware_localization_rviz_plugin/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"common/autoware_localization_rviz_plugin/#input","text":"Name Type Description input/pose geometry_msgs::msg::PoseStamped In input/pose, put the result of localization calculated by ekf_localizer or ndt_scan_matching","title":"Input"},{"location":"common/autoware_localization_rviz_plugin/#parameters","text":"","title":"Parameters"},{"location":"common/autoware_localization_rviz_plugin/#core-parameters","text":"Name Type Default Value Description property_buffer_size_ int 100 Buffer size of topic property_line_view_ bool true Use Line property or not property_line_width_ float 0.1 Width of Line property [m] property_line_alpha_ float 1.0 Alpha of Line property property_line_color_ QColor Qt::white Color of Line property","title":"Core Parameters"},{"location":"common/autoware_localization_rviz_plugin/#assumptions-known-limits","text":"TBD.","title":"Assumptions / Known limits"},{"location":"common/autoware_localization_rviz_plugin/#usage","text":"Start rviz and select Add under the Displays panel. Select autoware_localization_rviz_plugin/PoseHistory and press OK. Enter the name of the topic where you want to view the trajectory.","title":"Usage"},{"location":"common/autoware_perception_rviz_plugin/","text":"autoware_perception_rviz_plugin # Purpose # This plugin is used to generate dummy pedestrians, cars, and obstacles in planning simulator. Overview # The CarInitialPoseTool sends a topic for generating a dummy car. The PedestrianInitialPoseTool sends a topic for generating a dummy pedestrian. The UnknownInitialPoseTool sends a topic for generating a dummy obstacle. The DeleteAllObjectsTool deletes the dummy cars, pedestrians, and obstacles displayed by the above three tools. Inputs / Outputs # Output # Name Type Description /simulation/dummy_perception_publisher/object_info dummy_perception_publisher::msg::Object The topic on which to publish dummy object info Parameter # Core Parameters # CarPose # Name Type Default Value Description topic_property_ string /simulation/dummy_perception_publisher/object_info The topic on which to publish dummy object info std_dev_x_ float 0.03 X standard deviation for initial pose [m] std_dev_y_ float 0.03 Y standard deviation for initial pose [m] std_dev_z_ float 0.03 Z standard deviation for initial pose [m] std_dev_theta_ float 5.0 * M_PI / 180.0 Theta standard deviation for initial pose [rad] position_z_ float 0.0 Z position for initial pose [m] velocity_ float 0.0 Velocity [m/s] PedestrianPose # Name Type Default Value Description topic_property_ string /simulation/dummy_perception_publisher/object_info The topic on which to publish dummy object info std_dev_x_ float 0.03 X standard deviation for initial pose [m] std_dev_y_ float 0.03 Y standard deviation for initial pose [m] std_dev_z_ float 0.03 Z standard deviation for initial pose [m] std_dev_theta_ float 5.0 * M_PI / 180.0 Theta standard deviation for initial pose [rad] position_z_ float 0.0 Z position for initial pose [m] velocity_ float 0.0 Velocity [m/s] UnknownPose # Name Type Default Value Description topic_property_ string /simulation/dummy_perception_publisher/object_info The topic on which to publish dummy object info std_dev_x_ float 0.03 X standard deviation for initial pose [m] std_dev_y_ float 0.03 Y standard deviation for initial pose [m] std_dev_z_ float 0.03 Z standard deviation for initial pose [m] std_dev_theta_ float 5.0 * M_PI / 180.0 Theta standard deviation for initial pose [rad] position_z_ float 0.0 Z position for initial pose [m] velocity_ float 0.0 Velocity [m/s] DeleteAllObjects # Name Type Default Value Description topic_property_ string /simulation/dummy_perception_publisher/object_info The topic on which to publish dummy object info Assumptions / Known limits # Using a planning simulator Usage # Start rviz and select + on the tool tab. Select one of the following: autoware_perception_rviz_plugin and press OK. Select the new item in the tool tab (2D Dummy Car in the example) and click on it in rviz.","title":"autoware_perception_rviz_plugin"},{"location":"common/autoware_perception_rviz_plugin/#autoware_perception_rviz_plugin","text":"","title":"autoware_perception_rviz_plugin"},{"location":"common/autoware_perception_rviz_plugin/#purpose","text":"This plugin is used to generate dummy pedestrians, cars, and obstacles in planning simulator.","title":"Purpose"},{"location":"common/autoware_perception_rviz_plugin/#overview","text":"The CarInitialPoseTool sends a topic for generating a dummy car. The PedestrianInitialPoseTool sends a topic for generating a dummy pedestrian. The UnknownInitialPoseTool sends a topic for generating a dummy obstacle. The DeleteAllObjectsTool deletes the dummy cars, pedestrians, and obstacles displayed by the above three tools.","title":"Overview"},{"location":"common/autoware_perception_rviz_plugin/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"common/autoware_perception_rviz_plugin/#output","text":"Name Type Description /simulation/dummy_perception_publisher/object_info dummy_perception_publisher::msg::Object The topic on which to publish dummy object info","title":"Output"},{"location":"common/autoware_perception_rviz_plugin/#parameter","text":"","title":"Parameter"},{"location":"common/autoware_perception_rviz_plugin/#core-parameters","text":"","title":"Core Parameters"},{"location":"common/autoware_perception_rviz_plugin/#carpose","text":"Name Type Default Value Description topic_property_ string /simulation/dummy_perception_publisher/object_info The topic on which to publish dummy object info std_dev_x_ float 0.03 X standard deviation for initial pose [m] std_dev_y_ float 0.03 Y standard deviation for initial pose [m] std_dev_z_ float 0.03 Z standard deviation for initial pose [m] std_dev_theta_ float 5.0 * M_PI / 180.0 Theta standard deviation for initial pose [rad] position_z_ float 0.0 Z position for initial pose [m] velocity_ float 0.0 Velocity [m/s]","title":"CarPose"},{"location":"common/autoware_perception_rviz_plugin/#pedestrianpose","text":"Name Type Default Value Description topic_property_ string /simulation/dummy_perception_publisher/object_info The topic on which to publish dummy object info std_dev_x_ float 0.03 X standard deviation for initial pose [m] std_dev_y_ float 0.03 Y standard deviation for initial pose [m] std_dev_z_ float 0.03 Z standard deviation for initial pose [m] std_dev_theta_ float 5.0 * M_PI / 180.0 Theta standard deviation for initial pose [rad] position_z_ float 0.0 Z position for initial pose [m] velocity_ float 0.0 Velocity [m/s]","title":"PedestrianPose"},{"location":"common/autoware_perception_rviz_plugin/#unknownpose","text":"Name Type Default Value Description topic_property_ string /simulation/dummy_perception_publisher/object_info The topic on which to publish dummy object info std_dev_x_ float 0.03 X standard deviation for initial pose [m] std_dev_y_ float 0.03 Y standard deviation for initial pose [m] std_dev_z_ float 0.03 Z standard deviation for initial pose [m] std_dev_theta_ float 5.0 * M_PI / 180.0 Theta standard deviation for initial pose [rad] position_z_ float 0.0 Z position for initial pose [m] velocity_ float 0.0 Velocity [m/s]","title":"UnknownPose"},{"location":"common/autoware_perception_rviz_plugin/#deleteallobjects","text":"Name Type Default Value Description topic_property_ string /simulation/dummy_perception_publisher/object_info The topic on which to publish dummy object info","title":"DeleteAllObjects"},{"location":"common/autoware_perception_rviz_plugin/#assumptions-known-limits","text":"Using a planning simulator","title":"Assumptions / Known limits"},{"location":"common/autoware_perception_rviz_plugin/#usage","text":"Start rviz and select + on the tool tab. Select one of the following: autoware_perception_rviz_plugin and press OK. Select the new item in the tool tab (2D Dummy Car in the example) and click on it in rviz.","title":"Usage"},{"location":"common/autoware_planning_rviz_plugin/","text":"autoware_planning_rviz_plugin # This package is including jsk code. Note that jsk_overlay_utils.cpp and jsk_overlay_utils.hpp are BSD license. Purpose # This plugin displays the path, trajectory, and maximum speed. Inputs / Outputs # Input # Name Type Description /input/path autoware_auto_planning_msgs::msg::Path The topic on which to subscribe path /input/trajectory autoware_auto_planning_msgs::msg::Trajectory The topic on which to subscribe trajectory /planning/scenario_planning/current_max_velocity autoware_planning_msgs/msg/VelocityLimit The topic on which to publish max velocity Output # Name Type Description /planning/mission_planning/checkpoint geometry_msgs/msg/PoseStamped The topic on which to publish checkpoint Parameter # Core Parameters # MissionCheckpoint # Name \u3000 Type Default Value Description pose_topic_property_ \u3000 string mission_checkpoint The topic on which to publish checkpoint std_dev_x_ \u3000 float 0.5 X standard deviation for checkpoint pose [m] std_dev_y_ \u3000 float 0.5 Y standard deviation for checkpoint pose [m] std_dev_theta_ \u3000 float M_PI / 12.0 Theta standard deviation for checkpoint pose [rad] position_z_ \u3000 float 0.0 Z position for checkpoint pose [m] Path # Name Type Default Value Description property_path_view_ bool true Use Path property or not property_path_width_ float 2.0 Width of Path property [m] property_path_alpha_ float 1.0 Alpha of Path property property_path_color_view_ bool false Use Constant Color or not property_path_color_ QColor Qt::black Color of Path property property_velocity_view_ bool true Use Velocity property or not property_velocity_alpha_ float 1.0 Alpha of Velocity property property_velocity_scale_ float 0.3 Scale of Velocity property property_velocity_color_view_ bool false Use Constant Color or not property_velocity_color_ QColor Qt::black Color of Velocity property property_vel_max_ float 3.0 Max velocity [m/s] PathFootprint # Name Type Default Value Description property_path_footprint_view_ bool true Use Path Footprint property or not property_path_footprint_alpha_ float 1.0 Alpha of Path Footprint property property_path_footprint_color_ QColor Qt::black Color of Path Footprint property property_vehicle_length_ float 4.77 Vehicle length [m] property_vehicle_width_ float 1.83 Vehicle width [m] property_rear_overhang_ float 1.03 Rear overhang [m] Trajectory # Name Type Default Value Description property_path_view_ bool true Use Path property or not property_path_width_ float 2.0 Width of Path property [m] property_path_alpha_ float 1.0 Alpha of Path property property_path_color_view_ bool false Use Constant Color or not property_path_color_ QColor Qt::black Color of Path property property_velocity_view_ bool true Use Velocity property or not property_velocity_alpha_ float 1.0 Alpha of Velocity property property_velocity_scale_ float 0.3 Scale of Velocity property property_velocity_color_view_ bool false Use Constant Color or not property_velocity_color_ QColor Qt::black Color of Velocity property property_velocity_text_view_ bool false View text Velocity property_velocity_text_scale_ float 0.3 Scale of Velocity property property_vel_max_ float 3.0 Max velocity [m/s] TrajectoryFootprint # Name Type Default Value Description property_trajectory_footprint_view_ bool true Use Trajectory Footprint property or not property_trajectory_footprint_alpha_ float 1.0 Alpha of Trajectory Footprint property property_trajectory_footprint_color_ QColor QColor(230, 230, 50) Color of Trajectory Footprint property property_vehicle_length_ float 4.77 Vehicle length [m] property_vehicle_width_ float 1.83 Vehicle width [m] property_rear_overhang_ float 1.03 Rear overhang [m] property_trajectory_point_view_ bool false Use Trajectory Point property or not property_trajectory_point_alpha_ float 1.0 Alpha of Trajectory Point property property_trajectory_point_color_ QColor QColor(0, 60, 255) Color of Trajectory Point property property_trajectory_point_radius_ float 0.1 Radius of Trajectory Point property MaxVelocity # Name Type Default Value Description property_topic_name_ string /planning/scenario_planning/current_max_velocity The topic on which to subscribe max velocity property_text_color_ QColor QColor(255, 255, 255) Text color property_left_ int 128 Left of the plotter window [px] property_top_ int 128 Top of the plotter window [px] property_length_ int 96 Length of the plotter window [px] property_value_scale_ float 1.0 / 4.0 Value scale Usage # Start rviz and select Add under the Displays panel. Select any one of the autoware_planning_rviz_plugin and press OK. Enter the name of the topic where you want to view the path or trajectory.","title":"autoware_planning_rviz_plugin"},{"location":"common/autoware_planning_rviz_plugin/#autoware_planning_rviz_plugin","text":"This package is including jsk code. Note that jsk_overlay_utils.cpp and jsk_overlay_utils.hpp are BSD license.","title":"autoware_planning_rviz_plugin"},{"location":"common/autoware_planning_rviz_plugin/#purpose","text":"This plugin displays the path, trajectory, and maximum speed.","title":"Purpose"},{"location":"common/autoware_planning_rviz_plugin/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"common/autoware_planning_rviz_plugin/#input","text":"Name Type Description /input/path autoware_auto_planning_msgs::msg::Path The topic on which to subscribe path /input/trajectory autoware_auto_planning_msgs::msg::Trajectory The topic on which to subscribe trajectory /planning/scenario_planning/current_max_velocity autoware_planning_msgs/msg/VelocityLimit The topic on which to publish max velocity","title":"Input"},{"location":"common/autoware_planning_rviz_plugin/#output","text":"Name Type Description /planning/mission_planning/checkpoint geometry_msgs/msg/PoseStamped The topic on which to publish checkpoint","title":"Output"},{"location":"common/autoware_planning_rviz_plugin/#parameter","text":"","title":"Parameter"},{"location":"common/autoware_planning_rviz_plugin/#core-parameters","text":"","title":"Core Parameters"},{"location":"common/autoware_planning_rviz_plugin/#missioncheckpoint","text":"Name \u3000 Type Default Value Description pose_topic_property_ \u3000 string mission_checkpoint The topic on which to publish checkpoint std_dev_x_ \u3000 float 0.5 X standard deviation for checkpoint pose [m] std_dev_y_ \u3000 float 0.5 Y standard deviation for checkpoint pose [m] std_dev_theta_ \u3000 float M_PI / 12.0 Theta standard deviation for checkpoint pose [rad] position_z_ \u3000 float 0.0 Z position for checkpoint pose [m]","title":"MissionCheckpoint"},{"location":"common/autoware_planning_rviz_plugin/#path","text":"Name Type Default Value Description property_path_view_ bool true Use Path property or not property_path_width_ float 2.0 Width of Path property [m] property_path_alpha_ float 1.0 Alpha of Path property property_path_color_view_ bool false Use Constant Color or not property_path_color_ QColor Qt::black Color of Path property property_velocity_view_ bool true Use Velocity property or not property_velocity_alpha_ float 1.0 Alpha of Velocity property property_velocity_scale_ float 0.3 Scale of Velocity property property_velocity_color_view_ bool false Use Constant Color or not property_velocity_color_ QColor Qt::black Color of Velocity property property_vel_max_ float 3.0 Max velocity [m/s]","title":"Path"},{"location":"common/autoware_planning_rviz_plugin/#pathfootprint","text":"Name Type Default Value Description property_path_footprint_view_ bool true Use Path Footprint property or not property_path_footprint_alpha_ float 1.0 Alpha of Path Footprint property property_path_footprint_color_ QColor Qt::black Color of Path Footprint property property_vehicle_length_ float 4.77 Vehicle length [m] property_vehicle_width_ float 1.83 Vehicle width [m] property_rear_overhang_ float 1.03 Rear overhang [m]","title":"PathFootprint"},{"location":"common/autoware_planning_rviz_plugin/#trajectory","text":"Name Type Default Value Description property_path_view_ bool true Use Path property or not property_path_width_ float 2.0 Width of Path property [m] property_path_alpha_ float 1.0 Alpha of Path property property_path_color_view_ bool false Use Constant Color or not property_path_color_ QColor Qt::black Color of Path property property_velocity_view_ bool true Use Velocity property or not property_velocity_alpha_ float 1.0 Alpha of Velocity property property_velocity_scale_ float 0.3 Scale of Velocity property property_velocity_color_view_ bool false Use Constant Color or not property_velocity_color_ QColor Qt::black Color of Velocity property property_velocity_text_view_ bool false View text Velocity property_velocity_text_scale_ float 0.3 Scale of Velocity property property_vel_max_ float 3.0 Max velocity [m/s]","title":"Trajectory"},{"location":"common/autoware_planning_rviz_plugin/#trajectoryfootprint","text":"Name Type Default Value Description property_trajectory_footprint_view_ bool true Use Trajectory Footprint property or not property_trajectory_footprint_alpha_ float 1.0 Alpha of Trajectory Footprint property property_trajectory_footprint_color_ QColor QColor(230, 230, 50) Color of Trajectory Footprint property property_vehicle_length_ float 4.77 Vehicle length [m] property_vehicle_width_ float 1.83 Vehicle width [m] property_rear_overhang_ float 1.03 Rear overhang [m] property_trajectory_point_view_ bool false Use Trajectory Point property or not property_trajectory_point_alpha_ float 1.0 Alpha of Trajectory Point property property_trajectory_point_color_ QColor QColor(0, 60, 255) Color of Trajectory Point property property_trajectory_point_radius_ float 0.1 Radius of Trajectory Point property","title":"TrajectoryFootprint"},{"location":"common/autoware_planning_rviz_plugin/#maxvelocity","text":"Name Type Default Value Description property_topic_name_ string /planning/scenario_planning/current_max_velocity The topic on which to subscribe max velocity property_text_color_ QColor QColor(255, 255, 255) Text color property_left_ int 128 Left of the plotter window [px] property_top_ int 128 Top of the plotter window [px] property_length_ int 96 Length of the plotter window [px] property_value_scale_ float 1.0 / 4.0 Value scale","title":"MaxVelocity"},{"location":"common/autoware_planning_rviz_plugin/#usage","text":"Start rviz and select Add under the Displays panel. Select any one of the autoware_planning_rviz_plugin and press OK. Enter the name of the topic where you want to view the path or trajectory.","title":"Usage"},{"location":"common/autoware_state_rviz_plugin/","text":"autoware_state_rviz_plugin # Purpose # This plugin displays the current status of autoware. This plugin also can engage from the panel. Inputs / Outputs # Input # Name Type Description /control/current_gate_mode autoware_control_msgs::msg::GateMode The topic represents the state of AUTO or EXTERNAL /autoware/state autoware_auto_system_msgs::msg::AutowareState The topic represents the state of Autoware /vehicle/status/gear_status autoware_auto_vehicle_msgs::msg::GearReport The topic represents the state of Gear /api/external/get/engage autoware_external_api_msgs::msg::EngageStatus The topic represents the state of Engage Output # Name Type Description /api/external/set/engage autoware_external_api_msgs::srv::Engage The service inputs engage true HowToUse # Start rviz and select panels/Add new panel. Select autoware_state_rviz_plugin/AutowareStatePanel and press OK. If the AutowareState is WaitingForEngage, can engage by clicking the Engage button.","title":"autoware_state_rviz_plugin"},{"location":"common/autoware_state_rviz_plugin/#autoware_state_rviz_plugin","text":"","title":"autoware_state_rviz_plugin"},{"location":"common/autoware_state_rviz_plugin/#purpose","text":"This plugin displays the current status of autoware. This plugin also can engage from the panel.","title":"Purpose"},{"location":"common/autoware_state_rviz_plugin/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"common/autoware_state_rviz_plugin/#input","text":"Name Type Description /control/current_gate_mode autoware_control_msgs::msg::GateMode The topic represents the state of AUTO or EXTERNAL /autoware/state autoware_auto_system_msgs::msg::AutowareState The topic represents the state of Autoware /vehicle/status/gear_status autoware_auto_vehicle_msgs::msg::GearReport The topic represents the state of Gear /api/external/get/engage autoware_external_api_msgs::msg::EngageStatus The topic represents the state of Engage","title":"Input"},{"location":"common/autoware_state_rviz_plugin/#output","text":"Name Type Description /api/external/set/engage autoware_external_api_msgs::srv::Engage The service inputs engage true","title":"Output"},{"location":"common/autoware_state_rviz_plugin/#howtouse","text":"Start rviz and select panels/Add new panel. Select autoware_state_rviz_plugin/AutowareStatePanel and press OK. If the AutowareState is WaitingForEngage, can engage by clicking the Engage button.","title":"HowToUse"},{"location":"common/autoware_utils/","text":"autoware_utils # Purpose # This package contains many common functions used by other packages, so please refer to them as needed.","title":"autoware_utils"},{"location":"common/autoware_utils/#autoware_utils","text":"","title":"autoware_utils"},{"location":"common/autoware_utils/#purpose","text":"This package contains many common functions used by other packages, so please refer to them as needed.","title":"Purpose"},{"location":"common/autoware_vehicle_rviz_plugin/","text":"autoware_vehicle_rviz_plugin # This package is including jsk code. Note that jsk_overlay_utils.cpp and jsk_overlay_utils.hpp are BSD license. Purpose # This plugin provides a visual and easy-to-understand display of vehicle speed, turn signal and steering status. Inputs / Outputs # Input # Name Type Description /vehicle/status/velocity_status autoware_auto_vehicle_msgs::msg::VelocityReport The topic is vehicle twist /control/turn_signal_cmd autoware_auto_vehicle_msgs::msg::TurnIndicatorsReport The topic is status of turn signal /vehicle/status/steering_status autoware_auto_vehicle_msgs::msg::SteeringReport The topic is status of steering Parameter # Core Parameters # ConsoleMeter # Name Type Default Value Description property_text_color_ QColor QColor(25, 255, 240) Text color property_left_ int 128 Left of the plotter window [px] property_top_ int 128 Top of the plotter window [px] property_length_ int 256 Height of the plotter window [px] property_value_height_offset_ int 0 Height offset of the plotter window [px] property_value_scale_ float 1.0 / 6.667 Value scale SteeringAngle # Name Type Default Value Description property_text_color_ QColor QColor(25, 255, 240) Text color property_left_ int 128 Left of the plotter window [px] property_top_ int 128 Top of the plotter window [px] property_length_ int 256 Height of the plotter window [px] property_value_height_offset_ int 0 Height offset of the plotter window [px] property_value_scale_ float 1.0 / 6.667 Value scale property_handle_angle_scale_ float 3.0 Scale is steering angle to handle angle TurnSignal # Name Type Default Value Description property_left_ int 128 Left of the plotter window [px] property_top_ int 128 Top of the plotter window [px] property_width_ int 256 Left of the plotter window [px] property_height_ int 256 Width of the plotter window [px] VelocityHistory # Name Type Default Value Description property_velocity_timeout_ float 10.0 Timeout of velocity [s] property_velocity_alpha_ float 1.0 Alpha of velocity property_velocity_scale_ float 0.3 Scale of velocity property_velocity_color_view_ bool false Use Constant Color or not property_velocity_color_ QColor Qt::black Color of velocity history property_vel_max_ float 3.0 Color Border Vel Max [m/s] Assumptions / Known limits # TBD. Usage # Start rviz and select Add under the Displays panel. Select any one of the autoware_vehicle_rviz_plugin and press OK. Enter the name of the topic where you want to view the status.","title":"autoware_vehicle_rviz_plugin"},{"location":"common/autoware_vehicle_rviz_plugin/#autoware_vehicle_rviz_plugin","text":"This package is including jsk code. Note that jsk_overlay_utils.cpp and jsk_overlay_utils.hpp are BSD license.","title":"autoware_vehicle_rviz_plugin"},{"location":"common/autoware_vehicle_rviz_plugin/#purpose","text":"This plugin provides a visual and easy-to-understand display of vehicle speed, turn signal and steering status.","title":"Purpose"},{"location":"common/autoware_vehicle_rviz_plugin/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"common/autoware_vehicle_rviz_plugin/#input","text":"Name Type Description /vehicle/status/velocity_status autoware_auto_vehicle_msgs::msg::VelocityReport The topic is vehicle twist /control/turn_signal_cmd autoware_auto_vehicle_msgs::msg::TurnIndicatorsReport The topic is status of turn signal /vehicle/status/steering_status autoware_auto_vehicle_msgs::msg::SteeringReport The topic is status of steering","title":"Input"},{"location":"common/autoware_vehicle_rviz_plugin/#parameter","text":"","title":"Parameter"},{"location":"common/autoware_vehicle_rviz_plugin/#core-parameters","text":"","title":"Core Parameters"},{"location":"common/autoware_vehicle_rviz_plugin/#consolemeter","text":"Name Type Default Value Description property_text_color_ QColor QColor(25, 255, 240) Text color property_left_ int 128 Left of the plotter window [px] property_top_ int 128 Top of the plotter window [px] property_length_ int 256 Height of the plotter window [px] property_value_height_offset_ int 0 Height offset of the plotter window [px] property_value_scale_ float 1.0 / 6.667 Value scale","title":"ConsoleMeter"},{"location":"common/autoware_vehicle_rviz_plugin/#steeringangle","text":"Name Type Default Value Description property_text_color_ QColor QColor(25, 255, 240) Text color property_left_ int 128 Left of the plotter window [px] property_top_ int 128 Top of the plotter window [px] property_length_ int 256 Height of the plotter window [px] property_value_height_offset_ int 0 Height offset of the plotter window [px] property_value_scale_ float 1.0 / 6.667 Value scale property_handle_angle_scale_ float 3.0 Scale is steering angle to handle angle","title":"SteeringAngle"},{"location":"common/autoware_vehicle_rviz_plugin/#turnsignal","text":"Name Type Default Value Description property_left_ int 128 Left of the plotter window [px] property_top_ int 128 Top of the plotter window [px] property_width_ int 256 Left of the plotter window [px] property_height_ int 256 Width of the plotter window [px]","title":"TurnSignal"},{"location":"common/autoware_vehicle_rviz_plugin/#velocityhistory","text":"Name Type Default Value Description property_velocity_timeout_ float 10.0 Timeout of velocity [s] property_velocity_alpha_ float 1.0 Alpha of velocity property_velocity_scale_ float 0.3 Scale of velocity property_velocity_color_view_ bool false Use Constant Color or not property_velocity_color_ QColor Qt::black Color of velocity history property_vel_max_ float 3.0 Color Border Vel Max [m/s]","title":"VelocityHistory"},{"location":"common/autoware_vehicle_rviz_plugin/#assumptions-known-limits","text":"TBD.","title":"Assumptions / Known limits"},{"location":"common/autoware_vehicle_rviz_plugin/#usage","text":"Start rviz and select Add under the Displays panel. Select any one of the autoware_vehicle_rviz_plugin and press OK. Enter the name of the topic where you want to view the status.","title":"Usage"},{"location":"common/autoware_web_controller/","text":"autoware_web_controller # Purpose # This packages is for visualizing the status of Autoware and sending topics for Autoware from a web page. Inputs / Outputs # Input # Name Type Description /control/current_gate_mode autoware_control_msgs::msg::GateMode Gate mode (AUTO or EXTERNAL) /autoware/state autoware_auto_system_msgs::msg::AutowareState State of Autoware /autoware/engage autoware_auto_system_msgs::msg::Engage Engage signal for Autoware /vehicle/engage autoware_auto_system_msgs::msg::Engage Engage signal for a vehicle /planning/scenario_planning/max_velocity_default autoware_planning_msgs::msg::VelocityLimit Max velocity of Autoware Output # Name Type Description /planning/scenario_planning/lane_driving/behavior_planning/behavior_path_planner/path_change_approval autoware_planning_msgs::msg::Approval Send an approval signal for path change request such as lane change or obstacle avoidance /autoware/engage autoware_auto_system_msgs::msg::Engage Send an engage signal for Autoware /vehicle/engage autoware_auto_system_msgs::msg::Engage Send an engage signal for a vehicle /planning/scenario_planning/max_velocity_default autoware_planning_msgs::msg::VelocityLimit Set a max velocity of Autoware Parameter # Core Parameters # None Assumptions / Known limits # TBD. Usage # Access to http://localhost:8085/autoware_web_controller after launching Autoware. Check the status of Autoware or send topics by the buttons.","title":"autoware_web_controller"},{"location":"common/autoware_web_controller/#autoware_web_controller","text":"","title":"autoware_web_controller"},{"location":"common/autoware_web_controller/#purpose","text":"This packages is for visualizing the status of Autoware and sending topics for Autoware from a web page.","title":"Purpose"},{"location":"common/autoware_web_controller/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"common/autoware_web_controller/#input","text":"Name Type Description /control/current_gate_mode autoware_control_msgs::msg::GateMode Gate mode (AUTO or EXTERNAL) /autoware/state autoware_auto_system_msgs::msg::AutowareState State of Autoware /autoware/engage autoware_auto_system_msgs::msg::Engage Engage signal for Autoware /vehicle/engage autoware_auto_system_msgs::msg::Engage Engage signal for a vehicle /planning/scenario_planning/max_velocity_default autoware_planning_msgs::msg::VelocityLimit Max velocity of Autoware","title":"Input"},{"location":"common/autoware_web_controller/#output","text":"Name Type Description /planning/scenario_planning/lane_driving/behavior_planning/behavior_path_planner/path_change_approval autoware_planning_msgs::msg::Approval Send an approval signal for path change request such as lane change or obstacle avoidance /autoware/engage autoware_auto_system_msgs::msg::Engage Send an engage signal for Autoware /vehicle/engage autoware_auto_system_msgs::msg::Engage Send an engage signal for a vehicle /planning/scenario_planning/max_velocity_default autoware_planning_msgs::msg::VelocityLimit Set a max velocity of Autoware","title":"Output"},{"location":"common/autoware_web_controller/#parameter","text":"","title":"Parameter"},{"location":"common/autoware_web_controller/#core-parameters","text":"None","title":"Core Parameters"},{"location":"common/autoware_web_controller/#assumptions-known-limits","text":"TBD.","title":"Assumptions / Known limits"},{"location":"common/autoware_web_controller/#usage","text":"Access to http://localhost:8085/autoware_web_controller after launching Autoware. Check the status of Autoware or send topics by the buttons.","title":"Usage"},{"location":"common/goal_distance_calculator/Readme/","text":"goal_distance_calculator # Purpose # This node publishes deviation of self-pose from goal pose. Inner-workings / Algorithms # Inputs / Outputs # Input # Name Type Description /planning/mission_planning/route autoware_auto_planning_msgs::msg::Route Used to get goal pose /tf tf2_msgs/TFMessage TF (self-pose) Output # Name Type Description deviation/lateral autoware_debug_msgs::msg::Float64Stamped publish lateral deviation of self-pose from goal pose[m] deviation/longitudinal autoware_debug_msgs::msg::Float64Stamped publish longitudinal deviation of self-pose from goal pose[m] deviation/yaw autoware_debug_msgs::msg::Float64Stamped publish yaw deviation of self-pose from goal pose[rad] deviation/yaw_deg autoware_debug_msgs::msg::Float64Stamped publish yaw deviation of self-pose from goal pose[deg] Parameters # Node Parameters # Name Type Default Value Explanation update_rate double 10.0 Timer callback period. [Hz] Core Parameters # Name Type Default Value Explanation oneshot bool true publish deviations just once or repeatedly Assumptions / Known limits # TBD.","title":"goal_distance_calculator"},{"location":"common/goal_distance_calculator/Readme/#goal_distance_calculator","text":"","title":"goal_distance_calculator"},{"location":"common/goal_distance_calculator/Readme/#purpose","text":"This node publishes deviation of self-pose from goal pose.","title":"Purpose"},{"location":"common/goal_distance_calculator/Readme/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"common/goal_distance_calculator/Readme/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"common/goal_distance_calculator/Readme/#input","text":"Name Type Description /planning/mission_planning/route autoware_auto_planning_msgs::msg::Route Used to get goal pose /tf tf2_msgs/TFMessage TF (self-pose)","title":"Input"},{"location":"common/goal_distance_calculator/Readme/#output","text":"Name Type Description deviation/lateral autoware_debug_msgs::msg::Float64Stamped publish lateral deviation of self-pose from goal pose[m] deviation/longitudinal autoware_debug_msgs::msg::Float64Stamped publish longitudinal deviation of self-pose from goal pose[m] deviation/yaw autoware_debug_msgs::msg::Float64Stamped publish yaw deviation of self-pose from goal pose[rad] deviation/yaw_deg autoware_debug_msgs::msg::Float64Stamped publish yaw deviation of self-pose from goal pose[deg]","title":"Output"},{"location":"common/goal_distance_calculator/Readme/#parameters","text":"","title":"Parameters"},{"location":"common/goal_distance_calculator/Readme/#node-parameters","text":"Name Type Default Value Explanation update_rate double 10.0 Timer callback period. [Hz]","title":"Node Parameters"},{"location":"common/goal_distance_calculator/Readme/#core-parameters","text":"Name Type Default Value Explanation oneshot bool true publish deviations just once or repeatedly","title":"Core Parameters"},{"location":"common/goal_distance_calculator/Readme/#assumptions-known-limits","text":"TBD.","title":"Assumptions / Known limits"},{"location":"common/interpolation/","text":"Interpolation package # This package supplies linear and spline interpolation functions. Linear Interpolation # lerp(src_val, dst_val, ratio) (for scalar interpolation) interpolates src_val and dst_val with ratio . This will be replaced with std::lerp(src_val, dst_val, ratio) in C++20 . lerp(base_keys, base_values, query_keys) (for vector interpolation) applies linear regression to each two continuous points whose x values are base_keys and whose y values are base_values . Then it calculates interpolated values on y-axis for query_keys on x-axis. Spline Interpolation # slerp(base_keys, base_values, query_keys) (for vector interpolation) applies spline regression to each two continuous points whose x values are base_keys and whose y values are base_values . Then it calculates interpolated values on y-axis for query_keys on x-axis. Evaluation of calculation cost # We evaluated calculation cost of spline interpolation for 100 points, and adopted the best one which is tridiagonal matrix algorithm. Methods except for tridiagonal matrix algorithm exists in spline_interpolation package, which has been removed from Autoware. Method Calculation time Tridiagonal Matrix Algorithm 0.007 [ms] Preconditioned Conjugate Gradient 0.024 [ms] Successive Over-Relaxation 0.074 [ms] Spline Interpolation Algorithm # Assuming that the size of base_keys ( x_i x_i ) and base_values ( y_i y_i ) are N + 1 N + 1 , we aim to calculate spline interpolation with the following equation to interpolate between y_i y_i and y_{i+1} y_{i+1} . Y_i(x) = a_i (x - x_i)^3 + b_i (x - x_i)^2 + c_i (x - x_i) + d_i \\ \\ \\ (i = 0, \\dots, N-1) Y_i(x) = a_i (x - x_i)^3 + b_i (x - x_i)^2 + c_i (x - x_i) + d_i \\ \\ \\ (i = 0, \\dots, N-1) Constraints on spline interpolation are as follows. The number of constraints is 4N 4N , which is equal to the number of variables of spline interpolation. \\begin{align} Y_i (x_i) & = y_i \\ \\ \\ (i = 0, \\dots, N-1) \\\\ Y_i (x_{i+1}) & = y_{i+1} \\ \\ \\ (i = 0, \\dots, N-1) \\\\ Y'_i (x_{i+1}) & = Y'_{i+1} (x_{i+1}) \\ \\ \\ (i = 0, \\dots, N-2) \\\\ Y''_i (x_{i+1}) & = Y''_{i+1} (x_{i+1}) \\ \\ \\ (i = 0, \\dots, N-2) \\\\ Y''_0 (x_0) & = 0 \\\\ Y''_{N-1} (x_N) & = 0 \\end{align} \\begin{align} Y_i (x_i) & = y_i \\ \\ \\ (i = 0, \\dots, N-1) \\\\ Y_i (x_{i+1}) & = y_{i+1} \\ \\ \\ (i = 0, \\dots, N-1) \\\\ Y'_i (x_{i+1}) & = Y'_{i+1} (x_{i+1}) \\ \\ \\ (i = 0, \\dots, N-2) \\\\ Y''_i (x_{i+1}) & = Y''_{i+1} (x_{i+1}) \\ \\ \\ (i = 0, \\dots, N-2) \\\\ Y''_0 (x_0) & = 0 \\\\ Y''_{N-1} (x_N) & = 0 \\end{align} According to this article , spline interpolation is formulated as the following linear equation. \\begin{align} \\begin{pmatrix} 2(h_0 + h_1) & h_1 \\\\ h_0 & 2 (h_1 + h_2) & h_2 & & O \\\\ & & & \\ddots \\\\ O & & & & h_{N-2} & 2 (h_{N-2} + h_{N-1}) \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\\\ v_3 \\\\ \\vdots \\\\ v_{N-1} \\end{pmatrix}= \\begin{pmatrix} w_1 \\\\ w_2 \\\\ w_3 \\\\ \\vdots \\\\ w_{N-1} \\end{pmatrix} \\end{align} \\begin{align} \\begin{pmatrix} 2(h_0 + h_1) & h_1 \\\\ h_0 & 2 (h_1 + h_2) & h_2 & & O \\\\ & & & \\ddots \\\\ O & & & & h_{N-2} & 2 (h_{N-2} + h_{N-1}) \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\\\ v_3 \\\\ \\vdots \\\\ v_{N-1} \\end{pmatrix}= \\begin{pmatrix} w_1 \\\\ w_2 \\\\ w_3 \\\\ \\vdots \\\\ w_{N-1} \\end{pmatrix} \\end{align} where \\begin{align} h_i & = x_{i+1} - x_i \\ \\ \\ (i = 0, \\dots, N-1) \\\\ w_i & = 6 \\left(\\frac{y_{i+1} - y_{i+1}}{h_i} - \\frac{y_i - y_{i-1}}{h_{i-1}}\\right) \\ \\ \\ (i = 1, \\dots, N-1) \\end{align} \\begin{align} h_i & = x_{i+1} - x_i \\ \\ \\ (i = 0, \\dots, N-1) \\\\ w_i & = 6 \\left(\\frac{y_{i+1} - y_{i+1}}{h_i} - \\frac{y_i - y_{i-1}}{h_{i-1}}\\right) \\ \\ \\ (i = 1, \\dots, N-1) \\end{align} The coefficient matrix of this linear equation is tridiagonal matrix. Therefore, it can be solve with tridiagonal matrix algorithm, which can solve linear equations without gradient descent methods. Solving this linear equation with tridiagonal matrix algorithm, we can calculate coefficients of spline interpolation as follows. \\begin{align} a_i & = \\frac{v_{i+1} - v_i}{6 (x_{i+1} - x_i)} \\ \\ \\ (i = 0, \\dots, N-1) \\\\ b_i & = \\frac{v_i}{2} \\ \\ \\ (i = 0, \\dots, N-1) \\\\ c_i & = \\frac{y_{i+1} - y_i}{x_{i+1} - x_i} - \\frac{1}{6}(x_{i+1} - x_i)(2 v_i + v_{i+1}) \\ \\ \\ (i = 0, \\dots, N-1) \\\\ d_i & = y_i \\ \\ \\ (i = 0, \\dots, N-1) \\end{align} \\begin{align} a_i & = \\frac{v_{i+1} - v_i}{6 (x_{i+1} - x_i)} \\ \\ \\ (i = 0, \\dots, N-1) \\\\ b_i & = \\frac{v_i}{2} \\ \\ \\ (i = 0, \\dots, N-1) \\\\ c_i & = \\frac{y_{i+1} - y_i}{x_{i+1} - x_i} - \\frac{1}{6}(x_{i+1} - x_i)(2 v_i + v_{i+1}) \\ \\ \\ (i = 0, \\dots, N-1) \\\\ d_i & = y_i \\ \\ \\ (i = 0, \\dots, N-1) \\end{align} Tridiagonal Matrix Algorithm # We solve tridiagonal linear equation according to this article where variables of linear equation are expressed as follows in the implementation. \\begin{align} \\begin{pmatrix} b_0 & c_0 & & \\\\ a_0 & b_1 & c_2 & O \\\\ & & \\ddots \\\\ O & & a_{N-2} & b_{N-1} \\end{pmatrix} x = \\begin{pmatrix} d_0 \\\\ d_2 \\\\ d_3 \\\\ \\vdots \\\\ d_{N-1} \\end{pmatrix} \\end{align} \\begin{align} \\begin{pmatrix} b_0 & c_0 & & \\\\ a_0 & b_1 & c_2 & O \\\\ & & \\ddots \\\\ O & & a_{N-2} & b_{N-1} \\end{pmatrix} x = \\begin{pmatrix} d_0 \\\\ d_2 \\\\ d_3 \\\\ \\vdots \\\\ d_{N-1} \\end{pmatrix} \\end{align}","title":"Interpolation package"},{"location":"common/interpolation/#interpolation-package","text":"This package supplies linear and spline interpolation functions.","title":"Interpolation package"},{"location":"common/interpolation/#linear-interpolation","text":"lerp(src_val, dst_val, ratio) (for scalar interpolation) interpolates src_val and dst_val with ratio . This will be replaced with std::lerp(src_val, dst_val, ratio) in C++20 . lerp(base_keys, base_values, query_keys) (for vector interpolation) applies linear regression to each two continuous points whose x values are base_keys and whose y values are base_values . Then it calculates interpolated values on y-axis for query_keys on x-axis.","title":"Linear Interpolation"},{"location":"common/interpolation/#spline-interpolation","text":"slerp(base_keys, base_values, query_keys) (for vector interpolation) applies spline regression to each two continuous points whose x values are base_keys and whose y values are base_values . Then it calculates interpolated values on y-axis for query_keys on x-axis.","title":"Spline Interpolation"},{"location":"common/interpolation/#evaluation-of-calculation-cost","text":"We evaluated calculation cost of spline interpolation for 100 points, and adopted the best one which is tridiagonal matrix algorithm. Methods except for tridiagonal matrix algorithm exists in spline_interpolation package, which has been removed from Autoware. Method Calculation time Tridiagonal Matrix Algorithm 0.007 [ms] Preconditioned Conjugate Gradient 0.024 [ms] Successive Over-Relaxation 0.074 [ms]","title":"Evaluation of calculation cost"},{"location":"common/interpolation/#spline-interpolation-algorithm","text":"Assuming that the size of base_keys ( x_i x_i ) and base_values ( y_i y_i ) are N + 1 N + 1 , we aim to calculate spline interpolation with the following equation to interpolate between y_i y_i and y_{i+1} y_{i+1} . Y_i(x) = a_i (x - x_i)^3 + b_i (x - x_i)^2 + c_i (x - x_i) + d_i \\ \\ \\ (i = 0, \\dots, N-1) Y_i(x) = a_i (x - x_i)^3 + b_i (x - x_i)^2 + c_i (x - x_i) + d_i \\ \\ \\ (i = 0, \\dots, N-1) Constraints on spline interpolation are as follows. The number of constraints is 4N 4N , which is equal to the number of variables of spline interpolation. \\begin{align} Y_i (x_i) & = y_i \\ \\ \\ (i = 0, \\dots, N-1) \\\\ Y_i (x_{i+1}) & = y_{i+1} \\ \\ \\ (i = 0, \\dots, N-1) \\\\ Y'_i (x_{i+1}) & = Y'_{i+1} (x_{i+1}) \\ \\ \\ (i = 0, \\dots, N-2) \\\\ Y''_i (x_{i+1}) & = Y''_{i+1} (x_{i+1}) \\ \\ \\ (i = 0, \\dots, N-2) \\\\ Y''_0 (x_0) & = 0 \\\\ Y''_{N-1} (x_N) & = 0 \\end{align} \\begin{align} Y_i (x_i) & = y_i \\ \\ \\ (i = 0, \\dots, N-1) \\\\ Y_i (x_{i+1}) & = y_{i+1} \\ \\ \\ (i = 0, \\dots, N-1) \\\\ Y'_i (x_{i+1}) & = Y'_{i+1} (x_{i+1}) \\ \\ \\ (i = 0, \\dots, N-2) \\\\ Y''_i (x_{i+1}) & = Y''_{i+1} (x_{i+1}) \\ \\ \\ (i = 0, \\dots, N-2) \\\\ Y''_0 (x_0) & = 0 \\\\ Y''_{N-1} (x_N) & = 0 \\end{align} According to this article , spline interpolation is formulated as the following linear equation. \\begin{align} \\begin{pmatrix} 2(h_0 + h_1) & h_1 \\\\ h_0 & 2 (h_1 + h_2) & h_2 & & O \\\\ & & & \\ddots \\\\ O & & & & h_{N-2} & 2 (h_{N-2} + h_{N-1}) \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\\\ v_3 \\\\ \\vdots \\\\ v_{N-1} \\end{pmatrix}= \\begin{pmatrix} w_1 \\\\ w_2 \\\\ w_3 \\\\ \\vdots \\\\ w_{N-1} \\end{pmatrix} \\end{align} \\begin{align} \\begin{pmatrix} 2(h_0 + h_1) & h_1 \\\\ h_0 & 2 (h_1 + h_2) & h_2 & & O \\\\ & & & \\ddots \\\\ O & & & & h_{N-2} & 2 (h_{N-2} + h_{N-1}) \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\\\ v_3 \\\\ \\vdots \\\\ v_{N-1} \\end{pmatrix}= \\begin{pmatrix} w_1 \\\\ w_2 \\\\ w_3 \\\\ \\vdots \\\\ w_{N-1} \\end{pmatrix} \\end{align} where \\begin{align} h_i & = x_{i+1} - x_i \\ \\ \\ (i = 0, \\dots, N-1) \\\\ w_i & = 6 \\left(\\frac{y_{i+1} - y_{i+1}}{h_i} - \\frac{y_i - y_{i-1}}{h_{i-1}}\\right) \\ \\ \\ (i = 1, \\dots, N-1) \\end{align} \\begin{align} h_i & = x_{i+1} - x_i \\ \\ \\ (i = 0, \\dots, N-1) \\\\ w_i & = 6 \\left(\\frac{y_{i+1} - y_{i+1}}{h_i} - \\frac{y_i - y_{i-1}}{h_{i-1}}\\right) \\ \\ \\ (i = 1, \\dots, N-1) \\end{align} The coefficient matrix of this linear equation is tridiagonal matrix. Therefore, it can be solve with tridiagonal matrix algorithm, which can solve linear equations without gradient descent methods. Solving this linear equation with tridiagonal matrix algorithm, we can calculate coefficients of spline interpolation as follows. \\begin{align} a_i & = \\frac{v_{i+1} - v_i}{6 (x_{i+1} - x_i)} \\ \\ \\ (i = 0, \\dots, N-1) \\\\ b_i & = \\frac{v_i}{2} \\ \\ \\ (i = 0, \\dots, N-1) \\\\ c_i & = \\frac{y_{i+1} - y_i}{x_{i+1} - x_i} - \\frac{1}{6}(x_{i+1} - x_i)(2 v_i + v_{i+1}) \\ \\ \\ (i = 0, \\dots, N-1) \\\\ d_i & = y_i \\ \\ \\ (i = 0, \\dots, N-1) \\end{align} \\begin{align} a_i & = \\frac{v_{i+1} - v_i}{6 (x_{i+1} - x_i)} \\ \\ \\ (i = 0, \\dots, N-1) \\\\ b_i & = \\frac{v_i}{2} \\ \\ \\ (i = 0, \\dots, N-1) \\\\ c_i & = \\frac{y_{i+1} - y_i}{x_{i+1} - x_i} - \\frac{1}{6}(x_{i+1} - x_i)(2 v_i + v_{i+1}) \\ \\ \\ (i = 0, \\dots, N-1) \\\\ d_i & = y_i \\ \\ \\ (i = 0, \\dots, N-1) \\end{align}","title":"Spline Interpolation Algorithm"},{"location":"common/interpolation/#tridiagonal-matrix-algorithm","text":"We solve tridiagonal linear equation according to this article where variables of linear equation are expressed as follows in the implementation. \\begin{align} \\begin{pmatrix} b_0 & c_0 & & \\\\ a_0 & b_1 & c_2 & O \\\\ & & \\ddots \\\\ O & & a_{N-2} & b_{N-1} \\end{pmatrix} x = \\begin{pmatrix} d_0 \\\\ d_2 \\\\ d_3 \\\\ \\vdots \\\\ d_{N-1} \\end{pmatrix} \\end{align} \\begin{align} \\begin{pmatrix} b_0 & c_0 & & \\\\ a_0 & b_1 & c_2 & O \\\\ & & \\ddots \\\\ O & & a_{N-2} & b_{N-1} \\end{pmatrix} x = \\begin{pmatrix} d_0 \\\\ d_2 \\\\ d_3 \\\\ \\vdots \\\\ d_{N-1} \\end{pmatrix} \\end{align}","title":"Tridiagonal Matrix Algorithm"},{"location":"common/kalman_filter/","text":"kalman_filter # Purpose # This common package contains the kalman filter with time delay and the calculation of the kalman filter. Assumptions / Known limits # TBD.","title":"kalman_filter"},{"location":"common/kalman_filter/#kalman_filter","text":"","title":"kalman_filter"},{"location":"common/kalman_filter/#purpose","text":"This common package contains the kalman filter with time delay and the calculation of the kalman filter.","title":"Purpose"},{"location":"common/kalman_filter/#assumptions-known-limits","text":"TBD.","title":"Assumptions / Known limits"},{"location":"common/osqp_interface/design/osqp_interface-design/","text":"Interface for the OSQP library # This is the design document for the osqp_interface package. Purpose / Use cases # This packages provides a C++ interface for the OSQP library . Design # The class OSQPInterface takes a problem formulation as Eigen matrices and vectors, converts these objects into C-style Compressed-Column-Sparse matrices and dynamic arrays, loads the data into the OSQP workspace dataholder, and runs the optimizer. Inputs / Outputs / API # The interface can be used in several ways: 1. Initialize the interface WITHOUT data. Load the problem formulation at the optimization call. osqp_interface = OSQPInterface(); osqp_interface.optimize(P, A, q, l, u); 2. Initialize the interface WITH data. osqp_interface = OSQPInterface(P, A, q, l, u); osqp_interface.optimize(); 3. WARM START OPTIMIZATION by modifying the problem formulation between optimization runs. osqp_interface = OSQPInterface(P, A, q, l, u); osqp_interface.optimize(); osqp.initializeProblem(P_new, A_new, q_new, l_new, u_new); osqp_interface.optimize(); The optimization results are returned as a vector by the optimization function. std::tuple<std::vector<double>, std::vector<double>> result = osqp_interface.optimize(); std::vector<double> param = std::get<0>(result); double x_0 = param[0]; double x_1 = param[1]; References / External links # OSQP library: https://osqp.org/ Related issues # This package was introduced as a dependency of the MPC-based lateral controller: https://gitlab.com/autowarefoundation/autoware.auto/AutowareAuto/-/issues/1057","title":"Osqp interface design"},{"location":"common/osqp_interface/design/osqp_interface-design/#osqp_interface-package-design","text":"This is the design document for the osqp_interface package.","title":"Interface for the OSQP library"},{"location":"common/osqp_interface/design/osqp_interface-design/#purpose-use-cases","text":"This packages provides a C++ interface for the OSQP library .","title":"Purpose / Use cases"},{"location":"common/osqp_interface/design/osqp_interface-design/#design","text":"The class OSQPInterface takes a problem formulation as Eigen matrices and vectors, converts these objects into C-style Compressed-Column-Sparse matrices and dynamic arrays, loads the data into the OSQP workspace dataholder, and runs the optimizer.","title":"Design"},{"location":"common/osqp_interface/design/osqp_interface-design/#inputs-outputs-api","text":"The interface can be used in several ways: 1. Initialize the interface WITHOUT data. Load the problem formulation at the optimization call. osqp_interface = OSQPInterface(); osqp_interface.optimize(P, A, q, l, u); 2. Initialize the interface WITH data. osqp_interface = OSQPInterface(P, A, q, l, u); osqp_interface.optimize(); 3. WARM START OPTIMIZATION by modifying the problem formulation between optimization runs. osqp_interface = OSQPInterface(P, A, q, l, u); osqp_interface.optimize(); osqp.initializeProblem(P_new, A_new, q_new, l_new, u_new); osqp_interface.optimize(); The optimization results are returned as a vector by the optimization function. std::tuple<std::vector<double>, std::vector<double>> result = osqp_interface.optimize(); std::vector<double> param = std::get<0>(result); double x_0 = param[0]; double x_1 = param[1];","title":"Inputs / Outputs / API"},{"location":"common/osqp_interface/design/osqp_interface-design/#references-external-links","text":"OSQP library: https://osqp.org/","title":"References / External links"},{"location":"common/osqp_interface/design/osqp_interface-design/#related-issues","text":"This package was introduced as a dependency of the MPC-based lateral controller: https://gitlab.com/autowarefoundation/autoware.auto/AutowareAuto/-/issues/1057","title":"Related issues"},{"location":"common/path_distance_calculator/Readme/","text":"path_distance_calculator # Purpose # This node publishes a distance from the closest path point from the self-position to the end point of the path. Note that the distance means the arc-length along the path, not the Euclidean distance between the two points. Inner-workings / Algorithms # Inputs / Outputs # Input # Name Type Description /planning/scenario_planning/lane_driving/behavior_planning/path autoware_auto_planning_msgs::msg::Path Reference path /tf tf2_msgs/TFMessage TF (self-pose) Output # Name Type Description ~/distance autoware_debug_msgs::msg::Float64Stamped Publish a distance from the closest path point from the self-position to the end point of the path[m] Parameters # Node Parameters # None. Core Parameters # None. Assumptions / Known limits # TBD.","title":"path_distance_calculator"},{"location":"common/path_distance_calculator/Readme/#path_distance_calculator","text":"","title":"path_distance_calculator"},{"location":"common/path_distance_calculator/Readme/#purpose","text":"This node publishes a distance from the closest path point from the self-position to the end point of the path. Note that the distance means the arc-length along the path, not the Euclidean distance between the two points.","title":"Purpose"},{"location":"common/path_distance_calculator/Readme/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"common/path_distance_calculator/Readme/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"common/path_distance_calculator/Readme/#input","text":"Name Type Description /planning/scenario_planning/lane_driving/behavior_planning/path autoware_auto_planning_msgs::msg::Path Reference path /tf tf2_msgs/TFMessage TF (self-pose)","title":"Input"},{"location":"common/path_distance_calculator/Readme/#output","text":"Name Type Description ~/distance autoware_debug_msgs::msg::Float64Stamped Publish a distance from the closest path point from the self-position to the end point of the path[m]","title":"Output"},{"location":"common/path_distance_calculator/Readme/#parameters","text":"","title":"Parameters"},{"location":"common/path_distance_calculator/Readme/#node-parameters","text":"None.","title":"Node Parameters"},{"location":"common/path_distance_calculator/Readme/#core-parameters","text":"None.","title":"Core Parameters"},{"location":"common/path_distance_calculator/Readme/#assumptions-known-limits","text":"TBD.","title":"Assumptions / Known limits"},{"location":"common/polar_grid/Readme/","text":"Polar Grid # Purpose # This plugin displays polar grid around ego vehicle in Rviz. Core Parameters # Name Type Default Value Explanation Max Range float 200.0f max range for polar grid. [m] Wave Velocity float 100.0f wave ring velocity. [m/s] Delta Range float 10.0f wave ring distance for polar grid. [m] Assumptions / Known limits # TBD.","title":"Polar Grid"},{"location":"common/polar_grid/Readme/#polar-grid","text":"","title":"Polar Grid"},{"location":"common/polar_grid/Readme/#purpose","text":"This plugin displays polar grid around ego vehicle in Rviz.","title":"Purpose"},{"location":"common/polar_grid/Readme/#core-parameters","text":"Name Type Default Value Explanation Max Range float 200.0f max range for polar grid. [m] Wave Velocity float 100.0f wave ring velocity. [m/s] Delta Range float 10.0f wave ring distance for polar grid. [m]","title":"Core Parameters"},{"location":"common/polar_grid/Readme/#assumptions-known-limits","text":"TBD.","title":"Assumptions / Known limits"},{"location":"common/signal_processing/","text":"signal_processing # low-pass filter currently supports only the 1-D low pass filtering. Assumptions / Known limits # TBD.","title":"signal_processing"},{"location":"common/signal_processing/#signal_processing","text":"low-pass filter currently supports only the 1-D low pass filtering.","title":"signal_processing"},{"location":"common/signal_processing/#assumptions-known-limits","text":"TBD.","title":"Assumptions / Known limits"},{"location":"control/lane_departure_checker/","text":"Lane Departure Checker # The Lane Departure Checker checks if vehicle follows a trajectory. If it does not follow the trajectory, it reports its status via diagnostic_updater . Features # This package includes the following features: Lane Departure : Check if ego vehicle is going to be out of lane boundaries based on output from control module (predicted trajectory). Trajectory Deviation : Check if ego vehicle's pose does not deviate from the trajectory. Checking lateral, longitudinal and yaw deviation. Inner-workings / Algorithms # How to extend footprint by covariance # Calculate the standard deviation of error ellipse(covariance) in vehicle coordinate. 1.Transform covariance into vehicle coordinate. \\begin{align} \\left( \\begin{array}{cc} x_{vehicle}\\\\ y_{vehicle}\\\\ \\end{array} \\right) = R_{map2vehicle} \\left( \\begin{array}{cc} x_{map}\\\\ y_{map}\\\\ \\end{array} \\right) \\end{align} Calculate covariance in vehicle coordinate. \\begin{align} Cov_{vehicle} &= E \\left[ \\left( \\begin{array}{cc} x_{vehicle}\\\\ y_{vehicle}\\\\ \\end{array} \\right) (x_{vehicle}, y_{vehicle}) \\right] \\\\ &= E \\left[ R\\left( \\begin{array}{cc} x_{map}\\\\ y_{map}\\\\ \\end{array} \\right) (x_{map}, y_{map})R^t \\right] \\\\ &= R E\\left[ \\left( \\begin{array}{cc} x_{map}\\\\ y_{map}\\\\ \\end{array} \\right) (x_{map}, y_{map}) \\right] R^t \\\\ &= R Cov_{map} R^t \\end{align} 2.The longitudinal length we want to expand is correspond to marginal distribution of x_{vehicle} x_{vehicle} , which is represented in Cov_{vehicle}(0,0) Cov_{vehicle}(0,0) . In the same way, the lateral length is represented in Cov_{vehicle}(1,1) Cov_{vehicle}(1,1) . Wikipedia reference here . Expand footprint based on the standard deviation multiplied with footprint_margin_scale . Interface # Input # /localization/kinematic_state [ nav_msgs::msg::Odometry ] /map/vector_map [ autoware_auto_mapping_msgs::msg::HADMapBin ] /planning/mission_planning/route [ autoware_auto_planning_msgs::msg::HADMapRoute ] /planning/scenario_planning/trajectory [ autoware_auto_planning_msgs::msg::Trajectory ] /control/trajectory_follower/predicted_trajectory [ autoware_auto_planning_msgs::msg::Trajectory ] Output # [ diagnostic_updater ] lane_departure : Update diagnostic level when ego vehicle is out of lane. [ diagnostic_updater ] trajectory_deviation : Update diagnostic level when ego vehicle deviates from trajectory. Parameters # Node Parameters # Name Type Description Default value update_rate double Frequency for publishing [Hz] 10.0 visualize_lanelet bool Flag for visualizing lanelet False Core Parameters # Name Type Description Default value footprint_margin_scale double Coefficient for expanding footprint margin. Multiplied by 1 standard deviation. 1.0 resample_interval double Minimum Euclidean distance between points when resample trajectory.[m] 0.3 max_deceleration double Maximum deceleration when calculating braking distance. 2.8 delay_time double Delay time which took to actuate brake when calculating braking distance. [second] 1.3 max_lateral_deviation double Maximum lateral deviation in vehicle coordinate. [m] 2.0 max_longitudinal_deviation double Maximum longitudinal deviation in vehicle coordinate. [m] 2.0 max_yaw_deviation_deg double Maximum ego yaw deviation from trajectory. [deg] 60.0","title":"Lane Departure Checker"},{"location":"control/lane_departure_checker/#lane-departure-checker","text":"The Lane Departure Checker checks if vehicle follows a trajectory. If it does not follow the trajectory, it reports its status via diagnostic_updater .","title":"Lane Departure Checker"},{"location":"control/lane_departure_checker/#features","text":"This package includes the following features: Lane Departure : Check if ego vehicle is going to be out of lane boundaries based on output from control module (predicted trajectory). Trajectory Deviation : Check if ego vehicle's pose does not deviate from the trajectory. Checking lateral, longitudinal and yaw deviation.","title":"Features"},{"location":"control/lane_departure_checker/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"control/lane_departure_checker/#how-to-extend-footprint-by-covariance","text":"Calculate the standard deviation of error ellipse(covariance) in vehicle coordinate. 1.Transform covariance into vehicle coordinate. \\begin{align} \\left( \\begin{array}{cc} x_{vehicle}\\\\ y_{vehicle}\\\\ \\end{array} \\right) = R_{map2vehicle} \\left( \\begin{array}{cc} x_{map}\\\\ y_{map}\\\\ \\end{array} \\right) \\end{align} Calculate covariance in vehicle coordinate. \\begin{align} Cov_{vehicle} &= E \\left[ \\left( \\begin{array}{cc} x_{vehicle}\\\\ y_{vehicle}\\\\ \\end{array} \\right) (x_{vehicle}, y_{vehicle}) \\right] \\\\ &= E \\left[ R\\left( \\begin{array}{cc} x_{map}\\\\ y_{map}\\\\ \\end{array} \\right) (x_{map}, y_{map})R^t \\right] \\\\ &= R E\\left[ \\left( \\begin{array}{cc} x_{map}\\\\ y_{map}\\\\ \\end{array} \\right) (x_{map}, y_{map}) \\right] R^t \\\\ &= R Cov_{map} R^t \\end{align} 2.The longitudinal length we want to expand is correspond to marginal distribution of x_{vehicle} x_{vehicle} , which is represented in Cov_{vehicle}(0,0) Cov_{vehicle}(0,0) . In the same way, the lateral length is represented in Cov_{vehicle}(1,1) Cov_{vehicle}(1,1) . Wikipedia reference here . Expand footprint based on the standard deviation multiplied with footprint_margin_scale .","title":"How to extend footprint by covariance"},{"location":"control/lane_departure_checker/#interface","text":"","title":"Interface"},{"location":"control/lane_departure_checker/#input","text":"/localization/kinematic_state [ nav_msgs::msg::Odometry ] /map/vector_map [ autoware_auto_mapping_msgs::msg::HADMapBin ] /planning/mission_planning/route [ autoware_auto_planning_msgs::msg::HADMapRoute ] /planning/scenario_planning/trajectory [ autoware_auto_planning_msgs::msg::Trajectory ] /control/trajectory_follower/predicted_trajectory [ autoware_auto_planning_msgs::msg::Trajectory ]","title":"Input"},{"location":"control/lane_departure_checker/#output","text":"[ diagnostic_updater ] lane_departure : Update diagnostic level when ego vehicle is out of lane. [ diagnostic_updater ] trajectory_deviation : Update diagnostic level when ego vehicle deviates from trajectory.","title":"Output"},{"location":"control/lane_departure_checker/#parameters","text":"","title":"Parameters"},{"location":"control/lane_departure_checker/#node-parameters","text":"Name Type Description Default value update_rate double Frequency for publishing [Hz] 10.0 visualize_lanelet bool Flag for visualizing lanelet False","title":"Node Parameters"},{"location":"control/lane_departure_checker/#core-parameters","text":"Name Type Description Default value footprint_margin_scale double Coefficient for expanding footprint margin. Multiplied by 1 standard deviation. 1.0 resample_interval double Minimum Euclidean distance between points when resample trajectory.[m] 0.3 max_deceleration double Maximum deceleration when calculating braking distance. 2.8 delay_time double Delay time which took to actuate brake when calculating braking distance. [second] 1.3 max_lateral_deviation double Maximum lateral deviation in vehicle coordinate. [m] 2.0 max_longitudinal_deviation double Maximum longitudinal deviation in vehicle coordinate. [m] 2.0 max_yaw_deviation_deg double Maximum ego yaw deviation from trajectory. [deg] 60.0","title":"Core Parameters"},{"location":"control/obstacle_collision_checker/","text":"obstacle_collision_checker # Purpose # obstacle_collision_checker is a module to check obstacle collision for predicted trajectory and publish diagnostic errors if collision is found. Inner-workings / Algorithms # Flow chart # Algorithms # Check data # Check that obstacle_collision_checker receives no ground pointcloud, predicted_trajectory, reference trajectory, and current velocity data. Diagnostic update # If any collision is found on predicted path, this module sets ERROR level as diagnostic status else sets OK . Inputs / Outputs # Input # Name Type Description ~/input/trajectory autoware_auto_planning_msgs::msg::Trajectory Reference trajectory ~/input/trajectory autoware_auto_planning_msgs::msg::Trajectory Predicted trajectory /perception/obstacle_segmentation/pointcloud sensor_msgs::msg::PointCloud2 Pointcloud of obstacles which the ego-vehicle should stop or avoid /tf tf2_msgs::msg::TFMessage TF /tf_static tf2_msgs::msg::TFMessage TF static Output # Name Type Description ~/debug/marker visualization_msgs::msg::MarkerArray Marker for visualization Parameters # Name Type Description Default value delay_time double Delay time of vehicle [s] 0.3 footprint_margin double Foot print margin [m] 0.0 max_deceleration double Max deceleration for ego vehicle to stop [m/s^2] 2.0 resample_interval double Interval for resampling trajectory [m] 0.3 search_radius double Search distance from trajectory to point cloud [m] 5.0 Assumptions / Known limits # To perform proper collision check, it is necessary to get probably predicted trajectory and obstacle pointclouds without noise.","title":"obstacle_collision_checker"},{"location":"control/obstacle_collision_checker/#obstacle_collision_checker","text":"","title":"obstacle_collision_checker"},{"location":"control/obstacle_collision_checker/#purpose","text":"obstacle_collision_checker is a module to check obstacle collision for predicted trajectory and publish diagnostic errors if collision is found.","title":"Purpose"},{"location":"control/obstacle_collision_checker/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"control/obstacle_collision_checker/#flow-chart","text":"","title":"Flow chart"},{"location":"control/obstacle_collision_checker/#algorithms","text":"","title":"Algorithms"},{"location":"control/obstacle_collision_checker/#check-data","text":"Check that obstacle_collision_checker receives no ground pointcloud, predicted_trajectory, reference trajectory, and current velocity data.","title":"Check data"},{"location":"control/obstacle_collision_checker/#diagnostic-update","text":"If any collision is found on predicted path, this module sets ERROR level as diagnostic status else sets OK .","title":"Diagnostic update"},{"location":"control/obstacle_collision_checker/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"control/obstacle_collision_checker/#input","text":"Name Type Description ~/input/trajectory autoware_auto_planning_msgs::msg::Trajectory Reference trajectory ~/input/trajectory autoware_auto_planning_msgs::msg::Trajectory Predicted trajectory /perception/obstacle_segmentation/pointcloud sensor_msgs::msg::PointCloud2 Pointcloud of obstacles which the ego-vehicle should stop or avoid /tf tf2_msgs::msg::TFMessage TF /tf_static tf2_msgs::msg::TFMessage TF static","title":"Input"},{"location":"control/obstacle_collision_checker/#output","text":"Name Type Description ~/debug/marker visualization_msgs::msg::MarkerArray Marker for visualization","title":"Output"},{"location":"control/obstacle_collision_checker/#parameters","text":"Name Type Description Default value delay_time double Delay time of vehicle [s] 0.3 footprint_margin double Foot print margin [m] 0.0 max_deceleration double Max deceleration for ego vehicle to stop [m/s^2] 2.0 resample_interval double Interval for resampling trajectory [m] 0.3 search_radius double Search distance from trajectory to point cloud [m] 5.0","title":"Parameters"},{"location":"control/obstacle_collision_checker/#assumptions-known-limits","text":"To perform proper collision check, it is necessary to get probably predicted trajectory and obstacle pointclouds without noise.","title":"Assumptions / Known limits"},{"location":"control/shift_decider/","text":"Shift Decider # Purpose # shift_decider is a module to decide shift from ackermann control command. Inner-workings / Algorithms # Flow chart # Algorithms # Inputs / Outputs # Input # Name Type Description ~/input/control_cmd autoware_auto_control_msgs::msg::AckermannControlCommand Control command for vehicle. Output # Name Type Description ~output/shift_cmd autoware_auto_vehicle_msgs::msg::GearCommand Gear for drive forward / backward. Parameters # none. Assumptions / Known limits # TBD.","title":"Shift Decider"},{"location":"control/shift_decider/#shift-decider","text":"","title":"Shift Decider"},{"location":"control/shift_decider/#purpose","text":"shift_decider is a module to decide shift from ackermann control command.","title":"Purpose"},{"location":"control/shift_decider/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"control/shift_decider/#flow-chart","text":"","title":"Flow chart"},{"location":"control/shift_decider/#algorithms","text":"","title":"Algorithms"},{"location":"control/shift_decider/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"control/shift_decider/#input","text":"Name Type Description ~/input/control_cmd autoware_auto_control_msgs::msg::AckermannControlCommand Control command for vehicle.","title":"Input"},{"location":"control/shift_decider/#output","text":"Name Type Description ~output/shift_cmd autoware_auto_vehicle_msgs::msg::GearCommand Gear for drive forward / backward.","title":"Output"},{"location":"control/shift_decider/#parameters","text":"none.","title":"Parameters"},{"location":"control/shift_decider/#assumptions-known-limits","text":"TBD.","title":"Assumptions / Known limits"},{"location":"localization/ekf_localizer/","text":"Overview # The Extend Kalman Filter Localizer estimates robust and less noisy robot pose and twist by integrating the 2D vehicle dynamics model with input ego-pose and ego-twist messages. The algorithm is designed especially for fast moving robot such as autonomous driving system. Flowchart # The overall flowchart of the ekf_localizer is described below. Features # This package includes the following features: Time delay compensation for input messages, which enables proper integration of input information with varying time delay. This is important especially for high speed moving robot, such as autonomous driving vehicle. (see following figure). Automatic estimation of yaw bias prevents modeling errors caused by sensor mounting angle errors, which can improve estimation accuracy. Mahalanobis distance gate enables probabilistic outlier detection to determine which inputs should be used or ignored. Smooth update , the Kalman Filter measurement update is typically performed when a measurement is obtained, but it can cause large changes in the estimated value especially for low frequency measurements. Since the algorithm can consider the measurement time, the measurement data can be divided into multiple pieces and integrated smoothly while maintaining consistency (see following figure). Launch # The ekf_localizer starts with the default parameters with the following command. roslaunch ekf_localizer ekf_localizer.launch The parameters and input topic names can be set in the ekf_localizer.launch file. Node # Subscribed Topics # measured_pose_with_covariance (geometry_msgs/PoseWithCovarianceStamped) Input pose source with measurement covariance matrix. measured_twist_with_covariance (geometry_msgs/PoseWithCovarianceStamped) Input twist source with measurement covariance matrix. initialpose (geometry_msgs/PoseWithCovarianceStamped) Initial pose for EKF. The estimated pose is initialized with zeros at start. It is initialized with this message whenever published. Published Topics # ekf_odom (nav_msgs/Odometry) Estimated odometry. ekf_pose (geometry_msgs/PoseStamped) Estimated pose. ekf_pose_with_covariance (geometry_msgs/PoseWithCovarianceStamped) Estimated pose with covariance. ekf_pose_with_covariance (geometry_msgs/PoseStamped) Estimated pose without yawbias effect. ekf_pose_with_covariance_without_yawbias (geometry_msgs/PoseWithCovarianceStamped) Estimated pose with covariance without yawbias effect. ekf_twist (geometry_msgs/TwistStamped) Estimated twist. ekf_twist_with_covariance (geometry_msgs/TwistWithCovarianceStamped) Estimated twist with covariance. Published TF # base_link TF from \"map\" coordinate to estimated pose. Functions # Predict # The current robot state is predicted from previously estimated data using a given prediction model. This calculation is called at constant interval ( predict_frequency [Hz] ). The prediction equation is described at the end of this page. Measurement Update # Before update, the Mahalanobis distance is calculated between the measured input and the predicted state, the measurement update is not performed for inputs where the Mahalanobis distance exceeds the given threshold. The predicted state is updated with the latest measured inputs, measured_pose and measured_twist. The updates are performed with the same frequency as prediction, usually at a high frequency, in order to enable smooth state estimation. Parameter description # The parameters are set in launch/ekf_localizer.launch . For Node # Name Type Description Default value show_debug_info bool Flag to display debug info false predict_frequency double Frequency for filtering and publishing [Hz] 50.0 tf_rate double Frequency for tf broadcasting [Hz] 10.0 extend_state_step int Max delay step which can be dealt with in EKF. Large number increases computational cost. 50 enable_yaw_bias_estimation bool Flag to enable yaw bias estimation true For pose measurement # Name Type Description Default value pose_additional_delay double Additional delay time for pose measurement [s] 0.0 pose_measure_uncertainty_time double Measured time uncertainty used for covariance calculation [s] 0.01 pose_rate double Approximated input pose rate used for covariance calculation [Hz] 10.0 pose_gate_dist double Limit of Mahalanobis distance used for outliers detection 10000.0 For twist measurement # Name Type Description Default value twist_additional_delay double Additional delay time for twist [s] 0.0 twist_rate double Approximated input twist rate used for covariance calculation [Hz] 10.0 twist_gate_dist double Limit of Mahalanobis distance used for outliers detection 10000.0 For process noise # Name Type Description Default value proc_stddev_vx_c double Standard deviation of process noise in time differentiation expression of linear velocity x, noise for d_vx = 0 2.0 proc_stddev_wz_c double Standard deviation of process noise in time differentiation expression of angular velocity z, noise for d_wz = 0 0.2 proc_stddev_yaw_c double Standard deviation of process noise in time differentiation expression of yaw, noise for d_yaw = omega 0.005 proc_stddev_yaw_bias_c double Standard deviation of process noise in time differentiation expression of yaw_bias, noise for d_yaw_bias = 0 0.001 note: process noise for position x & y are calculated automatically from nonlinear dynamics. How to turn EKF parameters # 0. Preliminaries # Check header time in pose and twist message is set to sensor time appropriately, because time delay is calculated from this value. If it is difficult to set appropriate time due to timer synchronization problem, use twist_additional_delay and pose_additional_delay to correct the time. Check the relation between measurement pose and twist is appropriate (whether the derivative of pose has similar value to twist). This discrepancy is caused mainly by unit error (such as confusing radian/degree) or bias noise, and it causes large estimation errors. 1. Set sensor parameters # Set sensor-rate and standard-deviation from the basic information of the sensor. The pose_measure_uncertainty_time is for uncertainty of the header timestamp data. pose_measure_uncertainty_time pose_rate twist_rate 2. Set process model parameters # proc_stddev_vx_c : set to maximum linear acceleration proc_stddev_wz_c : set to maximum angular acceleration proc_stddev_yaw_c : This parameter describes the correlation between the yaw and yaw-rate. Large value means the change in yaw does not correlate to the estimated yaw-rate. If this is set to 0, it means the change in estimate yaw is equal to yaw-rate. Usually this should be set to 0. proc_stddev_yaw_bias_c : This parameter is the standard deviation for the rate of change in yaw bias. In most cases, yaw bias is constant, so it can be very small, but must be non-zero. Kalman Filter Model # kinematics model in update function # where b_k is the yaw-bias. time delay model # The measurement time delay is handled by an augmented states [1] (See, Section 7.3 FIXED-LAG SMOOTHING). Note that, although the dimension gets larger, since the analytical expansion can be applied based on the specific structures of the augmented states, the computational complexity does not significantly change. Test Result with Autoware NDT # reference # [1] Anderson, B. D. O., & Moore, J. B. (1979). Optimal filtering. Englewood Cliffs, NJ: Prentice-Hall.","title":"Overview"},{"location":"localization/ekf_localizer/#overview","text":"The Extend Kalman Filter Localizer estimates robust and less noisy robot pose and twist by integrating the 2D vehicle dynamics model with input ego-pose and ego-twist messages. The algorithm is designed especially for fast moving robot such as autonomous driving system.","title":"Overview"},{"location":"localization/ekf_localizer/#flowchart","text":"The overall flowchart of the ekf_localizer is described below.","title":"Flowchart"},{"location":"localization/ekf_localizer/#features","text":"This package includes the following features: Time delay compensation for input messages, which enables proper integration of input information with varying time delay. This is important especially for high speed moving robot, such as autonomous driving vehicle. (see following figure). Automatic estimation of yaw bias prevents modeling errors caused by sensor mounting angle errors, which can improve estimation accuracy. Mahalanobis distance gate enables probabilistic outlier detection to determine which inputs should be used or ignored. Smooth update , the Kalman Filter measurement update is typically performed when a measurement is obtained, but it can cause large changes in the estimated value especially for low frequency measurements. Since the algorithm can consider the measurement time, the measurement data can be divided into multiple pieces and integrated smoothly while maintaining consistency (see following figure).","title":"Features"},{"location":"localization/ekf_localizer/#launch","text":"The ekf_localizer starts with the default parameters with the following command. roslaunch ekf_localizer ekf_localizer.launch The parameters and input topic names can be set in the ekf_localizer.launch file.","title":"Launch"},{"location":"localization/ekf_localizer/#node","text":"","title":"Node"},{"location":"localization/ekf_localizer/#subscribed-topics","text":"measured_pose_with_covariance (geometry_msgs/PoseWithCovarianceStamped) Input pose source with measurement covariance matrix. measured_twist_with_covariance (geometry_msgs/PoseWithCovarianceStamped) Input twist source with measurement covariance matrix. initialpose (geometry_msgs/PoseWithCovarianceStamped) Initial pose for EKF. The estimated pose is initialized with zeros at start. It is initialized with this message whenever published.","title":"Subscribed Topics"},{"location":"localization/ekf_localizer/#published-topics","text":"ekf_odom (nav_msgs/Odometry) Estimated odometry. ekf_pose (geometry_msgs/PoseStamped) Estimated pose. ekf_pose_with_covariance (geometry_msgs/PoseWithCovarianceStamped) Estimated pose with covariance. ekf_pose_with_covariance (geometry_msgs/PoseStamped) Estimated pose without yawbias effect. ekf_pose_with_covariance_without_yawbias (geometry_msgs/PoseWithCovarianceStamped) Estimated pose with covariance without yawbias effect. ekf_twist (geometry_msgs/TwistStamped) Estimated twist. ekf_twist_with_covariance (geometry_msgs/TwistWithCovarianceStamped) Estimated twist with covariance.","title":"Published Topics"},{"location":"localization/ekf_localizer/#published-tf","text":"base_link TF from \"map\" coordinate to estimated pose.","title":"Published TF"},{"location":"localization/ekf_localizer/#functions","text":"","title":"Functions"},{"location":"localization/ekf_localizer/#predict","text":"The current robot state is predicted from previously estimated data using a given prediction model. This calculation is called at constant interval ( predict_frequency [Hz] ). The prediction equation is described at the end of this page.","title":"Predict"},{"location":"localization/ekf_localizer/#measurement-update","text":"Before update, the Mahalanobis distance is calculated between the measured input and the predicted state, the measurement update is not performed for inputs where the Mahalanobis distance exceeds the given threshold. The predicted state is updated with the latest measured inputs, measured_pose and measured_twist. The updates are performed with the same frequency as prediction, usually at a high frequency, in order to enable smooth state estimation.","title":"Measurement Update"},{"location":"localization/ekf_localizer/#parameter-description","text":"The parameters are set in launch/ekf_localizer.launch .","title":"Parameter description"},{"location":"localization/ekf_localizer/#for-node","text":"Name Type Description Default value show_debug_info bool Flag to display debug info false predict_frequency double Frequency for filtering and publishing [Hz] 50.0 tf_rate double Frequency for tf broadcasting [Hz] 10.0 extend_state_step int Max delay step which can be dealt with in EKF. Large number increases computational cost. 50 enable_yaw_bias_estimation bool Flag to enable yaw bias estimation true","title":"For Node"},{"location":"localization/ekf_localizer/#for-pose-measurement","text":"Name Type Description Default value pose_additional_delay double Additional delay time for pose measurement [s] 0.0 pose_measure_uncertainty_time double Measured time uncertainty used for covariance calculation [s] 0.01 pose_rate double Approximated input pose rate used for covariance calculation [Hz] 10.0 pose_gate_dist double Limit of Mahalanobis distance used for outliers detection 10000.0","title":"For pose measurement"},{"location":"localization/ekf_localizer/#for-twist-measurement","text":"Name Type Description Default value twist_additional_delay double Additional delay time for twist [s] 0.0 twist_rate double Approximated input twist rate used for covariance calculation [Hz] 10.0 twist_gate_dist double Limit of Mahalanobis distance used for outliers detection 10000.0","title":"For twist measurement"},{"location":"localization/ekf_localizer/#for-process-noise","text":"Name Type Description Default value proc_stddev_vx_c double Standard deviation of process noise in time differentiation expression of linear velocity x, noise for d_vx = 0 2.0 proc_stddev_wz_c double Standard deviation of process noise in time differentiation expression of angular velocity z, noise for d_wz = 0 0.2 proc_stddev_yaw_c double Standard deviation of process noise in time differentiation expression of yaw, noise for d_yaw = omega 0.005 proc_stddev_yaw_bias_c double Standard deviation of process noise in time differentiation expression of yaw_bias, noise for d_yaw_bias = 0 0.001 note: process noise for position x & y are calculated automatically from nonlinear dynamics.","title":"For process noise"},{"location":"localization/ekf_localizer/#how-to-turn-ekf-parameters","text":"","title":"How to turn EKF parameters"},{"location":"localization/ekf_localizer/#0-preliminaries","text":"Check header time in pose and twist message is set to sensor time appropriately, because time delay is calculated from this value. If it is difficult to set appropriate time due to timer synchronization problem, use twist_additional_delay and pose_additional_delay to correct the time. Check the relation between measurement pose and twist is appropriate (whether the derivative of pose has similar value to twist). This discrepancy is caused mainly by unit error (such as confusing radian/degree) or bias noise, and it causes large estimation errors.","title":"0. Preliminaries"},{"location":"localization/ekf_localizer/#1-set-sensor-parameters","text":"Set sensor-rate and standard-deviation from the basic information of the sensor. The pose_measure_uncertainty_time is for uncertainty of the header timestamp data. pose_measure_uncertainty_time pose_rate twist_rate","title":"1. Set sensor parameters"},{"location":"localization/ekf_localizer/#2-set-process-model-parameters","text":"proc_stddev_vx_c : set to maximum linear acceleration proc_stddev_wz_c : set to maximum angular acceleration proc_stddev_yaw_c : This parameter describes the correlation between the yaw and yaw-rate. Large value means the change in yaw does not correlate to the estimated yaw-rate. If this is set to 0, it means the change in estimate yaw is equal to yaw-rate. Usually this should be set to 0. proc_stddev_yaw_bias_c : This parameter is the standard deviation for the rate of change in yaw bias. In most cases, yaw bias is constant, so it can be very small, but must be non-zero.","title":"2. Set process model parameters"},{"location":"localization/ekf_localizer/#kalman-filter-model","text":"","title":"Kalman Filter Model"},{"location":"localization/ekf_localizer/#kinematics-model-in-update-function","text":"where b_k is the yaw-bias.","title":"kinematics model in update function"},{"location":"localization/ekf_localizer/#time-delay-model","text":"The measurement time delay is handled by an augmented states [1] (See, Section 7.3 FIXED-LAG SMOOTHING). Note that, although the dimension gets larger, since the analytical expansion can be applied based on the specific structures of the augmented states, the computational complexity does not significantly change.","title":"time delay model"},{"location":"localization/ekf_localizer/#test-result-with-autoware-ndt","text":"","title":"Test Result with Autoware NDT"},{"location":"localization/ekf_localizer/#reference","text":"[1] Anderson, B. D. O., & Moore, J. B. (1979). Optimal filtering. Englewood Cliffs, NJ: Prentice-Hall.","title":"reference"},{"location":"localization/gyro_odometer/","text":"gyro_odometer # Purpose # gyro_odometer is the package to estimate twist by combining imu and vehicle speed. Inputs / Outputs # Input # Name Type Description vehicle/twist_with_covariance geometry_msgs::msg::TwistWithCovarianceStamped twist with covariance from vehicle imu sensor_msgs::msg::Imu imu from sensor Output # Name Type Description twist geometry_msgs::msg::TwistStamped estimated twist twist_with_covariance geometry_msgs::msg::TwistWithCovarianceStamped estimated twist with covariance Parameters # Parameter Type Description output_frame String output's frame id Assumptions / Known limits # TBD.","title":"gyro_odometer"},{"location":"localization/gyro_odometer/#gyro_odometer","text":"","title":"gyro_odometer"},{"location":"localization/gyro_odometer/#purpose","text":"gyro_odometer is the package to estimate twist by combining imu and vehicle speed.","title":"Purpose"},{"location":"localization/gyro_odometer/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"localization/gyro_odometer/#input","text":"Name Type Description vehicle/twist_with_covariance geometry_msgs::msg::TwistWithCovarianceStamped twist with covariance from vehicle imu sensor_msgs::msg::Imu imu from sensor","title":"Input"},{"location":"localization/gyro_odometer/#output","text":"Name Type Description twist geometry_msgs::msg::TwistStamped estimated twist twist_with_covariance geometry_msgs::msg::TwistWithCovarianceStamped estimated twist with covariance","title":"Output"},{"location":"localization/gyro_odometer/#parameters","text":"Parameter Type Description output_frame String output's frame id","title":"Parameters"},{"location":"localization/gyro_odometer/#assumptions-known-limits","text":"TBD.","title":"Assumptions / Known limits"},{"location":"localization/localization_error_monitor/","text":"localization_error_monitor # Purpose # localization_error_monitor is a package for diagnosing localization errors by monitoring uncertainty of the localization results. The package monitors the following two values: size of long radius of confidence ellipse size of confidence ellipse along lateral direction (body-frame) Inputs / Outputs # Input # Name Type Description input/pose_with_cov geometry_msgs::msg::PoseWithCovarianceStamped localization result Output # Name Type Description debug/ellipse_marker visualization_msgs::msg::Marker ellipse marker diagnostics diagnostic_msgs::msg::DiagnosticArray diagnostics outputs Parameters # Name Type Description scale double scale factor for monitored values (default: 3.0) error_ellipse_size double error threshold for long radius of confidence ellipse [m] (default: 1.0) warn_ellipse_size double warning threshold for long radius of confidence ellipse [m] (default: 0.8) error_ellipse_size_lateral_direction double error threshold for size of confidence ellipse along lateral direction [m] (default: 0.3) warn_ellipse_size_lateral_direction double warning threshold for size of confidence ellipse along lateral direction [m] (default: 0.2)","title":"localization_error_monitor"},{"location":"localization/localization_error_monitor/#localization_error_monitor","text":"","title":"localization_error_monitor"},{"location":"localization/localization_error_monitor/#purpose","text":"localization_error_monitor is a package for diagnosing localization errors by monitoring uncertainty of the localization results. The package monitors the following two values: size of long radius of confidence ellipse size of confidence ellipse along lateral direction (body-frame)","title":"Purpose"},{"location":"localization/localization_error_monitor/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"localization/localization_error_monitor/#input","text":"Name Type Description input/pose_with_cov geometry_msgs::msg::PoseWithCovarianceStamped localization result","title":"Input"},{"location":"localization/localization_error_monitor/#output","text":"Name Type Description debug/ellipse_marker visualization_msgs::msg::Marker ellipse marker diagnostics diagnostic_msgs::msg::DiagnosticArray diagnostics outputs","title":"Output"},{"location":"localization/localization_error_monitor/#parameters","text":"Name Type Description scale double scale factor for monitored values (default: 3.0) error_ellipse_size double error threshold for long radius of confidence ellipse [m] (default: 1.0) warn_ellipse_size double warning threshold for long radius of confidence ellipse [m] (default: 0.8) error_ellipse_size_lateral_direction double error threshold for size of confidence ellipse along lateral direction [m] (default: 0.3) warn_ellipse_size_lateral_direction double warning threshold for size of confidence ellipse along lateral direction [m] (default: 0.2)","title":"Parameters"},{"location":"localization/ndt/","text":"ndt # This package aims to absorb the differences of several NDT implementations and provide a common interface.","title":"ndt"},{"location":"localization/ndt/#ndt","text":"This package aims to absorb the differences of several NDT implementations and provide a common interface.","title":"ndt"},{"location":"localization/ndt_pcl_modified/","text":"ndt_pcl_modified # Purpose # This is a modification of PCL 's NDT. Modifications # You can get the Hessian matrix by getHessian(). You can get the estimated position for each iteration by getFinalTransformationArray(). It optimizes rotational axes first, then jointly optimizes rotational and translational axes. [experimental feature]","title":"ndt_pcl_modified"},{"location":"localization/ndt_pcl_modified/#ndt_pcl_modified","text":"","title":"ndt_pcl_modified"},{"location":"localization/ndt_pcl_modified/#purpose","text":"This is a modification of PCL 's NDT.","title":"Purpose"},{"location":"localization/ndt_pcl_modified/#modifications","text":"You can get the Hessian matrix by getHessian(). You can get the estimated position for each iteration by getFinalTransformationArray(). It optimizes rotational axes first, then jointly optimizes rotational and translational axes. [experimental feature]","title":"Modifications"},{"location":"localization/ndt_scan_matcher/","text":"ndt_scan_matcher # Purpose # ndt_scan_matcher is a package for position estimation using the NDT scan matching method. There are two main functions in this package: estimate position by scan matching estimate initial position via the ROS service using the Monte Carlo method Inputs / Outputs # Input # Name Type Description ekf_pose_with_covariance geometry_msgs::msg::PoseWithCovarianceStamped initial pose pointcloud_map sensor_msgs::msg::PointCloud2 map pointcloud points_raw sensor_msgs::msg::PointCloud2 sensor pointcloud Output # Name Type Description ndt_pose geometry_msgs::msg::PoseStamped estimated pose ndt_pose_with_covariance geometry_msgs::msg::PoseWithCovarianceStamped estimated pose with covariance /diagnostics diagnostic_msgs::msg::DiagnosticArray diagnostics points_aligned sensor_msgs::msg::PointCloud2 [debug topic] pointcloud aligned by scan matching initial_pose_with_covariance geometry_msgs::msg::PoseWithCovarianceStamped [debug topic] initial pose used in scan matching exe_time_ms autoware_debug_msgs::msg::Float32Stamped [debug topic] execution time for scan matching [ms] transform_probability autoware_debug_msgs::msg::Float32Stamped [debug topic] score of scan matching iteration_num autoware_debug_msgs::msg::Int32Stamped [debug topic] number of scan matching iterations initial_to_result_distance autoware_debug_msgs::msg::Float32Stamped [debug topic] distance difference between the initial point and the convergence point [m] initial_to_result_distance_old autoware_debug_msgs::msg::Float32Stamped [debug topic] distance difference between the older of the two initial points used in linear interpolation and the convergence point [m] initial_to_result_distance_new autoware_debug_msgs::msg::Float32Stamped [debug topic] distance difference between the newer of the two initial points used in linear interpolation and the convergence point [m] ndt_marker visualization_msgs::msg::MarkerArray [debug topic] markers for debugging monte_carlo_initial_pose_marker visualization_msgs::msg::MarkerArray [debug topic] particles used in initial position estimation Service # Name Type Description ndt_align_srv autoware_localization_srvs::srv::PoseWithCovarianceStamped service to estimate initial pose Parameters # Core Parameters # Name Type Description base_frame string Vehicle reference frame input_sensor_points_queue_size int Subscriber queue size ndt_implement_type int NDT implementation type (0=PCL_GENERIC, 1=PCL_MODIFIED, 2=OMP) trans_epsilon double The maximum difference between two consecutive transformations in order to consider convergence step_size double The newton line search maximum step length resolution double The ND voxel grid resolution [m] max_iterations int The number of iterations required to calculate alignment converged_param_transform_probability double Threshold for deciding whether to trust the estimation result omp_neighborhood_search_method int neighborhood search method in OMP (0=KDTREE, 1=DIRECT26, 2=DIRECT7, 3=DIRECT1) omp_num_threads int Number of threads used for parallel computing","title":"ndt_scan_matcher"},{"location":"localization/ndt_scan_matcher/#ndt_scan_matcher","text":"","title":"ndt_scan_matcher"},{"location":"localization/ndt_scan_matcher/#purpose","text":"ndt_scan_matcher is a package for position estimation using the NDT scan matching method. There are two main functions in this package: estimate position by scan matching estimate initial position via the ROS service using the Monte Carlo method","title":"Purpose"},{"location":"localization/ndt_scan_matcher/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"localization/ndt_scan_matcher/#input","text":"Name Type Description ekf_pose_with_covariance geometry_msgs::msg::PoseWithCovarianceStamped initial pose pointcloud_map sensor_msgs::msg::PointCloud2 map pointcloud points_raw sensor_msgs::msg::PointCloud2 sensor pointcloud","title":"Input"},{"location":"localization/ndt_scan_matcher/#output","text":"Name Type Description ndt_pose geometry_msgs::msg::PoseStamped estimated pose ndt_pose_with_covariance geometry_msgs::msg::PoseWithCovarianceStamped estimated pose with covariance /diagnostics diagnostic_msgs::msg::DiagnosticArray diagnostics points_aligned sensor_msgs::msg::PointCloud2 [debug topic] pointcloud aligned by scan matching initial_pose_with_covariance geometry_msgs::msg::PoseWithCovarianceStamped [debug topic] initial pose used in scan matching exe_time_ms autoware_debug_msgs::msg::Float32Stamped [debug topic] execution time for scan matching [ms] transform_probability autoware_debug_msgs::msg::Float32Stamped [debug topic] score of scan matching iteration_num autoware_debug_msgs::msg::Int32Stamped [debug topic] number of scan matching iterations initial_to_result_distance autoware_debug_msgs::msg::Float32Stamped [debug topic] distance difference between the initial point and the convergence point [m] initial_to_result_distance_old autoware_debug_msgs::msg::Float32Stamped [debug topic] distance difference between the older of the two initial points used in linear interpolation and the convergence point [m] initial_to_result_distance_new autoware_debug_msgs::msg::Float32Stamped [debug topic] distance difference between the newer of the two initial points used in linear interpolation and the convergence point [m] ndt_marker visualization_msgs::msg::MarkerArray [debug topic] markers for debugging monte_carlo_initial_pose_marker visualization_msgs::msg::MarkerArray [debug topic] particles used in initial position estimation","title":"Output"},{"location":"localization/ndt_scan_matcher/#service","text":"Name Type Description ndt_align_srv autoware_localization_srvs::srv::PoseWithCovarianceStamped service to estimate initial pose","title":"Service"},{"location":"localization/ndt_scan_matcher/#parameters","text":"","title":"Parameters"},{"location":"localization/ndt_scan_matcher/#core-parameters","text":"Name Type Description base_frame string Vehicle reference frame input_sensor_points_queue_size int Subscriber queue size ndt_implement_type int NDT implementation type (0=PCL_GENERIC, 1=PCL_MODIFIED, 2=OMP) trans_epsilon double The maximum difference between two consecutive transformations in order to consider convergence step_size double The newton line search maximum step length resolution double The ND voxel grid resolution [m] max_iterations int The number of iterations required to calculate alignment converged_param_transform_probability double Threshold for deciding whether to trust the estimation result omp_neighborhood_search_method int neighborhood search method in OMP (0=KDTREE, 1=DIRECT26, 2=DIRECT7, 3=DIRECT1) omp_num_threads int Number of threads used for parallel computing","title":"Core Parameters"},{"location":"localization/pose2twist/","text":"pose2twist # Purpose # This pose2twist calculates the velocity from the input pose history. In addition to the computed twist, this node outputs the linear-x and angular-z components as a float message to simplify debugging. The twist.linear.x is calculated as sqrt(dx * dx + dy * dy + dz * dz) / dt , and the values in the y and z fields are zero. The twist.angular is calculated as d_roll / dt , d_pitch / dt and d_yaw / dt for each field. Inputs / Outputs # Input # Name Type Description pose geometry_msgs::msg::PoseStamped pose source to used for the velocity calculation. Output # Name Type Description twist geometry_msgs::msg::TwistStamped twist calculated from the input pose history. linear_x autoware_debug_msgs::msg::Float32Stamped linear-x field of the output twist. angular_z autoware_debug_msgs::msg::Float32Stamped angular-z field of the output twist. Parameters # none. Assumptions / Known limits # none.","title":"pose2twist"},{"location":"localization/pose2twist/#pose2twist","text":"","title":"pose2twist"},{"location":"localization/pose2twist/#purpose","text":"This pose2twist calculates the velocity from the input pose history. In addition to the computed twist, this node outputs the linear-x and angular-z components as a float message to simplify debugging. The twist.linear.x is calculated as sqrt(dx * dx + dy * dy + dz * dz) / dt , and the values in the y and z fields are zero. The twist.angular is calculated as d_roll / dt , d_pitch / dt and d_yaw / dt for each field.","title":"Purpose"},{"location":"localization/pose2twist/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"localization/pose2twist/#input","text":"Name Type Description pose geometry_msgs::msg::PoseStamped pose source to used for the velocity calculation.","title":"Input"},{"location":"localization/pose2twist/#output","text":"Name Type Description twist geometry_msgs::msg::TwistStamped twist calculated from the input pose history. linear_x autoware_debug_msgs::msg::Float32Stamped linear-x field of the output twist. angular_z autoware_debug_msgs::msg::Float32Stamped angular-z field of the output twist.","title":"Output"},{"location":"localization/pose2twist/#parameters","text":"none.","title":"Parameters"},{"location":"localization/pose2twist/#assumptions-known-limits","text":"none.","title":"Assumptions / Known limits"},{"location":"localization/stop_filter/","text":"stop_filter # Purpose # When this function did not exist, each node used a different criterion to determine whether the vehicle is stopping or not, resulting that some nodes were in operation of stopping the vehicle and some nodes continued running in the drive mode. This node aims to: apply a uniform stopping decision criterion to several nodes. suppress the control noise by overwriting the velocity and angular velocity with zero. Inputs / Outputs # Input # Name Type Description input/odom nav_msgs::msg::Odometry localization odometry Output # Name Type Description output/odom nav_msgs::msg::Odometry odometry with suppressed longitudinal and yaw twist debug/stop_flag autoware_debug_msgs::msg::BoolStamped flag to represent whether the vehicle is stopping or not Parameters # Name Type Description vx_threshold double longitudinal velocity threshold to determine if the vehicle is stopping [m/s] (default: 0.01) wz_threshold double yaw velocity threshold to determine if the vehicle is stopping [rad/s] (default: 0.01)","title":"stop_filter"},{"location":"localization/stop_filter/#stop_filter","text":"","title":"stop_filter"},{"location":"localization/stop_filter/#purpose","text":"When this function did not exist, each node used a different criterion to determine whether the vehicle is stopping or not, resulting that some nodes were in operation of stopping the vehicle and some nodes continued running in the drive mode. This node aims to: apply a uniform stopping decision criterion to several nodes. suppress the control noise by overwriting the velocity and angular velocity with zero.","title":"Purpose"},{"location":"localization/stop_filter/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"localization/stop_filter/#input","text":"Name Type Description input/odom nav_msgs::msg::Odometry localization odometry","title":"Input"},{"location":"localization/stop_filter/#output","text":"Name Type Description output/odom nav_msgs::msg::Odometry odometry with suppressed longitudinal and yaw twist debug/stop_flag autoware_debug_msgs::msg::BoolStamped flag to represent whether the vehicle is stopping or not","title":"Output"},{"location":"localization/stop_filter/#parameters","text":"Name Type Description vx_threshold double longitudinal velocity threshold to determine if the vehicle is stopping [m/s] (default: 0.01) wz_threshold double yaw velocity threshold to determine if the vehicle is stopping [rad/s] (default: 0.01)","title":"Parameters"},{"location":"localization/vehicle_velocity_converter/","text":"vehicle_velocity_converter # Purpose # This package is converted autoware_auto_vehicle_msgs::msg::VehicleReport message to geometry_msgs::msg::TwistWithCovarianceStamped for gyro odometer node. Inputs / Outputs # Input # Name Type Description velocity_status autoware_auto_vehicle_msgs::msg::VehicleReport vehicle velocity Output # Name Type Description twist_with_covariance geometry_msgs::msg::TwistWithCovarianceStamped twist with covariance converted from VehicleReport Parameters # Name Type Description frame_id string frame id for output message covariance double set covariance value to the twist message","title":"vehicle_velocity_converter"},{"location":"localization/vehicle_velocity_converter/#vehicle_velocity_converter","text":"","title":"vehicle_velocity_converter"},{"location":"localization/vehicle_velocity_converter/#purpose","text":"This package is converted autoware_auto_vehicle_msgs::msg::VehicleReport message to geometry_msgs::msg::TwistWithCovarianceStamped for gyro odometer node.","title":"Purpose"},{"location":"localization/vehicle_velocity_converter/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"localization/vehicle_velocity_converter/#input","text":"Name Type Description velocity_status autoware_auto_vehicle_msgs::msg::VehicleReport vehicle velocity","title":"Input"},{"location":"localization/vehicle_velocity_converter/#output","text":"Name Type Description twist_with_covariance geometry_msgs::msg::TwistWithCovarianceStamped twist with covariance converted from VehicleReport","title":"Output"},{"location":"localization/vehicle_velocity_converter/#parameters","text":"Name Type Description frame_id string frame id for output message covariance double set covariance value to the twist message","title":"Parameters"},{"location":"map/lanelet2_extension/","text":"lanelet2_extension package # This package contains external library for Lanelet2 and is meant to ease the use of Lanelet2 in Autoware. Lanelet Format for Autoware # Autoware uses extended Lanelet2 Format for Autoware, which means you need to add some tags to default OSM file if you want to fully use Lanelet2 maps. For details about custom tags, please refer to this document . Code API # IO # Autoware OSM Parser # Autoware Lanelet2 Format uses .osm extension as original Lanelet2. However, there are some custom tags that is used by the parser. Currently, this includes: overwriting x,y values with local_x and local_y tags. reading <MapMetaInfo> tag which contains information about map format version and map version. The parser is registered as \"autoware_osm_handler\" as lanelet parser Projection # MGRS Projector # MGRS projector projects latitude longitude into MGRS Coordinates. Regulatory Elements # Autoware Traffic Light # Autoware Traffic Light class allows you to retrieve information about traffic lights. Autoware Traffic Light class contains following members: traffic light shape light bulbs information of traffic lights stopline associated to traffic light Utility # Message Conversion # This contains functions to convert lanelet map objects into ROS messages. Currently it contains following conversions: lanelet::LaneletMapPtr to/from autoware_auto_mapping_msgs::msg::HADMapBin lanelet::Point3d to geometry_msgs::Point lanelet::Point2d to geometry_msgs::Point lanelet::BasicPoint3d to geometry_msgs::Point Query # This module contains functions to retrieve various information from maps. e.g. crosswalks, trafficlights, stoplines Utilities # This module contains other useful functions related to Lanelet. e.g. matching waypoint with lanelets Visualization # Visualization contains functions to convert lanelet objects into visualization marker messages. Currently it contains following conversions: lanelet::Lanelet to Triangle Markers lanelet::LineString to LineStrip Markers TrafficLights to Triangle Markers Nodes # lanelet2_extension_sample # Code for this explains how this lanelet2_extension library is used. The executable is not meant to do anything. autoware_lanelet2_extension # This node checks if an .osm file follows the Autoware version of Lanelet2 format. You can check by running: ros2 run lanelet2_extension autoware_lanelet2_validation _map_file: = <path/to/map.osm>","title":"lanelet2_extension package"},{"location":"map/lanelet2_extension/#lanelet2_extension-package","text":"This package contains external library for Lanelet2 and is meant to ease the use of Lanelet2 in Autoware.","title":"lanelet2_extension package"},{"location":"map/lanelet2_extension/#lanelet-format-for-autoware","text":"Autoware uses extended Lanelet2 Format for Autoware, which means you need to add some tags to default OSM file if you want to fully use Lanelet2 maps. For details about custom tags, please refer to this document .","title":"Lanelet Format for Autoware"},{"location":"map/lanelet2_extension/#code-api","text":"","title":"Code API"},{"location":"map/lanelet2_extension/#io","text":"","title":"IO"},{"location":"map/lanelet2_extension/#autoware-osm-parser","text":"Autoware Lanelet2 Format uses .osm extension as original Lanelet2. However, there are some custom tags that is used by the parser. Currently, this includes: overwriting x,y values with local_x and local_y tags. reading <MapMetaInfo> tag which contains information about map format version and map version. The parser is registered as \"autoware_osm_handler\" as lanelet parser","title":"Autoware OSM Parser"},{"location":"map/lanelet2_extension/#projection","text":"","title":"Projection"},{"location":"map/lanelet2_extension/#mgrs-projector","text":"MGRS projector projects latitude longitude into MGRS Coordinates.","title":"MGRS Projector"},{"location":"map/lanelet2_extension/#regulatory-elements","text":"","title":"Regulatory Elements"},{"location":"map/lanelet2_extension/#autoware-traffic-light","text":"Autoware Traffic Light class allows you to retrieve information about traffic lights. Autoware Traffic Light class contains following members: traffic light shape light bulbs information of traffic lights stopline associated to traffic light","title":"Autoware Traffic Light"},{"location":"map/lanelet2_extension/#utility","text":"","title":"Utility"},{"location":"map/lanelet2_extension/#message-conversion","text":"This contains functions to convert lanelet map objects into ROS messages. Currently it contains following conversions: lanelet::LaneletMapPtr to/from autoware_auto_mapping_msgs::msg::HADMapBin lanelet::Point3d to geometry_msgs::Point lanelet::Point2d to geometry_msgs::Point lanelet::BasicPoint3d to geometry_msgs::Point","title":"Message Conversion"},{"location":"map/lanelet2_extension/#query","text":"This module contains functions to retrieve various information from maps. e.g. crosswalks, trafficlights, stoplines","title":"Query"},{"location":"map/lanelet2_extension/#utilities","text":"This module contains other useful functions related to Lanelet. e.g. matching waypoint with lanelets","title":"Utilities"},{"location":"map/lanelet2_extension/#visualization","text":"Visualization contains functions to convert lanelet objects into visualization marker messages. Currently it contains following conversions: lanelet::Lanelet to Triangle Markers lanelet::LineString to LineStrip Markers TrafficLights to Triangle Markers","title":"Visualization"},{"location":"map/lanelet2_extension/#nodes","text":"","title":"Nodes"},{"location":"map/lanelet2_extension/#lanelet2_extension_sample","text":"Code for this explains how this lanelet2_extension library is used. The executable is not meant to do anything.","title":"lanelet2_extension_sample"},{"location":"map/lanelet2_extension/#autoware_lanelet2_extension","text":"This node checks if an .osm file follows the Autoware version of Lanelet2 format. You can check by running: ros2 run lanelet2_extension autoware_lanelet2_validation _map_file: = <path/to/map.osm>","title":"autoware_lanelet2_extension"},{"location":"map/lanelet2_extension/docs/extra_lanelet_subtypes/","text":"Extra Lanelet Subtypes # Roadside Lane # The subtypes for this lanelet classify the outer lanes adjacent to the driving lane.Since the list of lanelet subtypes defined in this link cannot represent the shoulder lane and pedestrian lane described below, two new subtypes are defined.When parking on the street, it is necessary to distinguish between a shoulder lane which can be used by vehicles, and a pedestrian lane which can be used by pedestrians and bicycles.If you park in a shoulder lane, you can use the entire lane for temporary parking, but if you park in a pedestrian lane, you must leave a space of at least 75cm. Road shoulder subtype # refers: lanelet with subtype attribute. Subtype explains what the type of roadside it represents. If there is an area outside of this roadside lane that is open to traffic, such as a sidewalk or bike lane, select the road_shoulder subtype. Sample road shoulder in .osm format is shown below: <relation id= \"120700\" > <member type= \"way\" role= \"left\" ref= \"34577\" /> <member type= \"way\" role= \"right\" ref= \"120694\" /> <tag k= \"type\" v= \"lanelet\" /> <tag k= \"subtype\" v= \"road_shoulder\" /> <tag k= \"speed_limit\" v= \"10\" /> <tag k= \"location\" v= \"urban\" /> <tag k= \"one_way\" v= \"yes\" /> </relation> Pedestrian lane subtype # refers: lanelet with subtype attribute. Subtype explains what the type of roadside it represents. If there are no passable areas outside of this roadside lane, select the pedestrian_lane subtype. Sample pedestrian lane in .osm format is shown below: <relation id= \"120700\" > <member type= \"way\" role= \"left\" ref= \"34577\" /> <member type= \"way\" role= \"right\" ref= \"120694\" /> <tag k= \"type\" v= \"lanelet\" /> <tag k= \"subtype\" v= \"pedestrian_lane\" /> <tag k= \"speed_limit\" v= \"10\" /> <tag k= \"location\" v= \"urban\" /> <tag k= \"one_way\" v= \"yes\" /> </relation>","title":"Extra Lanelet Subtypes"},{"location":"map/lanelet2_extension/docs/extra_lanelet_subtypes/#extra-lanelet-subtypes","text":"","title":"Extra Lanelet Subtypes"},{"location":"map/lanelet2_extension/docs/extra_lanelet_subtypes/#roadside-lane","text":"The subtypes for this lanelet classify the outer lanes adjacent to the driving lane.Since the list of lanelet subtypes defined in this link cannot represent the shoulder lane and pedestrian lane described below, two new subtypes are defined.When parking on the street, it is necessary to distinguish between a shoulder lane which can be used by vehicles, and a pedestrian lane which can be used by pedestrians and bicycles.If you park in a shoulder lane, you can use the entire lane for temporary parking, but if you park in a pedestrian lane, you must leave a space of at least 75cm.","title":"Roadside Lane"},{"location":"map/lanelet2_extension/docs/extra_lanelet_subtypes/#road-shoulder-subtype","text":"refers: lanelet with subtype attribute. Subtype explains what the type of roadside it represents. If there is an area outside of this roadside lane that is open to traffic, such as a sidewalk or bike lane, select the road_shoulder subtype. Sample road shoulder in .osm format is shown below: <relation id= \"120700\" > <member type= \"way\" role= \"left\" ref= \"34577\" /> <member type= \"way\" role= \"right\" ref= \"120694\" /> <tag k= \"type\" v= \"lanelet\" /> <tag k= \"subtype\" v= \"road_shoulder\" /> <tag k= \"speed_limit\" v= \"10\" /> <tag k= \"location\" v= \"urban\" /> <tag k= \"one_way\" v= \"yes\" /> </relation>","title":"Road shoulder subtype"},{"location":"map/lanelet2_extension/docs/extra_lanelet_subtypes/#pedestrian-lane-subtype","text":"refers: lanelet with subtype attribute. Subtype explains what the type of roadside it represents. If there are no passable areas outside of this roadside lane, select the pedestrian_lane subtype. Sample pedestrian lane in .osm format is shown below: <relation id= \"120700\" > <member type= \"way\" role= \"left\" ref= \"34577\" /> <member type= \"way\" role= \"right\" ref= \"120694\" /> <tag k= \"type\" v= \"lanelet\" /> <tag k= \"subtype\" v= \"pedestrian_lane\" /> <tag k= \"speed_limit\" v= \"10\" /> <tag k= \"location\" v= \"urban\" /> <tag k= \"one_way\" v= \"yes\" /> </relation>","title":"Pedestrian lane subtype"},{"location":"map/lanelet2_extension/docs/extra_regulatory_elements/","text":"Extra Regulatory Elements # Detection Area # This regulatory element specifies region of interest which vehicle must pay attention whenever it is driving along the associated lanelet. When there are any obstacle in the detection area, vehicle must stop at specified stopline. refers: refers to detection area polygon. There could be multiple detection areas registered to a single regulatory element. refline: refers to stop line of the detection area Sample detection area in .osm format is shown below: <node id= 1 version= '1' lat= '49.00541994701' lon= '8.41565013855' > <tag k= \u2019ele\u2019 v= \u20190\u2019/ > </node> <node id= 2 version= '1' lat= '49.00542091657' lon= '8.4156469497' > <tag k= \u2019ele\u2019 v= \u20190\u2019/ > </node> <node id= 3 version= '1' lat= '49.00542180052' lon= '8.41564400223' > <tag k= \u2019ele\u2019 v= \u20190\u2019/ > </node> <node id= 4 version= '1' lat= '49.00541994701' lon= '8.41564400223' > <tag k= \u2019ele\u2019 v= \u20190\u2019/ > </node> <node id= 5 version= '1' lat= '49.00542180052' lon= '8.41564400223' > <tag k= \u2019ele\u2019 v= \u20190\u2019/ > </node> <node id= 6 version= '1' lat= '49.00541994701' lon= '8.41564400223' > <tag k= \u2019ele\u2019 v= \u20190\u2019/ > </node> <way id= 11 version= '1' > <nd ref= 1 /> <nd ref= 2 /> <nd ref= 3 /> <nd ref= 4 /> <nd ref= 1 /> <tag k= 'type' v= \u2019detection_area\u2019 /> <tag k= 'area' v= \u2019yes\u2019 /> </way> <way id= 12 version= \"1\" > <nd ref= 5 /> <nd ref= 6 /> <tag k= 'type' v= stop_line\u2019 /> </way> <relation id= \"13\" > <tag k= \"type\" v= \"regulatory_element\" /> <tag k= \"subtype\" v= \"detection_area\" /> <member type= \"way\" ref= \"11\" role= \"refers\" /> <member type= \"way\" ref= \"12\" role= \"ref_line\" /> </relation> Road Marking # This regulatory element specifies related road markings to a lanelet as shown below. * Note that the stopline in the image is for stoplines that are for reference, and normal stoplines should be expressed using TrafficSign regulatory element. refers: linestring with type attribute. Type explains what road marking it represents (e.g. stopline).","title":"Extra Regulatory Elements"},{"location":"map/lanelet2_extension/docs/extra_regulatory_elements/#extra-regulatory-elements","text":"","title":"Extra Regulatory Elements"},{"location":"map/lanelet2_extension/docs/extra_regulatory_elements/#detection-area","text":"This regulatory element specifies region of interest which vehicle must pay attention whenever it is driving along the associated lanelet. When there are any obstacle in the detection area, vehicle must stop at specified stopline. refers: refers to detection area polygon. There could be multiple detection areas registered to a single regulatory element. refline: refers to stop line of the detection area Sample detection area in .osm format is shown below: <node id= 1 version= '1' lat= '49.00541994701' lon= '8.41565013855' > <tag k= \u2019ele\u2019 v= \u20190\u2019/ > </node> <node id= 2 version= '1' lat= '49.00542091657' lon= '8.4156469497' > <tag k= \u2019ele\u2019 v= \u20190\u2019/ > </node> <node id= 3 version= '1' lat= '49.00542180052' lon= '8.41564400223' > <tag k= \u2019ele\u2019 v= \u20190\u2019/ > </node> <node id= 4 version= '1' lat= '49.00541994701' lon= '8.41564400223' > <tag k= \u2019ele\u2019 v= \u20190\u2019/ > </node> <node id= 5 version= '1' lat= '49.00542180052' lon= '8.41564400223' > <tag k= \u2019ele\u2019 v= \u20190\u2019/ > </node> <node id= 6 version= '1' lat= '49.00541994701' lon= '8.41564400223' > <tag k= \u2019ele\u2019 v= \u20190\u2019/ > </node> <way id= 11 version= '1' > <nd ref= 1 /> <nd ref= 2 /> <nd ref= 3 /> <nd ref= 4 /> <nd ref= 1 /> <tag k= 'type' v= \u2019detection_area\u2019 /> <tag k= 'area' v= \u2019yes\u2019 /> </way> <way id= 12 version= \"1\" > <nd ref= 5 /> <nd ref= 6 /> <tag k= 'type' v= stop_line\u2019 /> </way> <relation id= \"13\" > <tag k= \"type\" v= \"regulatory_element\" /> <tag k= \"subtype\" v= \"detection_area\" /> <member type= \"way\" ref= \"11\" role= \"refers\" /> <member type= \"way\" ref= \"12\" role= \"ref_line\" /> </relation>","title":"Detection Area"},{"location":"map/lanelet2_extension/docs/extra_regulatory_elements/#road-marking","text":"This regulatory element specifies related road markings to a lanelet as shown below. * Note that the stopline in the image is for stoplines that are for reference, and normal stoplines should be expressed using TrafficSign regulatory element. refers: linestring with type attribute. Type explains what road marking it represents (e.g. stopline).","title":"Road Marking"},{"location":"map/lanelet2_extension/docs/lanelet2_format_extension/","text":"Modifying Lanelet2 format for Autoware # Overview # About the basics of the default format, please refer to main Lanelet2 repository . (see here about primitives) In addition to default Lanelet2 Format, users should add following mandatory/optional tags to their osm lanelet files as explained in reset of this document. Users may use autoware_lanelet2_validation node to check if their maps are valid. The following is the extra format added for Autoware: extra regulatory elements Detection Area Road Marking extra lanelet subtype Roadside Lane Mandatory Tags # Elevation Tags # Elevation(\"ele\") information for points( node ) is optional in default Lanelet2 format. However, some of Autoware packages(e.g. trafficlight_recognizer) need elevation to be included in HD map. Therefore, users must make sure that all points in their osm maps contain elevation tags. Here is an example osm syntax for node object. <node id= '1' visible= 'true' version= '1' lat= '49.00501435943' lon= '8.41687458512' > <tag k= 'ele' v= '3.0' /> <!-- this tag is mandatory for Autoware!! --> </node> TrafficLights # Default Lanelet2 format uses LineString( way ) or Polygon class to represent the shape of a traffic light. For Autoware, traffic light objects must be represented only by LineString to avoid confusion, where start point is at bottom left edge and end point is at bottom right edge. Also, \"height\" tag must be added in order to represent the size in vertical direction (not the position). The Following image illustrates how LineString is used to represent shape of Traffic Light in Autoware. Here is an example osm syntax for traffic light object. <way id= '13' visible= 'true' version= '1' > <nd ref= '6' /> <nd ref= '5' /> <tag k= 'type' v= 'traffic_light' /> <tag k= 'subtype' v= 'red_yellow_green' /> <tag k= 'height' v= '0.5' /> <!-- this tag is mandatory for Autoware!! --> </way> Turn Directions # Users must add \"turn_direction\" tags to lanelets within intersections to indicate vehicle's turning direction. You do not need this tags for lanelets that are not in intersections. If you do not have this tag, Autoware will not be able to light up turning indicators. This tags only take following values: left right straight Following image illustrates how lanelets should be tagged. Here is an example of osm syntax for lanelets in intersections. <relation id= '1' visible= 'true' version= '1' > <member type= 'way' ref= '2' role= 'left' /> <member type= 'way' ref= '3' role= 'right' /> <member type= 'relation' ref= '4' role= 'regulatory_element' /> <tag k= 'location' v= 'urban' /> <tag k= 'one_way' v= 'yes' /> <tag k= 'subtype' v= 'road' /> <tag k= 'type' v= 'lanelet' /> <tag k= 'turn_direction' v= 'left' /> <!-- this tag is mandatory for lanelets at intersections!! --> </relation> Optional Taggings # Following tags are optional tags that you may want to add depending on how you want to use your map in Autoware. Meta Info # Users may add the MetaInfo element to their OSM file to indicate format version and map version of their OSM file. This information is not meant to influence Autoware vehicle's behavior, but is published as ROS message so that developers could know which map was used from ROSBAG log files. MetaInfo elements exists in the same hierarchy with node , way , and relation elements, otherwise JOSM wouldn't be able to load the file correctly. Here is an example of MetaInfo in osm file: <?xml version='1.0' encoding='UTF-8'?> <osm version= '0.6' generator= 'JOSM' > <MetaInfo format_version= '1.0' map_version= '1.0' /> <node> ... </node> <way> ... </way> <relation> ... </relation> </osm> Local Coordinate Expression # Sometimes users might want to create Lanelet2 maps that are not georeferenced. In such a case, users may use \"local_x\", \"local_y\" taggings to express local positions instead of latitude and longitude. Autoware Osm Parser will overwrite x,y positions with these tags when they are present. For z values, use \"ele\" tags as default Lanelet2 Format. You would still need to fill in lat and lon attributes so that parser does not crush, but their values could be anything. Here is example node element in osm with \"local_x\", \"local_y\" taggings: <!-- lat/lon attributes are required, but their values can be anything --> <node id= '40648' visible= 'true' version= '1' lat= '0' lon= '0' > <tag k= 'local_x' v= 2.54'/ > <tag k= 'local_y' v= 4.38'/ > <tag k= 'ele' v= '3.0' /> </node> Light Bulbs in Traffic Lights # Default Lanelet format can only express shape (base + height) of traffic lights. However, region_tlr node in Autoware uses positions of each light bulbs to recognize color of traffic light. If users may wish to use this node, \"light_bulbs\"( way ) element must be registered to traffic_light regulatory_element object define position and color of each light bulb in traffic lights. If you are using other trafficlight_recognizer nodes(e.g. tlr_mxnet), which only uses bounding box of traffic light, then you do not need to add this object. \"light_bulbs\" object is defined using LineString( way ), and each node of line string is placed at the center of each light bulb. Also, each node should have \"color\" and optionally \"arrow\" tags to describe its type. Also, \"traffic_light_id\" tag is used to indicate which ID of relevant traffic_light element. \"color\" tag is used to express the color of the light bulb. Currently only three values are used: red yellow green \"arrow\" tag is used to express the direction of the arrow of light bulb: up right left up_right up_left Following image illustrates how \"light_bulbs\" LineString should be created. Here is an example of osm syntax for light_bulb object: <node id= 1 version= '1' lat= '49.00541994701' lon= '8.41565013855' > <tag k= 'ele' v= '5' /> <tag k= 'color' v= 'red' /> </node> <node id= 2 version= '1' lat= '49.00542091657' lon= '8.4156469497' > <tag k= 'ele' v= '5' /> <tag k= 'color' v= 'yellow' /> </node> <node id= 3 version= '1' lat= '49.00542180052' lon= '8.41564400223' > <tag k= 'ele' v= '5' /> <tag k= 'color' v= 'green' /> </node> <node id= 3 version= '1' lat= '49.00542180052' lon= '8.41564400223' > <tag k= 'ele' v= '4.6' /> <tag k= 'color' v= 'green' /> <tag k= arrow v= 'right' /> </node> <way id= 11 version= '1' > <nd ref= 1 /> <nd ref= 2 /> <nd ref= 3 /> <tag k= 'traffic_light_id' v= '10' /> <!-- id of linestring with type=\"traffic_light\" --> <tag k= 'type' v= 'light_bulbs' /> </way> After creating \"light_bulbs\" elements, you have to register them to traffic_light regulatory element as role \"light_bulbs\". The following illustrates how light_bulbs are registered to traffic_light regulatory elements. <relation id= '8' visible= 'true' version= '1' > <tag k= 'type' v= 'regulatory_element' /> <tag k= 'subtype' v= 'traffic_light' /> <member type= 'way' ref= '9' role= 'ref_line' /> <member type= 'way' ref= '10' role= 'refers' /> <!-- refers to the traffic light line string --> <member type= 'way' ref= '11' role= 'light_bulbs' /> <!-- refers to the light_bulb line string --> </relation>","title":"Modifying Lanelet2 format for Autoware"},{"location":"map/lanelet2_extension/docs/lanelet2_format_extension/#modifying-lanelet2-format-for-autoware","text":"","title":"Modifying Lanelet2 format for Autoware"},{"location":"map/lanelet2_extension/docs/lanelet2_format_extension/#overview","text":"About the basics of the default format, please refer to main Lanelet2 repository . (see here about primitives) In addition to default Lanelet2 Format, users should add following mandatory/optional tags to their osm lanelet files as explained in reset of this document. Users may use autoware_lanelet2_validation node to check if their maps are valid. The following is the extra format added for Autoware: extra regulatory elements Detection Area Road Marking extra lanelet subtype Roadside Lane","title":"Overview"},{"location":"map/lanelet2_extension/docs/lanelet2_format_extension/#mandatory-tags","text":"","title":"Mandatory Tags"},{"location":"map/lanelet2_extension/docs/lanelet2_format_extension/#elevation-tags","text":"Elevation(\"ele\") information for points( node ) is optional in default Lanelet2 format. However, some of Autoware packages(e.g. trafficlight_recognizer) need elevation to be included in HD map. Therefore, users must make sure that all points in their osm maps contain elevation tags. Here is an example osm syntax for node object. <node id= '1' visible= 'true' version= '1' lat= '49.00501435943' lon= '8.41687458512' > <tag k= 'ele' v= '3.0' /> <!-- this tag is mandatory for Autoware!! --> </node>","title":"Elevation Tags"},{"location":"map/lanelet2_extension/docs/lanelet2_format_extension/#trafficlights","text":"Default Lanelet2 format uses LineString( way ) or Polygon class to represent the shape of a traffic light. For Autoware, traffic light objects must be represented only by LineString to avoid confusion, where start point is at bottom left edge and end point is at bottom right edge. Also, \"height\" tag must be added in order to represent the size in vertical direction (not the position). The Following image illustrates how LineString is used to represent shape of Traffic Light in Autoware. Here is an example osm syntax for traffic light object. <way id= '13' visible= 'true' version= '1' > <nd ref= '6' /> <nd ref= '5' /> <tag k= 'type' v= 'traffic_light' /> <tag k= 'subtype' v= 'red_yellow_green' /> <tag k= 'height' v= '0.5' /> <!-- this tag is mandatory for Autoware!! --> </way>","title":"TrafficLights"},{"location":"map/lanelet2_extension/docs/lanelet2_format_extension/#turn-directions","text":"Users must add \"turn_direction\" tags to lanelets within intersections to indicate vehicle's turning direction. You do not need this tags for lanelets that are not in intersections. If you do not have this tag, Autoware will not be able to light up turning indicators. This tags only take following values: left right straight Following image illustrates how lanelets should be tagged. Here is an example of osm syntax for lanelets in intersections. <relation id= '1' visible= 'true' version= '1' > <member type= 'way' ref= '2' role= 'left' /> <member type= 'way' ref= '3' role= 'right' /> <member type= 'relation' ref= '4' role= 'regulatory_element' /> <tag k= 'location' v= 'urban' /> <tag k= 'one_way' v= 'yes' /> <tag k= 'subtype' v= 'road' /> <tag k= 'type' v= 'lanelet' /> <tag k= 'turn_direction' v= 'left' /> <!-- this tag is mandatory for lanelets at intersections!! --> </relation>","title":"Turn Directions"},{"location":"map/lanelet2_extension/docs/lanelet2_format_extension/#optional-taggings","text":"Following tags are optional tags that you may want to add depending on how you want to use your map in Autoware.","title":"Optional Taggings"},{"location":"map/lanelet2_extension/docs/lanelet2_format_extension/#meta-info","text":"Users may add the MetaInfo element to their OSM file to indicate format version and map version of their OSM file. This information is not meant to influence Autoware vehicle's behavior, but is published as ROS message so that developers could know which map was used from ROSBAG log files. MetaInfo elements exists in the same hierarchy with node , way , and relation elements, otherwise JOSM wouldn't be able to load the file correctly. Here is an example of MetaInfo in osm file: <?xml version='1.0' encoding='UTF-8'?> <osm version= '0.6' generator= 'JOSM' > <MetaInfo format_version= '1.0' map_version= '1.0' /> <node> ... </node> <way> ... </way> <relation> ... </relation> </osm>","title":"Meta Info"},{"location":"map/lanelet2_extension/docs/lanelet2_format_extension/#local-coordinate-expression","text":"Sometimes users might want to create Lanelet2 maps that are not georeferenced. In such a case, users may use \"local_x\", \"local_y\" taggings to express local positions instead of latitude and longitude. Autoware Osm Parser will overwrite x,y positions with these tags when they are present. For z values, use \"ele\" tags as default Lanelet2 Format. You would still need to fill in lat and lon attributes so that parser does not crush, but their values could be anything. Here is example node element in osm with \"local_x\", \"local_y\" taggings: <!-- lat/lon attributes are required, but their values can be anything --> <node id= '40648' visible= 'true' version= '1' lat= '0' lon= '0' > <tag k= 'local_x' v= 2.54'/ > <tag k= 'local_y' v= 4.38'/ > <tag k= 'ele' v= '3.0' /> </node>","title":"Local Coordinate Expression"},{"location":"map/lanelet2_extension/docs/lanelet2_format_extension/#light-bulbs-in-traffic-lights","text":"Default Lanelet format can only express shape (base + height) of traffic lights. However, region_tlr node in Autoware uses positions of each light bulbs to recognize color of traffic light. If users may wish to use this node, \"light_bulbs\"( way ) element must be registered to traffic_light regulatory_element object define position and color of each light bulb in traffic lights. If you are using other trafficlight_recognizer nodes(e.g. tlr_mxnet), which only uses bounding box of traffic light, then you do not need to add this object. \"light_bulbs\" object is defined using LineString( way ), and each node of line string is placed at the center of each light bulb. Also, each node should have \"color\" and optionally \"arrow\" tags to describe its type. Also, \"traffic_light_id\" tag is used to indicate which ID of relevant traffic_light element. \"color\" tag is used to express the color of the light bulb. Currently only three values are used: red yellow green \"arrow\" tag is used to express the direction of the arrow of light bulb: up right left up_right up_left Following image illustrates how \"light_bulbs\" LineString should be created. Here is an example of osm syntax for light_bulb object: <node id= 1 version= '1' lat= '49.00541994701' lon= '8.41565013855' > <tag k= 'ele' v= '5' /> <tag k= 'color' v= 'red' /> </node> <node id= 2 version= '1' lat= '49.00542091657' lon= '8.4156469497' > <tag k= 'ele' v= '5' /> <tag k= 'color' v= 'yellow' /> </node> <node id= 3 version= '1' lat= '49.00542180052' lon= '8.41564400223' > <tag k= 'ele' v= '5' /> <tag k= 'color' v= 'green' /> </node> <node id= 3 version= '1' lat= '49.00542180052' lon= '8.41564400223' > <tag k= 'ele' v= '4.6' /> <tag k= 'color' v= 'green' /> <tag k= arrow v= 'right' /> </node> <way id= 11 version= '1' > <nd ref= 1 /> <nd ref= 2 /> <nd ref= 3 /> <tag k= 'traffic_light_id' v= '10' /> <!-- id of linestring with type=\"traffic_light\" --> <tag k= 'type' v= 'light_bulbs' /> </way> After creating \"light_bulbs\" elements, you have to register them to traffic_light regulatory element as role \"light_bulbs\". The following illustrates how light_bulbs are registered to traffic_light regulatory elements. <relation id= '8' visible= 'true' version= '1' > <tag k= 'type' v= 'regulatory_element' /> <tag k= 'subtype' v= 'traffic_light' /> <member type= 'way' ref= '9' role= 'ref_line' /> <member type= 'way' ref= '10' role= 'refers' /> <!-- refers to the traffic light line string --> <member type= 'way' ref= '11' role= 'light_bulbs' /> <!-- refers to the light_bulb line string --> </relation>","title":"Light Bulbs in Traffic Lights"},{"location":"map/map_loader/","text":"map_loader package # This package provides the features of loading various maps. pointcloud_map_loader # Feature # pointcloud_map_loader loads PointCloud file and publishes the map data as sensor_msgs/PointCloud2 message. How to run # ros2 run map_loader pointcloud_map_loader --ros-args -p \"pcd_paths_or_directory:=[path/to/pointcloud1.pcd, path/to/pointcloud2.pcd, ...]\" Published Topics # pointcloud_map (sensor_msgs/PointCloud2) : PointCloud Map lanelet2_map_loader # Feature # lanelet2_map_loader loads Lanelet2 file and publishes the map data as autoware_lanelet2_msgs/MapBin message. The node projects lan/lon coordinates into MGRS coordinates. How to run # ros2 run map_loader lanelet2_map_loader --ros-args -p lanelet2_map_path:=path/to/map.osm Published Topics # ~output/lanelet2_map (autoware_lanelet2_msgs/MapBin) : Binary data of loaded Lanelet2 Map lanelet2_map_visualization # Feature # lanelet2_map_visualization visualizes autoware_lanelet2_msgs/MapBin messages into visualization_msgs/MarkerArray. How to Run # ros2 run map_loader lanelet2_map_visualization Subscribed Topics # ~input/lanelet2_map (autoware_lanelet2_msgs/MapBin) : binary data of Lanelet2 Map Published Topics # ~output/lanelet2_map_marker (visualization_msgs/MarkerArray) : visualization messages for RViz elevation_map_loader # Feature # Generate elevation_map from subscribed pointcloud_map and vector_map and publish it. Save the generated elevation_map locally and load it from next time. The elevation value of each cell is the average value of z of the points of the lowest cluster. Cells with No elevation value can be inpainted using the values of neighboring cells. How to run # ros2 run map_loader elevation_map_loader --ros-args -p param_file_path:=path/to/elevation_map_parameters.yaml -p elevation_map_directory:=path/to/elevation_map_directory -p pointcloud_map_path:=path/to/pointcloud.pcd Subscribed Topics # input/pointcloud_map (sensor_msgs:PointCloud2) : PointCloud Map input/vector_map (autoware_lanelet2_msgs/MapBin) : binary data of Lanelet2 Map Published Topics # output/elevation_map (grid_map_msgs/GridMap) : Elevation Map output/elevation_map_cloud (sensor_msgs:PointCloud2) : Pointcloud generated from the value of Elevation Map Parameter description # ROS parameters # Name Type Description Default value map_layer_name std::string elevation_map layer name elevation param_file_path std::string GridMap parameters config path_default elevation_map_file_path std::string elevation_map file (bag2) path_default map_frame std::string map_frame when loading elevation_map file map use_inpaint bool Whether to inpaint empty cells true inpaint_radius float Radius of a circular neighborhood of each point inpainted that is considered by the algorithm [m] 0.3 use_elevation_map_cloud_publisher bool Whether to publish output/elevation_map_cloud false use_lane_filter bool Whether to filter elevation_map with vector_map false lane_margin float Value of how much to expand the range of vector_map [m] 0.5 lane_height_diff_thresh float Only point clouds in the height range of this value from vector_map are used to generate elevation_map [m] 1.0 lane_filter_voxel_size_x float Voxel size x for calculating point clouds in vector_map [m] 0.04 lane_filter_voxel_size_y float Voxel size y for calculating point clouds in vector_map [m] 0.04 lane_filter_voxel_size_z float Voxel size z for calculating point clouds in vector_map [m] 0.04 GridMap parameters # The parameters are described on config/elevation_map_parameters.yaml . General parameters # Name Type Description Default value pcl_grid_map_extraction/num_processing_threads int Number of threads for processing grid map cells. Filtering of the raw input point cloud is not parallelized. 12 Grid map parameters # See: https://github.com/ANYbotics/grid_map/tree/ros2/grid_map_pcl Resulting grid map parameters. Name Type Description Default value pcl_grid_map_extraction/grid_map/min_num_points_per_cell int Minimum number of points in the point cloud that have to fall within any of the grid map cells. Otherwise the cell elevation will be set to NaN. 3 pcl_grid_map_extraction/grid_map/resolution float Resolution of the grid map. Width and length are computed automatically. 0.3 Point Cloud Pre-processing Parameters # Rigid body transform parameters # Rigid body transform that is applied to the point cloud before computing elevation. Name Type Description Default value pcl_grid_map_extraction/cloud_transform/translation float Translation (xyz) that is applied to the input point cloud before computing elevation. 0.0 pcl_grid_map_extraction/cloud_transform/rotation float Rotation (intrinsic rotation, convention X-Y'-Z'') that is applied to the input point cloud before computing elevation. 0.0 Cluster extraction parameters # Cluster extraction is based on pcl algorithms. See https://pointclouds.org/documentation/tutorials/cluster_extraction.html for more details. Name Type Description Default value pcl_grid_map_extraction/cluster_extraction/cluster_tolerance float Distance between points below which they will still be considered part of one cluster. 0.2 pcl_grid_map_extraction/cluster_extraction/min_num_points int Min number of points that a cluster needs to have (otherwise it will be discarded). 3 pcl_grid_map_extraction/cluster_extraction/max_num_points int Max number of points that a cluster can have (otherwise it will be discarded). 1000000 Outlier removal parameters # See https://pointclouds.org/documentation/tutorials/statistical_outlier.html for more explanation on outlier removal. Name Type Description Default value pcl_grid_map_extraction/outlier_removal/is_remove_outliers float Whether to perform statistical outlier removal. false pcl_grid_map_extraction/outlier_removal/mean_K float Number of neighbours to analyze for estimating statistics of a point. 10 pcl_grid_map_extraction/outlier_removal/stddev_threshold float Number of standard deviations under which points are considered to be inliers. 1.0 Subsampling parameters # See https://pointclouds.org/documentation/tutorials/voxel_grid.html for more explanation on point cloud downsampling. Name Type Description Default value pcl_grid_map_extraction/downsampling/is_downsample_cloud bool Whether to perform downsampling or not. false pcl_grid_map_extraction/downsampling/voxel_size float Voxel sizes (xyz) in meters. 0.02","title":"map_loader package"},{"location":"map/map_loader/#map_loader-package","text":"This package provides the features of loading various maps.","title":"map_loader package"},{"location":"map/map_loader/#pointcloud_map_loader","text":"","title":"pointcloud_map_loader"},{"location":"map/map_loader/#feature","text":"pointcloud_map_loader loads PointCloud file and publishes the map data as sensor_msgs/PointCloud2 message.","title":"Feature"},{"location":"map/map_loader/#how-to-run","text":"ros2 run map_loader pointcloud_map_loader --ros-args -p \"pcd_paths_or_directory:=[path/to/pointcloud1.pcd, path/to/pointcloud2.pcd, ...]\"","title":"How to run"},{"location":"map/map_loader/#published-topics","text":"pointcloud_map (sensor_msgs/PointCloud2) : PointCloud Map","title":"Published Topics"},{"location":"map/map_loader/#lanelet2_map_loader","text":"","title":"lanelet2_map_loader"},{"location":"map/map_loader/#feature_1","text":"lanelet2_map_loader loads Lanelet2 file and publishes the map data as autoware_lanelet2_msgs/MapBin message. The node projects lan/lon coordinates into MGRS coordinates.","title":"Feature"},{"location":"map/map_loader/#how-to-run_1","text":"ros2 run map_loader lanelet2_map_loader --ros-args -p lanelet2_map_path:=path/to/map.osm","title":"How to run"},{"location":"map/map_loader/#published-topics_1","text":"~output/lanelet2_map (autoware_lanelet2_msgs/MapBin) : Binary data of loaded Lanelet2 Map","title":"Published Topics"},{"location":"map/map_loader/#lanelet2_map_visualization","text":"","title":"lanelet2_map_visualization"},{"location":"map/map_loader/#feature_2","text":"lanelet2_map_visualization visualizes autoware_lanelet2_msgs/MapBin messages into visualization_msgs/MarkerArray.","title":"Feature"},{"location":"map/map_loader/#how-to-run_2","text":"ros2 run map_loader lanelet2_map_visualization","title":"How to Run"},{"location":"map/map_loader/#subscribed-topics","text":"~input/lanelet2_map (autoware_lanelet2_msgs/MapBin) : binary data of Lanelet2 Map","title":"Subscribed Topics"},{"location":"map/map_loader/#published-topics_2","text":"~output/lanelet2_map_marker (visualization_msgs/MarkerArray) : visualization messages for RViz","title":"Published Topics"},{"location":"map/map_loader/#elevation_map_loader","text":"","title":"elevation_map_loader"},{"location":"map/map_loader/#feature_3","text":"Generate elevation_map from subscribed pointcloud_map and vector_map and publish it. Save the generated elevation_map locally and load it from next time. The elevation value of each cell is the average value of z of the points of the lowest cluster. Cells with No elevation value can be inpainted using the values of neighboring cells.","title":"Feature"},{"location":"map/map_loader/#how-to-run_3","text":"ros2 run map_loader elevation_map_loader --ros-args -p param_file_path:=path/to/elevation_map_parameters.yaml -p elevation_map_directory:=path/to/elevation_map_directory -p pointcloud_map_path:=path/to/pointcloud.pcd","title":"How to run"},{"location":"map/map_loader/#subscribed-topics_1","text":"input/pointcloud_map (sensor_msgs:PointCloud2) : PointCloud Map input/vector_map (autoware_lanelet2_msgs/MapBin) : binary data of Lanelet2 Map","title":"Subscribed Topics"},{"location":"map/map_loader/#published-topics_3","text":"output/elevation_map (grid_map_msgs/GridMap) : Elevation Map output/elevation_map_cloud (sensor_msgs:PointCloud2) : Pointcloud generated from the value of Elevation Map","title":"Published Topics"},{"location":"map/map_loader/#parameter-description","text":"","title":"Parameter description"},{"location":"map/map_loader/#ros-parameters","text":"Name Type Description Default value map_layer_name std::string elevation_map layer name elevation param_file_path std::string GridMap parameters config path_default elevation_map_file_path std::string elevation_map file (bag2) path_default map_frame std::string map_frame when loading elevation_map file map use_inpaint bool Whether to inpaint empty cells true inpaint_radius float Radius of a circular neighborhood of each point inpainted that is considered by the algorithm [m] 0.3 use_elevation_map_cloud_publisher bool Whether to publish output/elevation_map_cloud false use_lane_filter bool Whether to filter elevation_map with vector_map false lane_margin float Value of how much to expand the range of vector_map [m] 0.5 lane_height_diff_thresh float Only point clouds in the height range of this value from vector_map are used to generate elevation_map [m] 1.0 lane_filter_voxel_size_x float Voxel size x for calculating point clouds in vector_map [m] 0.04 lane_filter_voxel_size_y float Voxel size y for calculating point clouds in vector_map [m] 0.04 lane_filter_voxel_size_z float Voxel size z for calculating point clouds in vector_map [m] 0.04","title":"ROS parameters"},{"location":"map/map_loader/#gridmap-parameters","text":"The parameters are described on config/elevation_map_parameters.yaml .","title":"GridMap parameters"},{"location":"map/map_loader/#general-parameters","text":"Name Type Description Default value pcl_grid_map_extraction/num_processing_threads int Number of threads for processing grid map cells. Filtering of the raw input point cloud is not parallelized. 12","title":"General parameters"},{"location":"map/map_loader/#grid-map-parameters","text":"See: https://github.com/ANYbotics/grid_map/tree/ros2/grid_map_pcl Resulting grid map parameters. Name Type Description Default value pcl_grid_map_extraction/grid_map/min_num_points_per_cell int Minimum number of points in the point cloud that have to fall within any of the grid map cells. Otherwise the cell elevation will be set to NaN. 3 pcl_grid_map_extraction/grid_map/resolution float Resolution of the grid map. Width and length are computed automatically. 0.3","title":"Grid map parameters"},{"location":"map/map_loader/#point-cloud-pre-processing-parameters","text":"","title":"Point Cloud Pre-processing Parameters"},{"location":"map/map_loader/#rigid-body-transform-parameters","text":"Rigid body transform that is applied to the point cloud before computing elevation. Name Type Description Default value pcl_grid_map_extraction/cloud_transform/translation float Translation (xyz) that is applied to the input point cloud before computing elevation. 0.0 pcl_grid_map_extraction/cloud_transform/rotation float Rotation (intrinsic rotation, convention X-Y'-Z'') that is applied to the input point cloud before computing elevation. 0.0","title":"Rigid body transform parameters"},{"location":"map/map_loader/#cluster-extraction-parameters","text":"Cluster extraction is based on pcl algorithms. See https://pointclouds.org/documentation/tutorials/cluster_extraction.html for more details. Name Type Description Default value pcl_grid_map_extraction/cluster_extraction/cluster_tolerance float Distance between points below which they will still be considered part of one cluster. 0.2 pcl_grid_map_extraction/cluster_extraction/min_num_points int Min number of points that a cluster needs to have (otherwise it will be discarded). 3 pcl_grid_map_extraction/cluster_extraction/max_num_points int Max number of points that a cluster can have (otherwise it will be discarded). 1000000","title":"Cluster extraction parameters"},{"location":"map/map_loader/#outlier-removal-parameters","text":"See https://pointclouds.org/documentation/tutorials/statistical_outlier.html for more explanation on outlier removal. Name Type Description Default value pcl_grid_map_extraction/outlier_removal/is_remove_outliers float Whether to perform statistical outlier removal. false pcl_grid_map_extraction/outlier_removal/mean_K float Number of neighbours to analyze for estimating statistics of a point. 10 pcl_grid_map_extraction/outlier_removal/stddev_threshold float Number of standard deviations under which points are considered to be inliers. 1.0","title":"Outlier removal parameters"},{"location":"map/map_loader/#subsampling-parameters","text":"See https://pointclouds.org/documentation/tutorials/voxel_grid.html for more explanation on point cloud downsampling. Name Type Description Default value pcl_grid_map_extraction/downsampling/is_downsample_cloud bool Whether to perform downsampling or not. false pcl_grid_map_extraction/downsampling/voxel_size float Voxel sizes (xyz) in meters. 0.02","title":"Subsampling parameters"},{"location":"map/map_tf_generator/Readme/","text":"map_tf_generator # Purpose # This node broadcasts viewer frames for visualization of pointcloud map in Rviz. The position of viewer frames is the geometric center of input pointclouds. Note that there is no module to need viewer frames and this is used only for visualization. Inner-workings / Algorithms # Inputs / Outputs # Input # Name Type Description /map/pointcloud_map sensor_msgs::msg::PointCloud2 Subscribe pointcloud map to calculate position of viewer frames Output # Name Type Description /tf_static tf2_msgs/msg/TFMessage Broadcast viewer frames Parameters # Node Parameters # None Core Parameters # Name Type Default Value Explanation viewer_frame string viewer Name of viewer frame map_frame string map The parent frame name of viewer frame Assumptions / Known limits # TBD.","title":"map_tf_generator"},{"location":"map/map_tf_generator/Readme/#map_tf_generator","text":"","title":"map_tf_generator"},{"location":"map/map_tf_generator/Readme/#purpose","text":"This node broadcasts viewer frames for visualization of pointcloud map in Rviz. The position of viewer frames is the geometric center of input pointclouds. Note that there is no module to need viewer frames and this is used only for visualization.","title":"Purpose"},{"location":"map/map_tf_generator/Readme/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"map/map_tf_generator/Readme/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"map/map_tf_generator/Readme/#input","text":"Name Type Description /map/pointcloud_map sensor_msgs::msg::PointCloud2 Subscribe pointcloud map to calculate position of viewer frames","title":"Input"},{"location":"map/map_tf_generator/Readme/#output","text":"Name Type Description /tf_static tf2_msgs/msg/TFMessage Broadcast viewer frames","title":"Output"},{"location":"map/map_tf_generator/Readme/#parameters","text":"","title":"Parameters"},{"location":"map/map_tf_generator/Readme/#node-parameters","text":"None","title":"Node Parameters"},{"location":"map/map_tf_generator/Readme/#core-parameters","text":"Name Type Default Value Explanation viewer_frame string viewer Name of viewer frame map_frame string map The parent frame name of viewer frame","title":"Core Parameters"},{"location":"map/map_tf_generator/Readme/#assumptions-known-limits","text":"TBD.","title":"Assumptions / Known limits"},{"location":"messages/","text":"autoware_iv_msgs #","title":"autoware_iv_msgs"},{"location":"messages/#autoware_iv_msgs","text":"","title":"autoware_iv_msgs"},{"location":"perception/compare_map_segmentation/","text":"compare_map_segmentation # Purpose # The compare_map_segmentation is a node that filters the ground points from the input pointcloud by using map info (e.g. pcd, elevation map). Inner-workings / Algorithms # Compare Elevation Map Filter # Compare the z of the input points with the value of elevation_map. The height difference is calculated by the binary integration of neighboring cells. Remove points whose height difference is below the height_diff_thresh . Distance Based Compare Map Filter # WIP Voxel Based Approximate Compare Map Filter # WIP Voxel Based Compare Map Filter # WIP Voxel Distance based Compare Map Filter # WIP Inputs / Outputs # Compare Elevation Map Filter # Input # Name Type Description ~/input/points sensor_msgs::msg::PointCloud2 reference points ~/input/elevation_map grid_map::msg::GridMap elevation map Output # Name Type Description ~/output/points sensor_msgs::msg::PointCloud2 filtered points Other Filters # Input # Name Type Description ~/input/points sensor_msgs::msg::PointCloud2 reference points ~/input/map grid_map::msg::GridMap map Output # Name Type Description ~/output/points sensor_msgs::msg::PointCloud2 filtered points Parameters # Core Parameters # Name Type Description Default value map_layer_name string elevation map layer name elevation map_frame float frame_id of the map that is temporarily used before elevation_map is subscribed map height_diff_thresh float Remove points whose height difference is below this value [m] 0.15 Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # (Optional) Future extensions / Unimplemented parts #","title":"compare_map_segmentation"},{"location":"perception/compare_map_segmentation/#compare_map_segmentation","text":"","title":"compare_map_segmentation"},{"location":"perception/compare_map_segmentation/#purpose","text":"The compare_map_segmentation is a node that filters the ground points from the input pointcloud by using map info (e.g. pcd, elevation map).","title":"Purpose"},{"location":"perception/compare_map_segmentation/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"perception/compare_map_segmentation/#compare-elevation-map-filter","text":"Compare the z of the input points with the value of elevation_map. The height difference is calculated by the binary integration of neighboring cells. Remove points whose height difference is below the height_diff_thresh .","title":"Compare Elevation Map Filter"},{"location":"perception/compare_map_segmentation/#distance-based-compare-map-filter","text":"WIP","title":"Distance Based Compare Map Filter"},{"location":"perception/compare_map_segmentation/#voxel-based-approximate-compare-map-filter","text":"WIP","title":"Voxel Based Approximate Compare Map Filter"},{"location":"perception/compare_map_segmentation/#voxel-based-compare-map-filter","text":"WIP","title":"Voxel Based Compare Map Filter"},{"location":"perception/compare_map_segmentation/#voxel-distance-based-compare-map-filter","text":"WIP","title":"Voxel Distance based Compare Map Filter"},{"location":"perception/compare_map_segmentation/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"perception/compare_map_segmentation/#compare-elevation-map-filter_1","text":"","title":"Compare Elevation Map Filter"},{"location":"perception/compare_map_segmentation/#input","text":"Name Type Description ~/input/points sensor_msgs::msg::PointCloud2 reference points ~/input/elevation_map grid_map::msg::GridMap elevation map","title":"Input"},{"location":"perception/compare_map_segmentation/#output","text":"Name Type Description ~/output/points sensor_msgs::msg::PointCloud2 filtered points","title":"Output"},{"location":"perception/compare_map_segmentation/#other-filters","text":"","title":"Other Filters"},{"location":"perception/compare_map_segmentation/#input_1","text":"Name Type Description ~/input/points sensor_msgs::msg::PointCloud2 reference points ~/input/map grid_map::msg::GridMap map","title":"Input"},{"location":"perception/compare_map_segmentation/#output_1","text":"Name Type Description ~/output/points sensor_msgs::msg::PointCloud2 filtered points","title":"Output"},{"location":"perception/compare_map_segmentation/#parameters","text":"","title":"Parameters"},{"location":"perception/compare_map_segmentation/#core-parameters","text":"Name Type Description Default value map_layer_name string elevation map layer name elevation map_frame float frame_id of the map that is temporarily used before elevation_map is subscribed map height_diff_thresh float Remove points whose height difference is below this value [m] 0.15","title":"Core Parameters"},{"location":"perception/compare_map_segmentation/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"perception/compare_map_segmentation/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"perception/compare_map_segmentation/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"perception/compare_map_segmentation/#optional-referencesexternal-links","text":"","title":"(Optional) References/External links"},{"location":"perception/compare_map_segmentation/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"perception/ground_segmentation/","text":"ground_segmentation # Purpose # The ground_segmentation is a node that filters the ground points from the input pointcloud. Inner-workings / Algorithms # Ransac Ground Filter # This algorithm is SAC-based ground segmentation. See: https://pcl.readthedocs.io/projects/tutorials/en/latest/planar_segmentation.html Ray Ground Filter # This algorithm is deprecated in current architecture. Scan Ground Filter # This algorithm works by following steps, Divide whole pointclouds into groups by horizontal angle and sort by xy-distance. Check the distance and vertical angle of the point one by one. Set a center of the ground contact point of the rear or front wheels as the initial point. Check vertical angle between the points. If the angle from the initial point is larger than \"global_slope_max\", the point is classified as \"no ground\". If the angle from the previous point is larger than \"local_max_slope\", the point is classified as \"no ground\". Otherwise the point is labeled as \"ground point\". If the distance from the last checked point is close, ignore any vertical angle and set current point attribute to the same as the last point. Inputs / Outputs # Input # Name Type Description ~input sensor_msgs::PointCloud2 outlier filtered pointcloud Output # Name Type Description ~output sensor_msgs::PointCloud2 no ground pointcloud Parameters # Ransac Ground Filter # Core Parameters # Name Type Default Value Description base_frame string \"base_link\" base_link frame unit_axis string \"z\" The axis which we need to search ground plane max_iterations int 1000 The maximum number of iterations outlier_threshold double 0.01 The distance threshold to the model [m] plane_slope_threshold double 10.0 The slope threshold to prevent mis-fitting [deg] voxel_size_x double 0.04 voxel size x [m] voxel_size_y double 0.04 voxel size y [m] voxel_size_z double 0.04 voxel size z [m] height_threshold double 0.01 The height threshold from ground plane for no ground points [m] debug bool false whether to output debug information Scan Ground Filter # Core Parameters # Name Type Default Value Description base_frame string \"base_link\" base_link frame global_slope_max double 8.0 The global angle to classify as the ground or object [deg] local_max_slope double 6.0 The local angle to classify as the ground or object [deg] radial_divider_angle double 1.0 The angle which divide the whole pointcloud to sliced group [deg] split_points_distance_tolerance double 0.2 The xy-distance threshold to to distinguishing far and near [m] split_height_distance double 0.2 The height threshold to distinguishing far and near [m] use_virtual_ground_point bool true whether to use the ground center of front wheels as the virtual ground point. Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # (Optional) Future extensions / Unimplemented parts # Horizontal check for classification is not implemented yet. Output ground visibility for diagnostic is not implemented yet.","title":"ground_segmentation"},{"location":"perception/ground_segmentation/#ground_segmentation","text":"","title":"ground_segmentation"},{"location":"perception/ground_segmentation/#purpose","text":"The ground_segmentation is a node that filters the ground points from the input pointcloud.","title":"Purpose"},{"location":"perception/ground_segmentation/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"perception/ground_segmentation/#ransac-ground-filter","text":"This algorithm is SAC-based ground segmentation. See: https://pcl.readthedocs.io/projects/tutorials/en/latest/planar_segmentation.html","title":"Ransac Ground Filter"},{"location":"perception/ground_segmentation/#ray-ground-filter","text":"This algorithm is deprecated in current architecture.","title":"Ray Ground Filter"},{"location":"perception/ground_segmentation/#scan-ground-filter","text":"This algorithm works by following steps, Divide whole pointclouds into groups by horizontal angle and sort by xy-distance. Check the distance and vertical angle of the point one by one. Set a center of the ground contact point of the rear or front wheels as the initial point. Check vertical angle between the points. If the angle from the initial point is larger than \"global_slope_max\", the point is classified as \"no ground\". If the angle from the previous point is larger than \"local_max_slope\", the point is classified as \"no ground\". Otherwise the point is labeled as \"ground point\". If the distance from the last checked point is close, ignore any vertical angle and set current point attribute to the same as the last point.","title":"Scan Ground Filter"},{"location":"perception/ground_segmentation/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"perception/ground_segmentation/#input","text":"Name Type Description ~input sensor_msgs::PointCloud2 outlier filtered pointcloud","title":"Input"},{"location":"perception/ground_segmentation/#output","text":"Name Type Description ~output sensor_msgs::PointCloud2 no ground pointcloud","title":"Output"},{"location":"perception/ground_segmentation/#parameters","text":"","title":"Parameters"},{"location":"perception/ground_segmentation/#ransac-ground-filter_1","text":"","title":"Ransac Ground Filter"},{"location":"perception/ground_segmentation/#core-parameters","text":"Name Type Default Value Description base_frame string \"base_link\" base_link frame unit_axis string \"z\" The axis which we need to search ground plane max_iterations int 1000 The maximum number of iterations outlier_threshold double 0.01 The distance threshold to the model [m] plane_slope_threshold double 10.0 The slope threshold to prevent mis-fitting [deg] voxel_size_x double 0.04 voxel size x [m] voxel_size_y double 0.04 voxel size y [m] voxel_size_z double 0.04 voxel size z [m] height_threshold double 0.01 The height threshold from ground plane for no ground points [m] debug bool false whether to output debug information","title":"Core Parameters"},{"location":"perception/ground_segmentation/#scan-ground-filter_1","text":"","title":"Scan Ground Filter"},{"location":"perception/ground_segmentation/#core-parameters_1","text":"Name Type Default Value Description base_frame string \"base_link\" base_link frame global_slope_max double 8.0 The global angle to classify as the ground or object [deg] local_max_slope double 6.0 The local angle to classify as the ground or object [deg] radial_divider_angle double 1.0 The angle which divide the whole pointcloud to sliced group [deg] split_points_distance_tolerance double 0.2 The xy-distance threshold to to distinguishing far and near [m] split_height_distance double 0.2 The height threshold to distinguishing far and near [m] use_virtual_ground_point bool true whether to use the ground center of front wheels as the virtual ground point.","title":"Core Parameters"},{"location":"perception/ground_segmentation/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"perception/ground_segmentation/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"perception/ground_segmentation/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"perception/ground_segmentation/#optional-referencesexternal-links","text":"","title":"(Optional) References/External links"},{"location":"perception/ground_segmentation/#optional-future-extensions-unimplemented-parts","text":"Horizontal check for classification is not implemented yet. Output ground visibility for diagnostic is not implemented yet.","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"perception/lidar_centerpoint/","text":"CenterPoint TensorRT # This is a 3D object detection implementation of CenterPoint supporting TensorRT inference. The object.existence_probability is stored the value of classification confidence of DNN, not probability. Parameters # Input Topics # Name Type Description input/pointcloud PointCloud2 Point Clouds (x, y, z and intensity) Output Topics # Name Type Description output/objects DynamicObjectWithFeatureArray 3D Bounding Box debug/pointcloud_densification PointCloud2 multi-frame pointcloud ROS Parameters # Name Type Description Default score_threshold float detected objects with score less than threshold are ignored 0.4 densification_base_frame string the base frame id to fuse multi-frame pointcloud map densification_past_frames int the number of past frames to fuse with the current frame 1 use_encoder_trt bool use TensorRT VoxelFeatureEncoder false use_head_trt bool use TensorRT DetectionHead true trt_precision string TensorRT inference precision: fp32 or fp16 fp16 encoder_onnx_path string path to VoxelFeatureEncoder ONNX file encoder_engine_path string path to VoxelFeatureEncoder TensorRT Engine file encoder_pt_path string path to VoxelFeatureEncoder TorchScript file head_onnx_path string path to DetectionHead ONNX file head_engine_path string path to DetectionHead TensorRT Engine file head_pt_path string path to DetectionHead TorchScript file For Developers # If you have an error like 'GOMP_4.5' not found , replace the OpenMP library in libtorch. sudo apt install libgomp1 -y rm /path/to/libtorch/lib/libgomp-75eea7e8.so.1 ln -s /usr/lib/x86_64-linux-gnu/libgomp.so.1 /path/to/libtorch/lib/libgomp-75eea7e8.so.1 Reference # Yin, Tianwei, Xingyi Zhou, and Philipp Kr\u00e4henb\u00fchl. \"Center-based 3d object detection and tracking.\" arXiv preprint arXiv:2006.11275 (2020). Reference Repositories # https://github.com/tianweiy/CenterPoint https://github.com/open-mmlab/OpenPCDet https://github.com/poodarchu/Det3D https://github.com/xingyizhou/CenterNet https://github.com/lzccccc/SMOKE https://github.com/yukkysaito/autoware_perception https://github.com/pytorch/pytorch","title":"CenterPoint TensorRT"},{"location":"perception/lidar_centerpoint/#centerpoint-tensorrt","text":"This is a 3D object detection implementation of CenterPoint supporting TensorRT inference. The object.existence_probability is stored the value of classification confidence of DNN, not probability.","title":"CenterPoint TensorRT"},{"location":"perception/lidar_centerpoint/#parameters","text":"","title":"Parameters"},{"location":"perception/lidar_centerpoint/#input-topics","text":"Name Type Description input/pointcloud PointCloud2 Point Clouds (x, y, z and intensity)","title":"Input Topics"},{"location":"perception/lidar_centerpoint/#output-topics","text":"Name Type Description output/objects DynamicObjectWithFeatureArray 3D Bounding Box debug/pointcloud_densification PointCloud2 multi-frame pointcloud","title":"Output Topics"},{"location":"perception/lidar_centerpoint/#ros-parameters","text":"Name Type Description Default score_threshold float detected objects with score less than threshold are ignored 0.4 densification_base_frame string the base frame id to fuse multi-frame pointcloud map densification_past_frames int the number of past frames to fuse with the current frame 1 use_encoder_trt bool use TensorRT VoxelFeatureEncoder false use_head_trt bool use TensorRT DetectionHead true trt_precision string TensorRT inference precision: fp32 or fp16 fp16 encoder_onnx_path string path to VoxelFeatureEncoder ONNX file encoder_engine_path string path to VoxelFeatureEncoder TensorRT Engine file encoder_pt_path string path to VoxelFeatureEncoder TorchScript file head_onnx_path string path to DetectionHead ONNX file head_engine_path string path to DetectionHead TensorRT Engine file head_pt_path string path to DetectionHead TorchScript file","title":"ROS Parameters"},{"location":"perception/lidar_centerpoint/#for-developers","text":"If you have an error like 'GOMP_4.5' not found , replace the OpenMP library in libtorch. sudo apt install libgomp1 -y rm /path/to/libtorch/lib/libgomp-75eea7e8.so.1 ln -s /usr/lib/x86_64-linux-gnu/libgomp.so.1 /path/to/libtorch/lib/libgomp-75eea7e8.so.1","title":"For Developers"},{"location":"perception/lidar_centerpoint/#reference","text":"Yin, Tianwei, Xingyi Zhou, and Philipp Kr\u00e4henb\u00fchl. \"Center-based 3d object detection and tracking.\" arXiv preprint arXiv:2006.11275 (2020).","title":"Reference"},{"location":"perception/lidar_centerpoint/#reference-repositories","text":"https://github.com/tianweiy/CenterPoint https://github.com/open-mmlab/OpenPCDet https://github.com/poodarchu/Det3D https://github.com/xingyizhou/CenterNet https://github.com/lzccccc/SMOKE https://github.com/yukkysaito/autoware_perception https://github.com/pytorch/pytorch","title":"Reference Repositories"},{"location":"perception/multi_object_tracker/","text":"multi_object_tracker # Purpose # The results of the detection are processed by a time series. The main purpose is to give ID and estimate velocity. Inner-workings / Algorithms # This multi object tracker consists of data association and EKF. Data association # The data association performs maximum score matching, called min cost max flow problem. In this package, mussp[1] is used as solver. In addition, when associating observations to tracers, data association have gates such as the area of the object from the BEV, Mahalanobis distance, and maximum distance, depending on the class label. EKF Tracker # Models for pedestrians, bicycles (motorcycles), cars and unknown are available. The pedestrian or bicycle tracker is running at the same time as the respective EKF model in order to enable the transition between pedestrian and bicycle tracking. For big vehicles such as trucks and buses, we have separate models for passenger cars and large vehicles because they are difficult to distinguish from passenger cars and are not stable. Therefore, separate models are prepared for passenger cars and big vehicles, and these models are run at the same time as the respective EKF models to ensure stability. Inputs / Outputs # Input # Name Type Description ~/input autoware_auto_perception_msgs::msg::DetectedObjects obstacles Output # Name Type Description ~/output autoware_auto_perception_msgs::msg::TrackedObjects modified obstacles Parameters # Core Parameters # Name Type Description can_assign_matrix double Assignment table for data association max_dist_matrix double Maximum distance table for data association max_area_matrix double Maximum area table for data association min_area_matrix double Minimum area table for data association max_rad_matrix double Maximum angle table for data association world_frame_id double tracking frame enable_delay_compensation bool Estimate obstacles at current time considering detection delay publish_rate double if enable_delay_compensation is true, how many hertz to output Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # Evaluation of muSSP # According to our evaluation, muSSP is faster than normal SSP when the matrix size is more than 100. Execution time for varying matrix size at 95% sparsity. In real data, the sparsity was often around 95%. Execution time for varying the sparsity with matrix size 100. (Optional) References/External links # This package makes use of external code. Name License Original Repository muSSP Apache 2.0 https://github.com/yu-lab-vt/muSSP [1] C. Wang, Y. Wang, Y. Wang, C.-t. Wu, and G. Yu, \u201cmuSSP: Efficient Min-cost Flow Algorithm for Multi-object Tracking,\u201d NeurIPS, 2019 (Optional) Future extensions / Unimplemented parts #","title":"multi_object_tracker"},{"location":"perception/multi_object_tracker/#multi_object_tracker","text":"","title":"multi_object_tracker"},{"location":"perception/multi_object_tracker/#purpose","text":"The results of the detection are processed by a time series. The main purpose is to give ID and estimate velocity.","title":"Purpose"},{"location":"perception/multi_object_tracker/#inner-workings-algorithms","text":"This multi object tracker consists of data association and EKF.","title":"Inner-workings / Algorithms"},{"location":"perception/multi_object_tracker/#data-association","text":"The data association performs maximum score matching, called min cost max flow problem. In this package, mussp[1] is used as solver. In addition, when associating observations to tracers, data association have gates such as the area of the object from the BEV, Mahalanobis distance, and maximum distance, depending on the class label.","title":"Data association"},{"location":"perception/multi_object_tracker/#ekf-tracker","text":"Models for pedestrians, bicycles (motorcycles), cars and unknown are available. The pedestrian or bicycle tracker is running at the same time as the respective EKF model in order to enable the transition between pedestrian and bicycle tracking. For big vehicles such as trucks and buses, we have separate models for passenger cars and large vehicles because they are difficult to distinguish from passenger cars and are not stable. Therefore, separate models are prepared for passenger cars and big vehicles, and these models are run at the same time as the respective EKF models to ensure stability.","title":"EKF Tracker"},{"location":"perception/multi_object_tracker/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"perception/multi_object_tracker/#input","text":"Name Type Description ~/input autoware_auto_perception_msgs::msg::DetectedObjects obstacles","title":"Input"},{"location":"perception/multi_object_tracker/#output","text":"Name Type Description ~/output autoware_auto_perception_msgs::msg::TrackedObjects modified obstacles","title":"Output"},{"location":"perception/multi_object_tracker/#parameters","text":"","title":"Parameters"},{"location":"perception/multi_object_tracker/#core-parameters","text":"Name Type Description can_assign_matrix double Assignment table for data association max_dist_matrix double Maximum distance table for data association max_area_matrix double Maximum area table for data association min_area_matrix double Minimum area table for data association max_rad_matrix double Maximum angle table for data association world_frame_id double tracking frame enable_delay_compensation bool Estimate obstacles at current time considering detection delay publish_rate double if enable_delay_compensation is true, how many hertz to output","title":"Core Parameters"},{"location":"perception/multi_object_tracker/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"perception/multi_object_tracker/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"perception/multi_object_tracker/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"perception/multi_object_tracker/#evaluation-of-mussp","text":"According to our evaluation, muSSP is faster than normal SSP when the matrix size is more than 100. Execution time for varying matrix size at 95% sparsity. In real data, the sparsity was often around 95%. Execution time for varying the sparsity with matrix size 100.","title":"Evaluation of muSSP"},{"location":"perception/multi_object_tracker/#optional-referencesexternal-links","text":"This package makes use of external code. Name License Original Repository muSSP Apache 2.0 https://github.com/yu-lab-vt/muSSP [1] C. Wang, Y. Wang, Y. Wang, C.-t. Wu, and G. Yu, \u201cmuSSP: Efficient Min-cost Flow Algorithm for Multi-object Tracking,\u201d NeurIPS, 2019","title":"(Optional) References/External links"},{"location":"perception/multi_object_tracker/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"perception/object_merger/","text":"object_merger # Purpose # object_merger is a package for merging detected objects from two methods by data association. Inner-workings / Algorithms # The successive shortest path algorithm is used to solve the data association problem (the minimum-cost flow problem). The cost is calculated by the distance between two objects and gate functions are applied to reset cost, s.t. the maximum distance, the maximum area and the minimum area. Inputs / Outputs # Input # Name Type Description input/object0 autoware_auto_perception_msgs::msg::DetectedObjects detection objects input/object1 autoware_auto_perception_msgs::msg::DetectedObjects detection objects Output # Name Type Description output/object autoware_auto_perception_msgs::msg::DetectedObjects modified Objects Parameters # No Parameters. Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # (Optional) Future extensions / Unimplemented parts # Data association algorithm was the same as that of multi_object_tracker, but the algorithm of multi_object_tracker was already updated.","title":"object_merger"},{"location":"perception/object_merger/#object_merger","text":"","title":"object_merger"},{"location":"perception/object_merger/#purpose","text":"object_merger is a package for merging detected objects from two methods by data association.","title":"Purpose"},{"location":"perception/object_merger/#inner-workings-algorithms","text":"The successive shortest path algorithm is used to solve the data association problem (the minimum-cost flow problem). The cost is calculated by the distance between two objects and gate functions are applied to reset cost, s.t. the maximum distance, the maximum area and the minimum area.","title":"Inner-workings / Algorithms"},{"location":"perception/object_merger/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"perception/object_merger/#input","text":"Name Type Description input/object0 autoware_auto_perception_msgs::msg::DetectedObjects detection objects input/object1 autoware_auto_perception_msgs::msg::DetectedObjects detection objects","title":"Input"},{"location":"perception/object_merger/#output","text":"Name Type Description output/object autoware_auto_perception_msgs::msg::DetectedObjects modified Objects","title":"Output"},{"location":"perception/object_merger/#parameters","text":"No Parameters.","title":"Parameters"},{"location":"perception/object_merger/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"perception/object_merger/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"perception/object_merger/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"perception/object_merger/#optional-referencesexternal-links","text":"","title":"(Optional) References/External links"},{"location":"perception/object_merger/#optional-future-extensions-unimplemented-parts","text":"Data association algorithm was the same as that of multi_object_tracker, but the algorithm of multi_object_tracker was already updated.","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"perception/shape_estimation/","text":"shape_estimation # Purpose # This node calculates a refined object shape (bounding box, cylinder, convex hull) in which a pointcloud cluster fits according to a label. Inner-workings / Algorithms # Fitting algorithms # bounding box L-shape fitting. See reference below for details. cylinder cv::minEnclosingCircle convex hull cv::convexHull Inputs / Outputs # Input # Name Type Description input autoware_perception_msgs::msg::DetectedObjectsWithFeature detected objects with labeled cluster Output # Name Type Description output/objects autoware_auto_perception_msgs::msg::DetectedObjects detected objects with refined shape Parameters # Name Type Default Value Description use_corrector bool true The flag to apply rule-based filter use_filter bool true The flag to apply rule-based corrector use_vehicle_reference_yaw bool true The flag to use vehicle reference yaw for corrector Assumptions / Known limits # TBD References/External links # L-shape fitting implementation of the paper: @conference { Zhang-2017-26536 , author = {Xiao Zhang and Wenda Xu and Chiyu Dong and John M. Dolan} , title = {Efficient L-Shape Fitting for Vehicle Detection Using Laser Scanners} , booktitle = {2017 IEEE Intelligent Vehicles Symposium} , year = {2017} , month = {June} , keywords = {autonomous driving, laser scanner, perception, segmentation} , }","title":"shape_estimation"},{"location":"perception/shape_estimation/#shape_estimation","text":"","title":"shape_estimation"},{"location":"perception/shape_estimation/#purpose","text":"This node calculates a refined object shape (bounding box, cylinder, convex hull) in which a pointcloud cluster fits according to a label.","title":"Purpose"},{"location":"perception/shape_estimation/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"perception/shape_estimation/#fitting-algorithms","text":"bounding box L-shape fitting. See reference below for details. cylinder cv::minEnclosingCircle convex hull cv::convexHull","title":"Fitting algorithms"},{"location":"perception/shape_estimation/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"perception/shape_estimation/#input","text":"Name Type Description input autoware_perception_msgs::msg::DetectedObjectsWithFeature detected objects with labeled cluster","title":"Input"},{"location":"perception/shape_estimation/#output","text":"Name Type Description output/objects autoware_auto_perception_msgs::msg::DetectedObjects detected objects with refined shape","title":"Output"},{"location":"perception/shape_estimation/#parameters","text":"Name Type Default Value Description use_corrector bool true The flag to apply rule-based filter use_filter bool true The flag to apply rule-based corrector use_vehicle_reference_yaw bool true The flag to use vehicle reference yaw for corrector","title":"Parameters"},{"location":"perception/shape_estimation/#assumptions-known-limits","text":"TBD","title":"Assumptions / Known limits"},{"location":"perception/shape_estimation/#referencesexternal-links","text":"L-shape fitting implementation of the paper: @conference { Zhang-2017-26536 , author = {Xiao Zhang and Wenda Xu and Chiyu Dong and John M. Dolan} , title = {Efficient L-Shape Fitting for Vehicle Detection Using Laser Scanners} , booktitle = {2017 IEEE Intelligent Vehicles Symposium} , year = {2017} , month = {June} , keywords = {autonomous driving, laser scanner, perception, segmentation} , }","title":"References/External links"},{"location":"perception/tensorrt_yolo/","text":"tensorrt_yolo # Purpose # This package detects 2D bounding boxes for target objects e.g., cars, trucks, bicycles, and pedestrians on a image based on YOLO(You only look once) model. Inner-workings / Algorithms # Cite # yolov3 Redmon, J., & Farhadi, A. (2018). Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767. yolov4 Bochkovskiy, A., Wang, C. Y., & Liao, H. Y. M. (2020). Yolov4: Optimal speed and accuracy of object detection. arXiv preprint arXiv:2004.10934. yolov5 Jocher, G., et al. (2021). ultralytics/yolov5: v6.0 - YOLOv5n 'Nano' models, Roboflow integration, TensorFlow export, OpenCV DNN support (v6.0). Zenodo. https://doi.org/10.5281/zenodo.5563715 Inputs / Outputs # Input # Name Type Description in/image sensor_msgs/Image The input image Output # Name Type Description out/objects autoware_perception_msgs/DetectedObjectsWithFeature The detected objects with 2D bounding boxes out/image sensor_msgs/Image The image with 2D bounding boxes for visualization Parameters # Core Parameters # Name Type Default Value Description anchors double array [10.0, 13.0, 16.0, 30.0, 33.0, 23.0, 30.0, 61.0, 62.0, 45.0, 59.0, 119.0, 116.0, 90.0, 156.0, 198.0, 373.0, 326.0] The anchors to create bounding box candidates scale_x_y double array [1.0, 1.0, 1.0] The scale parameter to eliminate grid sensitivity score_thresh double 0.1 If the objectness score is less than this value, the object is ignored in yolo layer. iou_thresh double 0.45 The iou threshold for NMS method detections_per_im int 100 The maximum detection number for one frame use_darknet_layer bool true The flag to use yolo layer in darknet ignore_thresh double 0.5 If the output score is less than this value, ths object is ignored. Node Parameters # Name Type Default Value Description onnx_file string \"\" The onnx file name for yolo model engine_file string \"\" The tensorrt engine file name for yolo model label_file string \"\" The label file with label names for detected objects written on it calib_image_directory string \"\" The directory name including calibration images for int8 inference calib_cache_file string \"\" The calibration cache file for int8 inference mode string \"FP32\" The inference mode: \"FP32\", \"FP16\", \"INT8\" Assumptions / Known limits # This package includes multiple licenses. Onnx model # YOLOv3 # YOLOv3 : Converted from darknet weight file and conf file . YOLOv4 # YOLOv4 : Converted from darknet weight file and conf file . YOLOv5 # Refer to this guide YOLOv5s YOLOv5m YOLOv5l YOLOv5x Reference repositories # https://github.com/pjreddie/darknet https://github.com/AlexeyAB/darknet https://github.com/ultralytics/yolov5 https://github.com/wang-xinyu/tensorrtx https://github.com/NVIDIA/retinanet-examples","title":"tensorrt_yolo"},{"location":"perception/tensorrt_yolo/#tensorrt_yolo","text":"","title":"tensorrt_yolo"},{"location":"perception/tensorrt_yolo/#purpose","text":"This package detects 2D bounding boxes for target objects e.g., cars, trucks, bicycles, and pedestrians on a image based on YOLO(You only look once) model.","title":"Purpose"},{"location":"perception/tensorrt_yolo/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"perception/tensorrt_yolo/#cite","text":"yolov3 Redmon, J., & Farhadi, A. (2018). Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767. yolov4 Bochkovskiy, A., Wang, C. Y., & Liao, H. Y. M. (2020). Yolov4: Optimal speed and accuracy of object detection. arXiv preprint arXiv:2004.10934. yolov5 Jocher, G., et al. (2021). ultralytics/yolov5: v6.0 - YOLOv5n 'Nano' models, Roboflow integration, TensorFlow export, OpenCV DNN support (v6.0). Zenodo. https://doi.org/10.5281/zenodo.5563715","title":"Cite"},{"location":"perception/tensorrt_yolo/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"perception/tensorrt_yolo/#input","text":"Name Type Description in/image sensor_msgs/Image The input image","title":"Input"},{"location":"perception/tensorrt_yolo/#output","text":"Name Type Description out/objects autoware_perception_msgs/DetectedObjectsWithFeature The detected objects with 2D bounding boxes out/image sensor_msgs/Image The image with 2D bounding boxes for visualization","title":"Output"},{"location":"perception/tensorrt_yolo/#parameters","text":"","title":"Parameters"},{"location":"perception/tensorrt_yolo/#core-parameters","text":"Name Type Default Value Description anchors double array [10.0, 13.0, 16.0, 30.0, 33.0, 23.0, 30.0, 61.0, 62.0, 45.0, 59.0, 119.0, 116.0, 90.0, 156.0, 198.0, 373.0, 326.0] The anchors to create bounding box candidates scale_x_y double array [1.0, 1.0, 1.0] The scale parameter to eliminate grid sensitivity score_thresh double 0.1 If the objectness score is less than this value, the object is ignored in yolo layer. iou_thresh double 0.45 The iou threshold for NMS method detections_per_im int 100 The maximum detection number for one frame use_darknet_layer bool true The flag to use yolo layer in darknet ignore_thresh double 0.5 If the output score is less than this value, ths object is ignored.","title":"Core Parameters"},{"location":"perception/tensorrt_yolo/#node-parameters","text":"Name Type Default Value Description onnx_file string \"\" The onnx file name for yolo model engine_file string \"\" The tensorrt engine file name for yolo model label_file string \"\" The label file with label names for detected objects written on it calib_image_directory string \"\" The directory name including calibration images for int8 inference calib_cache_file string \"\" The calibration cache file for int8 inference mode string \"FP32\" The inference mode: \"FP32\", \"FP16\", \"INT8\"","title":"Node Parameters"},{"location":"perception/tensorrt_yolo/#assumptions-known-limits","text":"This package includes multiple licenses.","title":"Assumptions / Known limits"},{"location":"perception/tensorrt_yolo/#onnx-model","text":"","title":"Onnx model"},{"location":"perception/tensorrt_yolo/#yolov3","text":"YOLOv3 : Converted from darknet weight file and conf file .","title":"YOLOv3"},{"location":"perception/tensorrt_yolo/#yolov4","text":"YOLOv4 : Converted from darknet weight file and conf file .","title":"YOLOv4"},{"location":"perception/tensorrt_yolo/#yolov5","text":"Refer to this guide YOLOv5s YOLOv5m YOLOv5l YOLOv5x","title":"YOLOv5"},{"location":"perception/tensorrt_yolo/#reference-repositories","text":"https://github.com/pjreddie/darknet https://github.com/AlexeyAB/darknet https://github.com/ultralytics/yolov5 https://github.com/wang-xinyu/tensorrtx https://github.com/NVIDIA/retinanet-examples","title":"Reference repositories"},{"location":"perception/traffic_light_classifier/","text":"traffic_light_classifier # Purpose # traffic_light_classifier is a package for classifying traffic light labels using cropped image around a traffic light. This package has two classifier models: cnn_classifier and hsv_classifier . Inner-workings / Algorithms # cnn_classifier # Traffic light labels are classified by MobileNetV2. hsv_classifier # Traffic light colors (green, yellow and red) are classified in HSV model. About Label # The message type is designed to comply with the unified road signs proposed at the Vienna Convention . This idea has been also proposed in Autoware.Auto . There are rules for naming labels that nodes receive. One traffic light is represented by the following character string separated by commas. color1-shape1, color2-shape2 . For example, the simple red and red cross traffic light label must be expressed as \"red-circle, red-cross\". These colors and shapes are assigned to the message as follows: Inputs / Outputs # Input # Name Type Description ~/input/image sensor_msgs::msg::Image input image ~/input/rois autoware_auto_perception_msgs::msg::TrafficLightRoiArray rois of traffic lights Output # Name Type Description ~/output/traffic_signals autoware_auto_perception_msgs::msg::TrafficSignalArray classified signals ~/output/debug/image sensor_msgs::msg::Image image for debugging Parameters # Node Parameters # Name Type Description classifier_type int if the value is 1 , cnn_classifier is used Core Parameters # cnn_classifier # Name Type Description model_file_path str path to the model file label_file_path str path to the label file precision str TensorRT precision, fp16 or int8 input_c str the channel size of an input image input_h str the height of an input image input_w str the width of an input image hsv_classifier # Name Type Description green_min_h int the minimum hue of green color green_min_s int the minimum saturation of green color green_min_v int the minimum value (brightness) of green color green_max_h int the maximum hue of green color green_max_s int the maximum saturation of green color green_max_v int the maximum value (brightness) of green color yellow_min_h int the minimum hue of yellow color yellow_min_s int the minimum saturation of yellow color yellow_min_v int the minimum value (brightness) of yellow color yellow_max_h int the maximum hue of yellow color yellow_max_s int the maximum saturation of yellow color yellow_max_v int the maximum value (brightness) of yellow color red_min_h int the minimum hue of red color red_min_s int the minimum saturation of red color red_min_v int the minimum value (brightness) of red color red_max_h int the maximum hue of red color red_max_s int the maximum saturation of red color red_max_v int the maximum value (brightness) of red color Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # References/External links # [1] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov and L. Chen, \"MobileNetV2: Inverted Residuals and Linear Bottlenecks,\" 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, 2018, pp. 4510-4520, doi: 10.1109/CVPR.2018.00474. (Optional) Future extensions / Unimplemented parts #","title":"traffic_light_classifier"},{"location":"perception/traffic_light_classifier/#traffic_light_classifier","text":"","title":"traffic_light_classifier"},{"location":"perception/traffic_light_classifier/#purpose","text":"traffic_light_classifier is a package for classifying traffic light labels using cropped image around a traffic light. This package has two classifier models: cnn_classifier and hsv_classifier .","title":"Purpose"},{"location":"perception/traffic_light_classifier/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"perception/traffic_light_classifier/#cnn_classifier","text":"Traffic light labels are classified by MobileNetV2.","title":"cnn_classifier"},{"location":"perception/traffic_light_classifier/#hsv_classifier","text":"Traffic light colors (green, yellow and red) are classified in HSV model.","title":"hsv_classifier"},{"location":"perception/traffic_light_classifier/#about-label","text":"The message type is designed to comply with the unified road signs proposed at the Vienna Convention . This idea has been also proposed in Autoware.Auto . There are rules for naming labels that nodes receive. One traffic light is represented by the following character string separated by commas. color1-shape1, color2-shape2 . For example, the simple red and red cross traffic light label must be expressed as \"red-circle, red-cross\". These colors and shapes are assigned to the message as follows:","title":"About Label"},{"location":"perception/traffic_light_classifier/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"perception/traffic_light_classifier/#input","text":"Name Type Description ~/input/image sensor_msgs::msg::Image input image ~/input/rois autoware_auto_perception_msgs::msg::TrafficLightRoiArray rois of traffic lights","title":"Input"},{"location":"perception/traffic_light_classifier/#output","text":"Name Type Description ~/output/traffic_signals autoware_auto_perception_msgs::msg::TrafficSignalArray classified signals ~/output/debug/image sensor_msgs::msg::Image image for debugging","title":"Output"},{"location":"perception/traffic_light_classifier/#parameters","text":"","title":"Parameters"},{"location":"perception/traffic_light_classifier/#node-parameters","text":"Name Type Description classifier_type int if the value is 1 , cnn_classifier is used","title":"Node Parameters"},{"location":"perception/traffic_light_classifier/#core-parameters","text":"","title":"Core Parameters"},{"location":"perception/traffic_light_classifier/#cnn_classifier_1","text":"Name Type Description model_file_path str path to the model file label_file_path str path to the label file precision str TensorRT precision, fp16 or int8 input_c str the channel size of an input image input_h str the height of an input image input_w str the width of an input image","title":"cnn_classifier"},{"location":"perception/traffic_light_classifier/#hsv_classifier_1","text":"Name Type Description green_min_h int the minimum hue of green color green_min_s int the minimum saturation of green color green_min_v int the minimum value (brightness) of green color green_max_h int the maximum hue of green color green_max_s int the maximum saturation of green color green_max_v int the maximum value (brightness) of green color yellow_min_h int the minimum hue of yellow color yellow_min_s int the minimum saturation of yellow color yellow_min_v int the minimum value (brightness) of yellow color yellow_max_h int the maximum hue of yellow color yellow_max_s int the maximum saturation of yellow color yellow_max_v int the maximum value (brightness) of yellow color red_min_h int the minimum hue of red color red_min_s int the minimum saturation of red color red_min_v int the minimum value (brightness) of red color red_max_h int the maximum hue of red color red_max_s int the maximum saturation of red color red_max_v int the maximum value (brightness) of red color","title":"hsv_classifier"},{"location":"perception/traffic_light_classifier/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"perception/traffic_light_classifier/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"perception/traffic_light_classifier/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"perception/traffic_light_classifier/#referencesexternal-links","text":"[1] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov and L. Chen, \"MobileNetV2: Inverted Residuals and Linear Bottlenecks,\" 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, 2018, pp. 4510-4520, doi: 10.1109/CVPR.2018.00474.","title":"References/External links"},{"location":"perception/traffic_light_classifier/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"perception/traffic_light_map_based_detector/","text":"The traffic_light_map_based_detector Package # Overview # traffic_light_map_based_detector calculates where the traffic lights will appear in the image based on the HD map. Calibration and vibration errors can be entered as parameters, and the size of the detected RegionOfInterest will change according to the error. If the node receives route information, it only looks at traffic lights on that route. If the node receives no route information, it looks at a radius of 200 meters and the angle between the traffic light and the camera is less than 40 degrees. Input topics # Name Type Description ~input/vector_map autoware_auto_mapping_msgs::HADMapBin vector map ~input/camera_info sensor_msgs::CameraInfo target camera parameter ~input/route autoware_auto_planning_msgs::HADMapRoute optional: route Output topics # Name Type Description ~output/rois autoware_auto_perception_msgs::TrafficLightRoiArray location of traffic lights in image corresponding to the camera info ~debug/markers visualization_msgs::MarkerArray visualization to debug Node parameters # Parameter Type Description max_vibration_pitch double Maximum error in pitch direction. If -5~+5, it will be 10. max_vibration_yaw double Maximum error in yaw direction. If -5~+5, it will be 10. max_vibration_height double Maximum error in height direction. If -5~+5, it will be 10. max_vibration_width double Maximum error in width direction. If -5~+5, it will be 10. max_vibration_depth double Maximum error in depth direction. If -5~+5, it will be 10.","title":"The `traffic_light_map_based_detector` Package"},{"location":"perception/traffic_light_map_based_detector/#the-traffic_light_map_based_detector-package","text":"","title":"The traffic_light_map_based_detector Package"},{"location":"perception/traffic_light_map_based_detector/#overview","text":"traffic_light_map_based_detector calculates where the traffic lights will appear in the image based on the HD map. Calibration and vibration errors can be entered as parameters, and the size of the detected RegionOfInterest will change according to the error. If the node receives route information, it only looks at traffic lights on that route. If the node receives no route information, it looks at a radius of 200 meters and the angle between the traffic light and the camera is less than 40 degrees.","title":"Overview"},{"location":"perception/traffic_light_map_based_detector/#input-topics","text":"Name Type Description ~input/vector_map autoware_auto_mapping_msgs::HADMapBin vector map ~input/camera_info sensor_msgs::CameraInfo target camera parameter ~input/route autoware_auto_planning_msgs::HADMapRoute optional: route","title":"Input topics"},{"location":"perception/traffic_light_map_based_detector/#output-topics","text":"Name Type Description ~output/rois autoware_auto_perception_msgs::TrafficLightRoiArray location of traffic lights in image corresponding to the camera info ~debug/markers visualization_msgs::MarkerArray visualization to debug","title":"Output topics"},{"location":"perception/traffic_light_map_based_detector/#node-parameters","text":"Parameter Type Description max_vibration_pitch double Maximum error in pitch direction. If -5~+5, it will be 10. max_vibration_yaw double Maximum error in yaw direction. If -5~+5, it will be 10. max_vibration_height double Maximum error in height direction. If -5~+5, it will be 10. max_vibration_width double Maximum error in width direction. If -5~+5, it will be 10. max_vibration_depth double Maximum error in depth direction. If -5~+5, it will be 10.","title":"Node parameters"},{"location":"perception/traffic_light_ssd_fine_detector/","text":"traffic_light_ssd_fine_detector # Purpose # It is a package for traffic light detection using MobileNetV2 and SSDLite. The trained model is based on pytorch-ssd . Inner-workings / Algorithms # Based on the camera image and the global ROI array detected by map_based_detection node, a CNN-based detection method enables highly accurate traffic light detection. Inputs / Outputs # Input # Name Type Description ~/input/image sensor_msgs/Image The full size camera image ~/input/rois autoware_auto_perception_msgs::msg::TrafficLightRoiArray The array of ROIs detected by map_based_detector Output # Name Type Description ~/output/rois autoware_auto_perception_msgs::msg::TrafficLightRoiArray The detected accurate rois ~/debug/exe_time_ms autoware_debug_msgs::msg::Float32Stamped The time taken for inference Parameters # Core Parameters # Name Type Default Value Description score_thresh double 0.7 If the objectness score is less than this value, the object is ignored mean std::vector [0.5,0.5,0.5] Average value of the normalized values of the image data used for training std std::vector [0.5,0.5,0.5] Standard deviation of the normalized values of the image data used for training Node Parameters # Name Type Default Value Description onnx_file string \"./data/mb2-ssd-lite-tlr.onnx\" The onnx file name for yolo model label_file string \"./data/voc_labels_tl.txt\" The label file with label names for detected objects written on it mode string \"FP32\" The inference mode: \"FP32\", \"FP16\", \"INT8\" max_batch_size int 8 The size of the batch processed at one time by inference by TensorRT approximate_sync bool false Flag for whether to ues approximate sync policy Assumptions / Known limits # Onnx model # https://drive.google.com/uc?id=1USFDPRH9JrVdGoqt27qHjRgittwc0kcO Reference repositories # pytorch-ssd github repository https://github.com/qfgaohao/pytorch-ssd MobileNetV2 M. Sandler, A. Howard, M. Zhu, A. Zhmoginov and L. Chen, \"MobileNetV2: Inverted Residuals and Linear Bottlenecks,\" 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, 2018, pp. 4510-4520, doi: 10.1109/CVPR.2018.00474.","title":"traffic_light_ssd_fine_detector"},{"location":"perception/traffic_light_ssd_fine_detector/#traffic_light_ssd_fine_detector","text":"","title":"traffic_light_ssd_fine_detector"},{"location":"perception/traffic_light_ssd_fine_detector/#purpose","text":"It is a package for traffic light detection using MobileNetV2 and SSDLite. The trained model is based on pytorch-ssd .","title":"Purpose"},{"location":"perception/traffic_light_ssd_fine_detector/#inner-workings-algorithms","text":"Based on the camera image and the global ROI array detected by map_based_detection node, a CNN-based detection method enables highly accurate traffic light detection.","title":"Inner-workings / Algorithms"},{"location":"perception/traffic_light_ssd_fine_detector/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"perception/traffic_light_ssd_fine_detector/#input","text":"Name Type Description ~/input/image sensor_msgs/Image The full size camera image ~/input/rois autoware_auto_perception_msgs::msg::TrafficLightRoiArray The array of ROIs detected by map_based_detector","title":"Input"},{"location":"perception/traffic_light_ssd_fine_detector/#output","text":"Name Type Description ~/output/rois autoware_auto_perception_msgs::msg::TrafficLightRoiArray The detected accurate rois ~/debug/exe_time_ms autoware_debug_msgs::msg::Float32Stamped The time taken for inference","title":"Output"},{"location":"perception/traffic_light_ssd_fine_detector/#parameters","text":"","title":"Parameters"},{"location":"perception/traffic_light_ssd_fine_detector/#core-parameters","text":"Name Type Default Value Description score_thresh double 0.7 If the objectness score is less than this value, the object is ignored mean std::vector [0.5,0.5,0.5] Average value of the normalized values of the image data used for training std std::vector [0.5,0.5,0.5] Standard deviation of the normalized values of the image data used for training","title":"Core Parameters"},{"location":"perception/traffic_light_ssd_fine_detector/#node-parameters","text":"Name Type Default Value Description onnx_file string \"./data/mb2-ssd-lite-tlr.onnx\" The onnx file name for yolo model label_file string \"./data/voc_labels_tl.txt\" The label file with label names for detected objects written on it mode string \"FP32\" The inference mode: \"FP32\", \"FP16\", \"INT8\" max_batch_size int 8 The size of the batch processed at one time by inference by TensorRT approximate_sync bool false Flag for whether to ues approximate sync policy","title":"Node Parameters"},{"location":"perception/traffic_light_ssd_fine_detector/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"perception/traffic_light_ssd_fine_detector/#onnx-model","text":"https://drive.google.com/uc?id=1USFDPRH9JrVdGoqt27qHjRgittwc0kcO","title":"Onnx model"},{"location":"perception/traffic_light_ssd_fine_detector/#reference-repositories","text":"pytorch-ssd github repository https://github.com/qfgaohao/pytorch-ssd MobileNetV2 M. Sandler, A. Howard, M. Zhu, A. Zhmoginov and L. Chen, \"MobileNetV2: Inverted Residuals and Linear Bottlenecks,\" 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, 2018, pp. 4510-4520, doi: 10.1109/CVPR.2018.00474.","title":"Reference repositories"},{"location":"planning/costmap_generator/","text":"costmap_generator # costmap_generator_node # This node reads PointCloud and/or DynamicObjectArray and creates an OccupancyGrid and GridMap . VectorMap(Lanelet2) is optional. Input topics # Name Type Description ~input/objects autoware_auto_perception_msgs::PredictedObjects predicted objects, for obstacles areas ~input/points_no_ground sensor_msgs::PointCloud2 ground-removed points, for obstacle areas which can't be detected as objects ~input/vector_map autoware_auto_mapping_msgs::HADMapBin vector map, for drivable areas ~input/scenario autoware_planning_msgs::Scenario scenarios to be activated, for node activation Output topics # Name Type Description ~output/grid_map grid_map_msgs::GridMap costmap as GridMap, values are from 0.0 to 1.0 ~output/occupancy_grid nav_msgs::OccupancyGrid costmap as OccupancyGrid, values are from 0 to 100 Output TFs # None How to launch # Write your remapping info in costmap_generator.launch or add args when executing roslaunch Run roslaunch costmap_generator costmap_generator.launch Parameters # Name Type Description update_rate double timer's update rate use_objects bool whether using ~input/objects or not use_points bool whether using ~input/points_no_ground or not use_wayarea bool whether using wayarea from ~input/vector_map or not costmap_frame string created costmap's coordinate vehicle_frame string vehicle's coordinate map_frame string map's coordinate grid_min_value double minimum cost for gridmap grid_max_value double maximum cost for gridmap grid_resolution double resolution for gridmap grid_length_x int size of gridmap for x direction grid_length_y int size of gridmap for y direction grid_position_x int offset from coordinate in x direction grid_position_y int offset from coordinate in y direction maximum_lidar_height_thres double maximum height threshold for pointcloud data minimum_lidar_height_thres double minimum height threshold for pointcloud data expand_rectangle_size double expand object's rectangle with this value size_of_expansion_kernel int kernel size for blurring effect on object's costmap Flowchart #","title":"costmap_generator"},{"location":"planning/costmap_generator/#costmap_generator","text":"","title":"costmap_generator"},{"location":"planning/costmap_generator/#costmap_generator_node","text":"This node reads PointCloud and/or DynamicObjectArray and creates an OccupancyGrid and GridMap . VectorMap(Lanelet2) is optional.","title":"costmap_generator_node"},{"location":"planning/costmap_generator/#input-topics","text":"Name Type Description ~input/objects autoware_auto_perception_msgs::PredictedObjects predicted objects, for obstacles areas ~input/points_no_ground sensor_msgs::PointCloud2 ground-removed points, for obstacle areas which can't be detected as objects ~input/vector_map autoware_auto_mapping_msgs::HADMapBin vector map, for drivable areas ~input/scenario autoware_planning_msgs::Scenario scenarios to be activated, for node activation","title":"Input topics"},{"location":"planning/costmap_generator/#output-topics","text":"Name Type Description ~output/grid_map grid_map_msgs::GridMap costmap as GridMap, values are from 0.0 to 1.0 ~output/occupancy_grid nav_msgs::OccupancyGrid costmap as OccupancyGrid, values are from 0 to 100","title":"Output topics"},{"location":"planning/costmap_generator/#output-tfs","text":"None","title":"Output TFs"},{"location":"planning/costmap_generator/#how-to-launch","text":"Write your remapping info in costmap_generator.launch or add args when executing roslaunch Run roslaunch costmap_generator costmap_generator.launch","title":"How to launch"},{"location":"planning/costmap_generator/#parameters","text":"Name Type Description update_rate double timer's update rate use_objects bool whether using ~input/objects or not use_points bool whether using ~input/points_no_ground or not use_wayarea bool whether using wayarea from ~input/vector_map or not costmap_frame string created costmap's coordinate vehicle_frame string vehicle's coordinate map_frame string map's coordinate grid_min_value double minimum cost for gridmap grid_max_value double maximum cost for gridmap grid_resolution double resolution for gridmap grid_length_x int size of gridmap for x direction grid_length_y int size of gridmap for y direction grid_position_x int offset from coordinate in x direction grid_position_y int offset from coordinate in y direction maximum_lidar_height_thres double maximum height threshold for pointcloud data minimum_lidar_height_thres double minimum height threshold for pointcloud data expand_rectangle_size double expand object's rectangle with this value size_of_expansion_kernel int kernel size for blurring effect on object's costmap","title":"Parameters"},{"location":"planning/costmap_generator/#flowchart","text":"","title":"Flowchart"},{"location":"planning/obstacle_avoidance_planner/","text":"Obstacle Avoidance Planner # Purpose # This package generates a trajectory that is feasible to drive and collision free based on a reference path, drivable area, and static/dynamic obstacles. Only position and orientation of trajectory are calculated in this module, and velocity or acceleration will be updated in the latter modules. Inputs / Outputs # input # Name Type Description ~/input/path autoware_auto_planning_msgs/Path Reference path and the corresponding drivable area ~/input/objects autoware_auto_perception_msgs/PredictedObjects Recognized objects around the vehicle /localization/kinematic_state nav_msgs/Odometry Current Velocity of ego vehicle /planning/scenario_planning/lane_driving/obstacle_avoidance_approval autoware_planning_msgs/EnableAvoidance Approval to execute obstacle avoidance output # Name Type Description ~/output/path autoware_auto_planning_msgs/Trajectory Optimized trajectory that is feasible to drive and collision-free Flowchart # Each module is explained briefly here based on the flowchart. Manage trajectory generation # When one of the following conditions area realized, callback function to generate a trajectory is called and publish the trajectory. Otherwise, previously generated trajectory is published. Ego moves a certain distance compared to the previous ego pose (default: 10.0 [m]) Time passes (default: 1.0 [s]) The path shape is changed (e.g. when starting planning lane change) Ego is far from the previously generated trajectory The previously generated trajectory is memorized, but it is not when the path shape is changed and ego is far from the previously generated trajectory. Select objects to avoid # Only obstacles that are static and locate in a shoulder lane is decided to avoid. In detail, this equals to the following three conditions at the same time, and the red obstacles in the figure (id: 3, 4, 5) is to be avoided. Velocity is under a certain value (default: 0.1 [m/s]) CoG of the obstacles is not on the center line so that the ego will not avoid the car in front of the ego in the same lane. At least one point of the obstacle polygon is outside the drivable area. Generate object costmap # Cost map is generated according to the distance to the target obstacles to be avoided. Generate road boundary costmap # Cost map is generated according to the distance to the road boundaries. These cost maps area used in the optimization to generate a collision-free trajectory. Smooth path # The latter optimization assumes that the reference path is smooth enough. Therefore the path from behavior is smoothed here, and send to the optimization as a format of trajectory. Obstacles are not considered. Optimize trajectory # This module makes the trajectory kinematically-feasible and collision-free. We define vehicle pose in the frenet coordinate, and minimize tracking errors by optimization. This optimization also considers vehicle kinematics and collision checking with road boundary and obstacles. To decrease the computation cost, the optimization is applied to the shorter trajectory than the whole trajectory, and concatenate the remained trajectory with the optimized one at last. The trajectory just in front of the ego must not be changed a lot so that the steering wheel will be stable. Therefore, we use the previously generated trajectory in front of the ego. Optimization center on the vehicle, that tries to locate just on the trajectory, can be tuned along side the vehicle vertical axis. This parameter optimization center offset is defined as the signed length from the back-wheel center to the optimization center. Some examples are shown in the following figure, and it is shown that the trajectory of vehicle shape differs according to the optimization center even if the reference trajectory (green one) is the same. Check drivability, and extend trajectory # Optimized trajectory is too short for velocity planning, therefore extend the trajectory by concatenating the optimized trajectory and the behavior path considering drivability. Generated trajectory is checked if it is inside the drivable area or not, and if outside drivable area, output a trajectory inside drivable area with the behavior path or the previously generated trajectory. As described above, the behavior path is separated into two paths: one is for optimization and the other is the remain. The first path becomes optimized trajectory, and the second path just is transformed to a trajectory. Then a trajectory inside the drivable area is calculated as follows. If optimized trajectory is inside the drivable area , and the remained trajectory is inside/outside the drivable area, the output trajectory will be just concatenation of those two trajectories. In this case, we do not care if the remained trajectory is inside or outside the drivable area since generally it is outside the drivable area (especially in a narrow road), but we want to pass a trajectory as long as possible to the latter module. If optimized trajectory is outside the drivable area , and the remained trajectory is inside/outside the drivable area, and if the previously generated trajectory is memorized , the output trajectory will be a part of previously generated trajectory, whose end firstly goes outside the drivable area. and if the previously generated trajectory is not memorized , the output trajectory will be a part of trajectory just transformed from the behavior path, whose end firstly goes outside the drivable area. Optimization failure is dealt with the same as if the optimized trajectory is outside the drivable area. The output trajectory is memorized as a previously generated trajectory for the next cycle. Rationale In the current design, since there are some modelling errors, the constraints are considered to be soft constraints. Therefore, we have to make sure that the optimized trajectory is inside the drivable area or not after optimization. Assign trajectory velocity # Velocity is assigned in the result trajectory by the behavior path. The shapes of the trajectory and the path are different, therefore the each nearest trajectory point to the path is searched and interpolated linearly. Algorithms # In this section, Elastic band (to smooth the path) and Model Predictive Trajectory (to optimize the trajectory) will be explained in detail. Elastic band # Abstract # Elastic band smooths the path generated in the behavior. Since the latter process of optimization uses the curvature and normal vector of the reference path, smoothing should be applied here so that the optimization will be stable. This smoothing process does not consider collision. Therefore the output path may have a collision with road boundaries or obstacles. Formulation # We formulate a QP problem minimizing the distance between the previous point and the next point for each point. Conditions that each point can move to a certain extent are used so that the path will not changed a lot but will be smoother. For k k 'th point ( \\boldsymbol{p}_k \\boldsymbol{p}_k ), the objective function is as follows. The beginning and end point are fixed during the optimization. \\min \\sum_{k=1}^{n-1} |\\boldsymbol{p}_{k+1} - \\boldsymbol{p}_{k}| - |\\boldsymbol{p}_{k} - \\boldsymbol{p}_{k-1}| \\min \\sum_{k=1}^{n-1} |\\boldsymbol{p}_{k+1} - \\boldsymbol{p}_{k}| - |\\boldsymbol{p}_{k} - \\boldsymbol{p}_{k-1}| Model predictive trajectory # Abstract # Model Predictive Trajectory (MPT) calculates the trajectory that realizes the following conditions. Kinematically feasible for linear vehicle kinematics model Collision free with obstacles and road boundaries Conditions for collision free is considered to be not hard constraints but soft constraints. When the optimization failed or the optimized trajectory is not collision free, the output trajectory will be previously generated trajectory. Trajectory near the ego must be stable, therefore the condition where trajectory points near the ego are the same as previously generated trajectory is considered, and this is the only hard constraints in MPT. Formulation # As the following figure, we consider the bicycle kinematics model in the frenet frame to track the reference path. At time step k k , we define lateral distance to the reference path, heading angle against the reference path, and steer angle as y_k y_k , \\theta_k \\theta_k , and \\delta_k \\delta_k respectively. Assuming that the commanded steer angle is \\delta_{des, k} \\delta_{des, k} , the kinematics model in the frenet frame is formulated as follows. We also assume that the steer angle \\delta_k \\delta_k is first-order lag to the commanded one. \\begin{align} y_{k+1} & = y_{k} + v \\sin \\theta_k dt \\\\ \\theta_{k+1} & = \\theta_k + \\frac{v \\tan \\delta_k}{L}dt - \\kappa_k v \\cos \\theta_k dt \\\\ \\delta_{k+1} & = \\delta_k - \\frac{\\delta_k - \\delta_{des,k}}{\\tau}dt \\end{align} \\begin{align} y_{k+1} & = y_{k} + v \\sin \\theta_k dt \\\\ \\theta_{k+1} & = \\theta_k + \\frac{v \\tan \\delta_k}{L}dt - \\kappa_k v \\cos \\theta_k dt \\\\ \\delta_{k+1} & = \\delta_k - \\frac{\\delta_k - \\delta_{des,k}}{\\tau}dt \\end{align} Then we linearize these equations. y_k y_k and \\theta_k \\theta_k are tracking errors, so we assume that those are small enough. Therefore \\sin \\theta_k \\approx \\theta_k \\sin \\theta_k \\approx \\theta_k . Since \\delta_k \\delta_k is a steer angle, it is not always small. By using a reference steer angle \\delta_{\\mathrm{ref}, k} \\delta_{\\mathrm{ref}, k} calculated by the reference path curvature \\kappa_k \\kappa_k , we express \\delta_k \\delta_k with a small value \\Delta \\delta_k \\Delta \\delta_k . Note that the steer angle \\delta_k \\delta_k is within the steer angle limitation \\delta_{\\max} \\delta_{\\max} . When the reference steer angle \\delta_{\\mathrm{ref}, k} \\delta_{\\mathrm{ref}, k} is larger than the steer angle limitation \\delta_{\\max} \\delta_{\\max} , and \\delta_{\\mathrm{ref}, k} \\delta_{\\mathrm{ref}, k} is used to linearize the steer angle, \\Delta \\delta_k \\Delta \\delta_k is \\Delta \\delta_k = \\delta - \\delta_{\\mathrm{ref}, k} = \\delta_{\\max} - \\delta_{\\mathrm{ref}, k} \\Delta \\delta_k = \\delta - \\delta_{\\mathrm{ref}, k} = \\delta_{\\max} - \\delta_{\\mathrm{ref}, k} , and the absolute \\Delta \\delta_k \\Delta \\delta_k gets larger. Therefore, we have to apply the steer angle limitation to \\delta_{\\mathrm{ref}, k} \\delta_{\\mathrm{ref}, k} as well. \\begin{align} \\delta_{\\mathrm{ref}, k} & = \\mathrm{clamp}(\\arctan(L \\kappa_k), -\\delta_{\\max}, \\delta_{\\max}) \\\\ \\delta_k & = \\delta_{\\mathrm{ref}, k} + \\Delta \\delta_k, \\ \\Delta \\delta_k \\ll 1 \\\\ \\end{align} \\begin{align} \\delta_{\\mathrm{ref}, k} & = \\mathrm{clamp}(\\arctan(L \\kappa_k), -\\delta_{\\max}, \\delta_{\\max}) \\\\ \\delta_k & = \\delta_{\\mathrm{ref}, k} + \\Delta \\delta_k, \\ \\Delta \\delta_k \\ll 1 \\\\ \\end{align} \\mathrm{clamp}(v, v_{\\min}, v_{\\max}) \\mathrm{clamp}(v, v_{\\min}, v_{\\max}) is a function to convert v v to be larger than v_{\\min} v_{\\min} and smaller than v_{\\max} v_{\\max} . Using this \\delta_{\\mathrm{ref}, k} \\delta_{\\mathrm{ref}, k} , \\tan \\delta_k \\tan \\delta_k is linearized as follows. \\begin{align} \\tan \\delta_k & \\approx \\tan \\delta_{\\mathrm{ref}, k} + \\left.\\frac{d \\tan \\delta}{d \\delta}\\right|_{\\delta = \\delta_{\\mathrm{ref}, k}} \\Delta \\delta_k \\\\ & = \\tan \\delta_{\\mathrm{ref}, k} + \\left.\\frac{d \\tan \\delta}{d \\delta}\\right|_{\\delta = \\delta_{\\mathrm{ref}, k}} (\\delta_{\\mathrm{ref}, k} - \\delta_k) \\\\ & = \\tan \\delta_{\\mathrm{ref}, k} - \\frac{\\delta_{\\mathrm{ref}, k}}{\\cos^2 \\delta_{\\mathrm{ref}, k}} + \\frac{1}{\\cos^2 \\delta_{\\mathrm{ref}, k}} \\delta_k \\end{align} \\begin{align} \\tan \\delta_k & \\approx \\tan \\delta_{\\mathrm{ref}, k} + \\left.\\frac{d \\tan \\delta}{d \\delta}\\right|_{\\delta = \\delta_{\\mathrm{ref}, k}} \\Delta \\delta_k \\\\ & = \\tan \\delta_{\\mathrm{ref}, k} + \\left.\\frac{d \\tan \\delta}{d \\delta}\\right|_{\\delta = \\delta_{\\mathrm{ref}, k}} (\\delta_{\\mathrm{ref}, k} - \\delta_k) \\\\ & = \\tan \\delta_{\\mathrm{ref}, k} - \\frac{\\delta_{\\mathrm{ref}, k}}{\\cos^2 \\delta_{\\mathrm{ref}, k}} + \\frac{1}{\\cos^2 \\delta_{\\mathrm{ref}, k}} \\delta_k \\end{align} Based on the linearization, the error kinematics is formulated with the following linear equations. \\begin{align} \\begin{pmatrix} y_{k+1} \\\\ \\theta_{k+1} \\\\ \\delta_{k+1} \\end{pmatrix} = \\begin{pmatrix} 1 & v dt & 0 \\\\ 0 & 1 & \\frac{v dt}{L \\cos^{2} \\delta_{\\mathrm{ref}, k}} \\\\ 0 & 0 & 1 - \\frac{dt}{\\tau} \\end{pmatrix} \\begin{pmatrix} y_k \\\\ \\theta_k \\\\ \\delta_k \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 0 \\\\ \\frac{dt}{\\tau} \\end{pmatrix} \\delta_{des} + \\begin{pmatrix} 0 \\\\ \\frac{v \\tan(\\delta_{\\mathrm{ref}, k}) dt}{L} - \\frac{v \\delta_{\\mathrm{ref}, k} dt}{L \\cos^{2} \\delta_{\\mathrm{ref}, k}} - \\kappa_k v dt\\\\ 0 \\end{pmatrix} \\end{align} \\begin{align} \\begin{pmatrix} y_{k+1} \\\\ \\theta_{k+1} \\\\ \\delta_{k+1} \\end{pmatrix} = \\begin{pmatrix} 1 & v dt & 0 \\\\ 0 & 1 & \\frac{v dt}{L \\cos^{2} \\delta_{\\mathrm{ref}, k}} \\\\ 0 & 0 & 1 - \\frac{dt}{\\tau} \\end{pmatrix} \\begin{pmatrix} y_k \\\\ \\theta_k \\\\ \\delta_k \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 0 \\\\ \\frac{dt}{\\tau} \\end{pmatrix} \\delta_{des} + \\begin{pmatrix} 0 \\\\ \\frac{v \\tan(\\delta_{\\mathrm{ref}, k}) dt}{L} - \\frac{v \\delta_{\\mathrm{ref}, k} dt}{L \\cos^{2} \\delta_{\\mathrm{ref}, k}} - \\kappa_k v dt\\\\ 0 \\end{pmatrix} \\end{align} The objective function for smoothing and tracking is shown as follows. \\begin{align} J_1 & (y_{0...N-1}, \\theta_{0...N-1}, \\delta_{0...N-1}) \\\\ & = w_y \\sum_{k} y_k^2 + w_{\\theta} \\sum_{k} \\theta_k^2 + w_{\\delta} \\sum_k \\delta_k^2 + w_{\\dot{\\delta}} \\sum_k \\dot{\\delta}_k^2 + w_{\\ddot{\\delta}} \\sum_k \\ddot{\\delta}_k^2 \\end{align} \\begin{align} J_1 & (y_{0...N-1}, \\theta_{0...N-1}, \\delta_{0...N-1}) \\\\ & = w_y \\sum_{k} y_k^2 + w_{\\theta} \\sum_{k} \\theta_k^2 + w_{\\delta} \\sum_k \\delta_k^2 + w_{\\dot{\\delta}} \\sum_k \\dot{\\delta}_k^2 + w_{\\ddot{\\delta}} \\sum_k \\ddot{\\delta}_k^2 \\end{align} As mentioned before, the constraints to be collision free with obstacles and road boundaries are formulated to be soft constraints. Assuming that the lateral distance to the road boundaries or obstacles from the back wheel center, front wheel center, and the point between them are y_{\\mathrm{base}, k}, y_{\\mathrm{top}, k}, y_{\\mathrm{mid}, k} y_{\\mathrm{base}, k}, y_{\\mathrm{top}, k}, y_{\\mathrm{mid}, k} respectively, and slack variables for each point are \\lambda_{\\mathrm{base}}, \\lambda_{\\mathrm{top}}, \\lambda_{\\mathrm{mid}} \\lambda_{\\mathrm{base}}, \\lambda_{\\mathrm{top}}, \\lambda_{\\mathrm{mid}} , the soft constraints can be formulated as follows. y_{\\mathrm{base}, k, \\min} - \\lambda_{\\mathrm{base}, k} \\leq y_{\\mathrm{base}, k} (y_k) \\leq y_{\\mathrm{base}, k, \\max} + \\lambda_{\\mathrm{base}, k}\\\\ y_{\\mathrm{top}, k, \\min} - \\lambda_{\\mathrm{top}, k} \\leq y_{\\mathrm{top}, k} (y_k) \\leq y_{\\mathrm{top}, k, \\max} + \\lambda_{\\mathrm{top}, k}\\\\ y_{\\mathrm{mid}, k, \\min} - \\lambda_{\\mathrm{mid}, k} \\leq y_{\\mathrm{mid}, k} (y_k) \\leq y_{\\mathrm{mid}, k, \\max} + \\lambda_{\\mathrm{mid}, k} y_{\\mathrm{base}, k, \\min} - \\lambda_{\\mathrm{base}, k} \\leq y_{\\mathrm{base}, k} (y_k) \\leq y_{\\mathrm{base}, k, \\max} + \\lambda_{\\mathrm{base}, k}\\\\ y_{\\mathrm{top}, k, \\min} - \\lambda_{\\mathrm{top}, k} \\leq y_{\\mathrm{top}, k} (y_k) \\leq y_{\\mathrm{top}, k, \\max} + \\lambda_{\\mathrm{top}, k}\\\\ y_{\\mathrm{mid}, k, \\min} - \\lambda_{\\mathrm{mid}, k} \\leq y_{\\mathrm{mid}, k} (y_k) \\leq y_{\\mathrm{mid}, k, \\max} + \\lambda_{\\mathrm{mid}, k} Since y_{\\mathrm{base}, k}, y_{\\mathrm{top}, k}, y_{\\mathrm{mid}, k} y_{\\mathrm{base}, k}, y_{\\mathrm{top}, k}, y_{\\mathrm{mid}, k} is formulated as a linear function of y_k y_k , the objective function for soft constraints is formulated as follows. \\begin{align} J_2 & (\\lambda_{\\mathrm{base}, 0...N-1}, \\lambda_{\\mathrm{mid}, 0...N-1}, \\lambda_{\\mathrm{top}, 0...N-1}) \\\\ & = w_{\\mathrm{base}} \\sum_{k} \\lambda_{\\mathrm{base}, k}^2 + w_{\\mathrm{mid}} \\sum_k \\lambda_{\\mathrm{mid}, k}^2 + w_{\\mathrm{top}} \\sum_k \\lambda_{\\mathrm{top}, k}^2 \\end{align} \\begin{align} J_2 & (\\lambda_{\\mathrm{base}, 0...N-1}, \\lambda_{\\mathrm{mid}, 0...N-1}, \\lambda_{\\mathrm{top}, 0...N-1}) \\\\ & = w_{\\mathrm{base}} \\sum_{k} \\lambda_{\\mathrm{base}, k}^2 + w_{\\mathrm{mid}} \\sum_k \\lambda_{\\mathrm{mid}, k}^2 + w_{\\mathrm{top}} \\sum_k \\lambda_{\\mathrm{top}, k}^2 \\end{align} Slack variables are also design variables for optimization. We define a vector \\boldsymbol{x} \\boldsymbol{x} , that concatenates all the design variables. \\begin{align} \\boldsymbol{x} = \\begin{pmatrix} ... & y_k & \\lambda_{\\mathrm{base}, k} & \\lambda_{\\mathrm{top}, k} & \\lambda_{\\mathrm{mid}, k} & ... \\end{pmatrix}^T \\end{align} \\begin{align} \\boldsymbol{x} = \\begin{pmatrix} ... & y_k & \\lambda_{\\mathrm{base}, k} & \\lambda_{\\mathrm{top}, k} & \\lambda_{\\mathrm{mid}, k} & ... \\end{pmatrix}^T \\end{align} The summation of these two objective functions is the objective function for the optimization problem. \\begin{align} \\min_{\\boldsymbol{x}} J (\\boldsymbol{x}) = \\min_{\\boldsymbol{x}} J_1 & (y_{0...N-1}, \\theta_{0...N-1}, \\delta_{0...N-1}) + J_2 (\\lambda_{\\mathrm{base}, 0...N-1}, \\lambda_{\\mathrm{mid}, 0...N-1}, \\lambda_{\\mathrm{top}, 0...N-1}) \\end{align} \\begin{align} \\min_{\\boldsymbol{x}} J (\\boldsymbol{x}) = \\min_{\\boldsymbol{x}} J_1 & (y_{0...N-1}, \\theta_{0...N-1}, \\delta_{0...N-1}) + J_2 (\\lambda_{\\mathrm{base}, 0...N-1}, \\lambda_{\\mathrm{mid}, 0...N-1}, \\lambda_{\\mathrm{top}, 0...N-1}) \\end{align} As mentioned before, we use hard constraints where some trajectory points in front of the ego are the same as the previously generated trajectory points. This hard constraints is formulated as follows. \\begin{align} \\delta_k = \\delta_{k}^{\\mathrm{prev}} (0 \\leq i \\leq N_{\\mathrm{fix}}) \\end{align} \\begin{align} \\delta_k = \\delta_{k}^{\\mathrm{prev}} (0 \\leq i \\leq N_{\\mathrm{fix}}) \\end{align} Finally we transform those objective functions to the following QP problem, and solve it. \\begin{align} \\min_{\\boldsymbol{x}} \\ & \\frac{1}{2} \\boldsymbol{x}^T \\boldsymbol{P} \\boldsymbol{x} + \\boldsymbol{q} \\boldsymbol{x} \\\\ \\mathrm{s.t.} \\ & \\boldsymbol{b}_l \\leq \\boldsymbol{A} \\boldsymbol{x} \\leq \\boldsymbol{b}_u \\end{align} \\begin{align} \\min_{\\boldsymbol{x}} \\ & \\frac{1}{2} \\boldsymbol{x}^T \\boldsymbol{P} \\boldsymbol{x} + \\boldsymbol{q} \\boldsymbol{x} \\\\ \\mathrm{s.t.} \\ & \\boldsymbol{b}_l \\leq \\boldsymbol{A} \\boldsymbol{x} \\leq \\boldsymbol{b}_u \\end{align} Limitation # When turning right or left in the intersection, the output trajectory is close to the outside road boundary. Roles of planning for behavior_path_planner and obstacle_avoidance_planner are not decided clearly. High computation cost Comparison to other methods # Planning a trajectory that satisfies kinematic feasibility and collision-free has two main characteristics that makes hard to be solved: one is non-convex and the other is high dimension. According to the characteristics, we investigate pros and cons of the typical planning methods: optimization-based, sampling-based, and learning-based method. Optimization-based method # pros: comparatively fast against high dimension by leveraging the gradient descent cons: often converge to the local minima in the non-convex problem Sampling-based method # pros: realize global optimization cons: high computation cost especially in the complex case Learning-based method # under research yet Based on these pros/cons, we chose the optimization-based planner first. Although it has a cons to converge to the local minima, it can get a good solution by the preprocessing to approximate the convex problem that almost equals to the original non-convex problem. How to debug # Topics for debugging will be explained in this section. interpolated_points_marker Trajectory points that is resampled from the input trajectory of obstacle avoidance planner. Whether this trajectory is inside the drivable area, smooth enough can be checked. smoothed_points_text The output trajectory points from Elastic Band. Not points but small numbers are visualized. Whether these smoothed points are distorted or not can be checked. (base/top/mid)_bounds_line Lateral Distance to the road boundaries to check collision in MPT (More precisely, - vehicle_width / 2.0). This collision check is done with three points on the vehicle (base = back wheel center, top, mid), therefore three line markers are visualized for each trajectory point. If the distance between the edge of line markers and the road boundaries is not about the half width of the vehicle, collision check will fail. optimized_points_marker The output trajectory points from MPT. Whether the trajectory is outside the drivable area can be checked. Trajectory with footprint Trajectory footprints can be visualized by TrajectoryFootprint of rviz_plugin. Whether trajectory footprints of input/output of obstacle_avoidance_planner is inside the drivable area or not can be checked. Drivable Area Drivable area. When drivable area generation failed, the drawn area may be distorted. nav_msgs/msg/OccupancyGrid topic name: /planning/scenario_planning/lane_driving/behavior_planning/behavior_path_planner/debug/drivable_area area_with_objects Area where obstacles are removed from the drivable area nav_msgs/msg/OccupancyGrid object_clearance_map Cost map for obstacles (distance to the target obstacles to be avoided) nav_msgs/msg/OccupancyGrid clearance_map Cost map for drivable area (distance to the road boundaries) nav_msgs/msg/OccupancyGrid","title":"Obstacle Avoidance Planner"},{"location":"planning/obstacle_avoidance_planner/#obstacle-avoidance-planner","text":"","title":"Obstacle Avoidance Planner"},{"location":"planning/obstacle_avoidance_planner/#purpose","text":"This package generates a trajectory that is feasible to drive and collision free based on a reference path, drivable area, and static/dynamic obstacles. Only position and orientation of trajectory are calculated in this module, and velocity or acceleration will be updated in the latter modules.","title":"Purpose"},{"location":"planning/obstacle_avoidance_planner/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"planning/obstacle_avoidance_planner/#input","text":"Name Type Description ~/input/path autoware_auto_planning_msgs/Path Reference path and the corresponding drivable area ~/input/objects autoware_auto_perception_msgs/PredictedObjects Recognized objects around the vehicle /localization/kinematic_state nav_msgs/Odometry Current Velocity of ego vehicle /planning/scenario_planning/lane_driving/obstacle_avoidance_approval autoware_planning_msgs/EnableAvoidance Approval to execute obstacle avoidance","title":"input"},{"location":"planning/obstacle_avoidance_planner/#output","text":"Name Type Description ~/output/path autoware_auto_planning_msgs/Trajectory Optimized trajectory that is feasible to drive and collision-free","title":"output"},{"location":"planning/obstacle_avoidance_planner/#flowchart","text":"Each module is explained briefly here based on the flowchart.","title":"Flowchart"},{"location":"planning/obstacle_avoidance_planner/#manage-trajectory-generation","text":"When one of the following conditions area realized, callback function to generate a trajectory is called and publish the trajectory. Otherwise, previously generated trajectory is published. Ego moves a certain distance compared to the previous ego pose (default: 10.0 [m]) Time passes (default: 1.0 [s]) The path shape is changed (e.g. when starting planning lane change) Ego is far from the previously generated trajectory The previously generated trajectory is memorized, but it is not when the path shape is changed and ego is far from the previously generated trajectory.","title":"Manage trajectory generation"},{"location":"planning/obstacle_avoidance_planner/#select-objects-to-avoid","text":"Only obstacles that are static and locate in a shoulder lane is decided to avoid. In detail, this equals to the following three conditions at the same time, and the red obstacles in the figure (id: 3, 4, 5) is to be avoided. Velocity is under a certain value (default: 0.1 [m/s]) CoG of the obstacles is not on the center line so that the ego will not avoid the car in front of the ego in the same lane. At least one point of the obstacle polygon is outside the drivable area.","title":"Select objects to avoid"},{"location":"planning/obstacle_avoidance_planner/#generate-object-costmap","text":"Cost map is generated according to the distance to the target obstacles to be avoided.","title":"Generate object costmap"},{"location":"planning/obstacle_avoidance_planner/#generate-road-boundary-costmap","text":"Cost map is generated according to the distance to the road boundaries. These cost maps area used in the optimization to generate a collision-free trajectory.","title":"Generate road boundary costmap"},{"location":"planning/obstacle_avoidance_planner/#smooth-path","text":"The latter optimization assumes that the reference path is smooth enough. Therefore the path from behavior is smoothed here, and send to the optimization as a format of trajectory. Obstacles are not considered.","title":"Smooth path"},{"location":"planning/obstacle_avoidance_planner/#optimize-trajectory","text":"This module makes the trajectory kinematically-feasible and collision-free. We define vehicle pose in the frenet coordinate, and minimize tracking errors by optimization. This optimization also considers vehicle kinematics and collision checking with road boundary and obstacles. To decrease the computation cost, the optimization is applied to the shorter trajectory than the whole trajectory, and concatenate the remained trajectory with the optimized one at last. The trajectory just in front of the ego must not be changed a lot so that the steering wheel will be stable. Therefore, we use the previously generated trajectory in front of the ego. Optimization center on the vehicle, that tries to locate just on the trajectory, can be tuned along side the vehicle vertical axis. This parameter optimization center offset is defined as the signed length from the back-wheel center to the optimization center. Some examples are shown in the following figure, and it is shown that the trajectory of vehicle shape differs according to the optimization center even if the reference trajectory (green one) is the same.","title":"Optimize trajectory"},{"location":"planning/obstacle_avoidance_planner/#check-drivability-and-extend-trajectory","text":"Optimized trajectory is too short for velocity planning, therefore extend the trajectory by concatenating the optimized trajectory and the behavior path considering drivability. Generated trajectory is checked if it is inside the drivable area or not, and if outside drivable area, output a trajectory inside drivable area with the behavior path or the previously generated trajectory. As described above, the behavior path is separated into two paths: one is for optimization and the other is the remain. The first path becomes optimized trajectory, and the second path just is transformed to a trajectory. Then a trajectory inside the drivable area is calculated as follows. If optimized trajectory is inside the drivable area , and the remained trajectory is inside/outside the drivable area, the output trajectory will be just concatenation of those two trajectories. In this case, we do not care if the remained trajectory is inside or outside the drivable area since generally it is outside the drivable area (especially in a narrow road), but we want to pass a trajectory as long as possible to the latter module. If optimized trajectory is outside the drivable area , and the remained trajectory is inside/outside the drivable area, and if the previously generated trajectory is memorized , the output trajectory will be a part of previously generated trajectory, whose end firstly goes outside the drivable area. and if the previously generated trajectory is not memorized , the output trajectory will be a part of trajectory just transformed from the behavior path, whose end firstly goes outside the drivable area. Optimization failure is dealt with the same as if the optimized trajectory is outside the drivable area. The output trajectory is memorized as a previously generated trajectory for the next cycle. Rationale In the current design, since there are some modelling errors, the constraints are considered to be soft constraints. Therefore, we have to make sure that the optimized trajectory is inside the drivable area or not after optimization.","title":"Check drivability, and extend trajectory"},{"location":"planning/obstacle_avoidance_planner/#assign-trajectory-velocity","text":"Velocity is assigned in the result trajectory by the behavior path. The shapes of the trajectory and the path are different, therefore the each nearest trajectory point to the path is searched and interpolated linearly.","title":"Assign trajectory velocity"},{"location":"planning/obstacle_avoidance_planner/#algorithms","text":"In this section, Elastic band (to smooth the path) and Model Predictive Trajectory (to optimize the trajectory) will be explained in detail.","title":"Algorithms"},{"location":"planning/obstacle_avoidance_planner/#elastic-band","text":"","title":"Elastic band"},{"location":"planning/obstacle_avoidance_planner/#abstract","text":"Elastic band smooths the path generated in the behavior. Since the latter process of optimization uses the curvature and normal vector of the reference path, smoothing should be applied here so that the optimization will be stable. This smoothing process does not consider collision. Therefore the output path may have a collision with road boundaries or obstacles.","title":"Abstract"},{"location":"planning/obstacle_avoidance_planner/#formulation","text":"We formulate a QP problem minimizing the distance between the previous point and the next point for each point. Conditions that each point can move to a certain extent are used so that the path will not changed a lot but will be smoother. For k k 'th point ( \\boldsymbol{p}_k \\boldsymbol{p}_k ), the objective function is as follows. The beginning and end point are fixed during the optimization. \\min \\sum_{k=1}^{n-1} |\\boldsymbol{p}_{k+1} - \\boldsymbol{p}_{k}| - |\\boldsymbol{p}_{k} - \\boldsymbol{p}_{k-1}| \\min \\sum_{k=1}^{n-1} |\\boldsymbol{p}_{k+1} - \\boldsymbol{p}_{k}| - |\\boldsymbol{p}_{k} - \\boldsymbol{p}_{k-1}|","title":"Formulation"},{"location":"planning/obstacle_avoidance_planner/#model-predictive-trajectory","text":"","title":"Model predictive trajectory"},{"location":"planning/obstacle_avoidance_planner/#abstract_1","text":"Model Predictive Trajectory (MPT) calculates the trajectory that realizes the following conditions. Kinematically feasible for linear vehicle kinematics model Collision free with obstacles and road boundaries Conditions for collision free is considered to be not hard constraints but soft constraints. When the optimization failed or the optimized trajectory is not collision free, the output trajectory will be previously generated trajectory. Trajectory near the ego must be stable, therefore the condition where trajectory points near the ego are the same as previously generated trajectory is considered, and this is the only hard constraints in MPT.","title":"Abstract"},{"location":"planning/obstacle_avoidance_planner/#formulation_1","text":"As the following figure, we consider the bicycle kinematics model in the frenet frame to track the reference path. At time step k k , we define lateral distance to the reference path, heading angle against the reference path, and steer angle as y_k y_k , \\theta_k \\theta_k , and \\delta_k \\delta_k respectively. Assuming that the commanded steer angle is \\delta_{des, k} \\delta_{des, k} , the kinematics model in the frenet frame is formulated as follows. We also assume that the steer angle \\delta_k \\delta_k is first-order lag to the commanded one. \\begin{align} y_{k+1} & = y_{k} + v \\sin \\theta_k dt \\\\ \\theta_{k+1} & = \\theta_k + \\frac{v \\tan \\delta_k}{L}dt - \\kappa_k v \\cos \\theta_k dt \\\\ \\delta_{k+1} & = \\delta_k - \\frac{\\delta_k - \\delta_{des,k}}{\\tau}dt \\end{align} \\begin{align} y_{k+1} & = y_{k} + v \\sin \\theta_k dt \\\\ \\theta_{k+1} & = \\theta_k + \\frac{v \\tan \\delta_k}{L}dt - \\kappa_k v \\cos \\theta_k dt \\\\ \\delta_{k+1} & = \\delta_k - \\frac{\\delta_k - \\delta_{des,k}}{\\tau}dt \\end{align} Then we linearize these equations. y_k y_k and \\theta_k \\theta_k are tracking errors, so we assume that those are small enough. Therefore \\sin \\theta_k \\approx \\theta_k \\sin \\theta_k \\approx \\theta_k . Since \\delta_k \\delta_k is a steer angle, it is not always small. By using a reference steer angle \\delta_{\\mathrm{ref}, k} \\delta_{\\mathrm{ref}, k} calculated by the reference path curvature \\kappa_k \\kappa_k , we express \\delta_k \\delta_k with a small value \\Delta \\delta_k \\Delta \\delta_k . Note that the steer angle \\delta_k \\delta_k is within the steer angle limitation \\delta_{\\max} \\delta_{\\max} . When the reference steer angle \\delta_{\\mathrm{ref}, k} \\delta_{\\mathrm{ref}, k} is larger than the steer angle limitation \\delta_{\\max} \\delta_{\\max} , and \\delta_{\\mathrm{ref}, k} \\delta_{\\mathrm{ref}, k} is used to linearize the steer angle, \\Delta \\delta_k \\Delta \\delta_k is \\Delta \\delta_k = \\delta - \\delta_{\\mathrm{ref}, k} = \\delta_{\\max} - \\delta_{\\mathrm{ref}, k} \\Delta \\delta_k = \\delta - \\delta_{\\mathrm{ref}, k} = \\delta_{\\max} - \\delta_{\\mathrm{ref}, k} , and the absolute \\Delta \\delta_k \\Delta \\delta_k gets larger. Therefore, we have to apply the steer angle limitation to \\delta_{\\mathrm{ref}, k} \\delta_{\\mathrm{ref}, k} as well. \\begin{align} \\delta_{\\mathrm{ref}, k} & = \\mathrm{clamp}(\\arctan(L \\kappa_k), -\\delta_{\\max}, \\delta_{\\max}) \\\\ \\delta_k & = \\delta_{\\mathrm{ref}, k} + \\Delta \\delta_k, \\ \\Delta \\delta_k \\ll 1 \\\\ \\end{align} \\begin{align} \\delta_{\\mathrm{ref}, k} & = \\mathrm{clamp}(\\arctan(L \\kappa_k), -\\delta_{\\max}, \\delta_{\\max}) \\\\ \\delta_k & = \\delta_{\\mathrm{ref}, k} + \\Delta \\delta_k, \\ \\Delta \\delta_k \\ll 1 \\\\ \\end{align} \\mathrm{clamp}(v, v_{\\min}, v_{\\max}) \\mathrm{clamp}(v, v_{\\min}, v_{\\max}) is a function to convert v v to be larger than v_{\\min} v_{\\min} and smaller than v_{\\max} v_{\\max} . Using this \\delta_{\\mathrm{ref}, k} \\delta_{\\mathrm{ref}, k} , \\tan \\delta_k \\tan \\delta_k is linearized as follows. \\begin{align} \\tan \\delta_k & \\approx \\tan \\delta_{\\mathrm{ref}, k} + \\left.\\frac{d \\tan \\delta}{d \\delta}\\right|_{\\delta = \\delta_{\\mathrm{ref}, k}} \\Delta \\delta_k \\\\ & = \\tan \\delta_{\\mathrm{ref}, k} + \\left.\\frac{d \\tan \\delta}{d \\delta}\\right|_{\\delta = \\delta_{\\mathrm{ref}, k}} (\\delta_{\\mathrm{ref}, k} - \\delta_k) \\\\ & = \\tan \\delta_{\\mathrm{ref}, k} - \\frac{\\delta_{\\mathrm{ref}, k}}{\\cos^2 \\delta_{\\mathrm{ref}, k}} + \\frac{1}{\\cos^2 \\delta_{\\mathrm{ref}, k}} \\delta_k \\end{align} \\begin{align} \\tan \\delta_k & \\approx \\tan \\delta_{\\mathrm{ref}, k} + \\left.\\frac{d \\tan \\delta}{d \\delta}\\right|_{\\delta = \\delta_{\\mathrm{ref}, k}} \\Delta \\delta_k \\\\ & = \\tan \\delta_{\\mathrm{ref}, k} + \\left.\\frac{d \\tan \\delta}{d \\delta}\\right|_{\\delta = \\delta_{\\mathrm{ref}, k}} (\\delta_{\\mathrm{ref}, k} - \\delta_k) \\\\ & = \\tan \\delta_{\\mathrm{ref}, k} - \\frac{\\delta_{\\mathrm{ref}, k}}{\\cos^2 \\delta_{\\mathrm{ref}, k}} + \\frac{1}{\\cos^2 \\delta_{\\mathrm{ref}, k}} \\delta_k \\end{align} Based on the linearization, the error kinematics is formulated with the following linear equations. \\begin{align} \\begin{pmatrix} y_{k+1} \\\\ \\theta_{k+1} \\\\ \\delta_{k+1} \\end{pmatrix} = \\begin{pmatrix} 1 & v dt & 0 \\\\ 0 & 1 & \\frac{v dt}{L \\cos^{2} \\delta_{\\mathrm{ref}, k}} \\\\ 0 & 0 & 1 - \\frac{dt}{\\tau} \\end{pmatrix} \\begin{pmatrix} y_k \\\\ \\theta_k \\\\ \\delta_k \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 0 \\\\ \\frac{dt}{\\tau} \\end{pmatrix} \\delta_{des} + \\begin{pmatrix} 0 \\\\ \\frac{v \\tan(\\delta_{\\mathrm{ref}, k}) dt}{L} - \\frac{v \\delta_{\\mathrm{ref}, k} dt}{L \\cos^{2} \\delta_{\\mathrm{ref}, k}} - \\kappa_k v dt\\\\ 0 \\end{pmatrix} \\end{align} \\begin{align} \\begin{pmatrix} y_{k+1} \\\\ \\theta_{k+1} \\\\ \\delta_{k+1} \\end{pmatrix} = \\begin{pmatrix} 1 & v dt & 0 \\\\ 0 & 1 & \\frac{v dt}{L \\cos^{2} \\delta_{\\mathrm{ref}, k}} \\\\ 0 & 0 & 1 - \\frac{dt}{\\tau} \\end{pmatrix} \\begin{pmatrix} y_k \\\\ \\theta_k \\\\ \\delta_k \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 0 \\\\ \\frac{dt}{\\tau} \\end{pmatrix} \\delta_{des} + \\begin{pmatrix} 0 \\\\ \\frac{v \\tan(\\delta_{\\mathrm{ref}, k}) dt}{L} - \\frac{v \\delta_{\\mathrm{ref}, k} dt}{L \\cos^{2} \\delta_{\\mathrm{ref}, k}} - \\kappa_k v dt\\\\ 0 \\end{pmatrix} \\end{align} The objective function for smoothing and tracking is shown as follows. \\begin{align} J_1 & (y_{0...N-1}, \\theta_{0...N-1}, \\delta_{0...N-1}) \\\\ & = w_y \\sum_{k} y_k^2 + w_{\\theta} \\sum_{k} \\theta_k^2 + w_{\\delta} \\sum_k \\delta_k^2 + w_{\\dot{\\delta}} \\sum_k \\dot{\\delta}_k^2 + w_{\\ddot{\\delta}} \\sum_k \\ddot{\\delta}_k^2 \\end{align} \\begin{align} J_1 & (y_{0...N-1}, \\theta_{0...N-1}, \\delta_{0...N-1}) \\\\ & = w_y \\sum_{k} y_k^2 + w_{\\theta} \\sum_{k} \\theta_k^2 + w_{\\delta} \\sum_k \\delta_k^2 + w_{\\dot{\\delta}} \\sum_k \\dot{\\delta}_k^2 + w_{\\ddot{\\delta}} \\sum_k \\ddot{\\delta}_k^2 \\end{align} As mentioned before, the constraints to be collision free with obstacles and road boundaries are formulated to be soft constraints. Assuming that the lateral distance to the road boundaries or obstacles from the back wheel center, front wheel center, and the point between them are y_{\\mathrm{base}, k}, y_{\\mathrm{top}, k}, y_{\\mathrm{mid}, k} y_{\\mathrm{base}, k}, y_{\\mathrm{top}, k}, y_{\\mathrm{mid}, k} respectively, and slack variables for each point are \\lambda_{\\mathrm{base}}, \\lambda_{\\mathrm{top}}, \\lambda_{\\mathrm{mid}} \\lambda_{\\mathrm{base}}, \\lambda_{\\mathrm{top}}, \\lambda_{\\mathrm{mid}} , the soft constraints can be formulated as follows. y_{\\mathrm{base}, k, \\min} - \\lambda_{\\mathrm{base}, k} \\leq y_{\\mathrm{base}, k} (y_k) \\leq y_{\\mathrm{base}, k, \\max} + \\lambda_{\\mathrm{base}, k}\\\\ y_{\\mathrm{top}, k, \\min} - \\lambda_{\\mathrm{top}, k} \\leq y_{\\mathrm{top}, k} (y_k) \\leq y_{\\mathrm{top}, k, \\max} + \\lambda_{\\mathrm{top}, k}\\\\ y_{\\mathrm{mid}, k, \\min} - \\lambda_{\\mathrm{mid}, k} \\leq y_{\\mathrm{mid}, k} (y_k) \\leq y_{\\mathrm{mid}, k, \\max} + \\lambda_{\\mathrm{mid}, k} y_{\\mathrm{base}, k, \\min} - \\lambda_{\\mathrm{base}, k} \\leq y_{\\mathrm{base}, k} (y_k) \\leq y_{\\mathrm{base}, k, \\max} + \\lambda_{\\mathrm{base}, k}\\\\ y_{\\mathrm{top}, k, \\min} - \\lambda_{\\mathrm{top}, k} \\leq y_{\\mathrm{top}, k} (y_k) \\leq y_{\\mathrm{top}, k, \\max} + \\lambda_{\\mathrm{top}, k}\\\\ y_{\\mathrm{mid}, k, \\min} - \\lambda_{\\mathrm{mid}, k} \\leq y_{\\mathrm{mid}, k} (y_k) \\leq y_{\\mathrm{mid}, k, \\max} + \\lambda_{\\mathrm{mid}, k} Since y_{\\mathrm{base}, k}, y_{\\mathrm{top}, k}, y_{\\mathrm{mid}, k} y_{\\mathrm{base}, k}, y_{\\mathrm{top}, k}, y_{\\mathrm{mid}, k} is formulated as a linear function of y_k y_k , the objective function for soft constraints is formulated as follows. \\begin{align} J_2 & (\\lambda_{\\mathrm{base}, 0...N-1}, \\lambda_{\\mathrm{mid}, 0...N-1}, \\lambda_{\\mathrm{top}, 0...N-1}) \\\\ & = w_{\\mathrm{base}} \\sum_{k} \\lambda_{\\mathrm{base}, k}^2 + w_{\\mathrm{mid}} \\sum_k \\lambda_{\\mathrm{mid}, k}^2 + w_{\\mathrm{top}} \\sum_k \\lambda_{\\mathrm{top}, k}^2 \\end{align} \\begin{align} J_2 & (\\lambda_{\\mathrm{base}, 0...N-1}, \\lambda_{\\mathrm{mid}, 0...N-1}, \\lambda_{\\mathrm{top}, 0...N-1}) \\\\ & = w_{\\mathrm{base}} \\sum_{k} \\lambda_{\\mathrm{base}, k}^2 + w_{\\mathrm{mid}} \\sum_k \\lambda_{\\mathrm{mid}, k}^2 + w_{\\mathrm{top}} \\sum_k \\lambda_{\\mathrm{top}, k}^2 \\end{align} Slack variables are also design variables for optimization. We define a vector \\boldsymbol{x} \\boldsymbol{x} , that concatenates all the design variables. \\begin{align} \\boldsymbol{x} = \\begin{pmatrix} ... & y_k & \\lambda_{\\mathrm{base}, k} & \\lambda_{\\mathrm{top}, k} & \\lambda_{\\mathrm{mid}, k} & ... \\end{pmatrix}^T \\end{align} \\begin{align} \\boldsymbol{x} = \\begin{pmatrix} ... & y_k & \\lambda_{\\mathrm{base}, k} & \\lambda_{\\mathrm{top}, k} & \\lambda_{\\mathrm{mid}, k} & ... \\end{pmatrix}^T \\end{align} The summation of these two objective functions is the objective function for the optimization problem. \\begin{align} \\min_{\\boldsymbol{x}} J (\\boldsymbol{x}) = \\min_{\\boldsymbol{x}} J_1 & (y_{0...N-1}, \\theta_{0...N-1}, \\delta_{0...N-1}) + J_2 (\\lambda_{\\mathrm{base}, 0...N-1}, \\lambda_{\\mathrm{mid}, 0...N-1}, \\lambda_{\\mathrm{top}, 0...N-1}) \\end{align} \\begin{align} \\min_{\\boldsymbol{x}} J (\\boldsymbol{x}) = \\min_{\\boldsymbol{x}} J_1 & (y_{0...N-1}, \\theta_{0...N-1}, \\delta_{0...N-1}) + J_2 (\\lambda_{\\mathrm{base}, 0...N-1}, \\lambda_{\\mathrm{mid}, 0...N-1}, \\lambda_{\\mathrm{top}, 0...N-1}) \\end{align} As mentioned before, we use hard constraints where some trajectory points in front of the ego are the same as the previously generated trajectory points. This hard constraints is formulated as follows. \\begin{align} \\delta_k = \\delta_{k}^{\\mathrm{prev}} (0 \\leq i \\leq N_{\\mathrm{fix}}) \\end{align} \\begin{align} \\delta_k = \\delta_{k}^{\\mathrm{prev}} (0 \\leq i \\leq N_{\\mathrm{fix}}) \\end{align} Finally we transform those objective functions to the following QP problem, and solve it. \\begin{align} \\min_{\\boldsymbol{x}} \\ & \\frac{1}{2} \\boldsymbol{x}^T \\boldsymbol{P} \\boldsymbol{x} + \\boldsymbol{q} \\boldsymbol{x} \\\\ \\mathrm{s.t.} \\ & \\boldsymbol{b}_l \\leq \\boldsymbol{A} \\boldsymbol{x} \\leq \\boldsymbol{b}_u \\end{align} \\begin{align} \\min_{\\boldsymbol{x}} \\ & \\frac{1}{2} \\boldsymbol{x}^T \\boldsymbol{P} \\boldsymbol{x} + \\boldsymbol{q} \\boldsymbol{x} \\\\ \\mathrm{s.t.} \\ & \\boldsymbol{b}_l \\leq \\boldsymbol{A} \\boldsymbol{x} \\leq \\boldsymbol{b}_u \\end{align}","title":"Formulation"},{"location":"planning/obstacle_avoidance_planner/#limitation","text":"When turning right or left in the intersection, the output trajectory is close to the outside road boundary. Roles of planning for behavior_path_planner and obstacle_avoidance_planner are not decided clearly. High computation cost","title":"Limitation"},{"location":"planning/obstacle_avoidance_planner/#comparison-to-other-methods","text":"Planning a trajectory that satisfies kinematic feasibility and collision-free has two main characteristics that makes hard to be solved: one is non-convex and the other is high dimension. According to the characteristics, we investigate pros and cons of the typical planning methods: optimization-based, sampling-based, and learning-based method.","title":"Comparison to other methods"},{"location":"planning/obstacle_avoidance_planner/#optimization-based-method","text":"pros: comparatively fast against high dimension by leveraging the gradient descent cons: often converge to the local minima in the non-convex problem","title":"Optimization-based method"},{"location":"planning/obstacle_avoidance_planner/#sampling-based-method","text":"pros: realize global optimization cons: high computation cost especially in the complex case","title":"Sampling-based method"},{"location":"planning/obstacle_avoidance_planner/#learning-based-method","text":"under research yet Based on these pros/cons, we chose the optimization-based planner first. Although it has a cons to converge to the local minima, it can get a good solution by the preprocessing to approximate the convex problem that almost equals to the original non-convex problem.","title":"Learning-based method"},{"location":"planning/obstacle_avoidance_planner/#how-to-debug","text":"Topics for debugging will be explained in this section. interpolated_points_marker Trajectory points that is resampled from the input trajectory of obstacle avoidance planner. Whether this trajectory is inside the drivable area, smooth enough can be checked. smoothed_points_text The output trajectory points from Elastic Band. Not points but small numbers are visualized. Whether these smoothed points are distorted or not can be checked. (base/top/mid)_bounds_line Lateral Distance to the road boundaries to check collision in MPT (More precisely, - vehicle_width / 2.0). This collision check is done with three points on the vehicle (base = back wheel center, top, mid), therefore three line markers are visualized for each trajectory point. If the distance between the edge of line markers and the road boundaries is not about the half width of the vehicle, collision check will fail. optimized_points_marker The output trajectory points from MPT. Whether the trajectory is outside the drivable area can be checked. Trajectory with footprint Trajectory footprints can be visualized by TrajectoryFootprint of rviz_plugin. Whether trajectory footprints of input/output of obstacle_avoidance_planner is inside the drivable area or not can be checked. Drivable Area Drivable area. When drivable area generation failed, the drawn area may be distorted. nav_msgs/msg/OccupancyGrid topic name: /planning/scenario_planning/lane_driving/behavior_planning/behavior_path_planner/debug/drivable_area area_with_objects Area where obstacles are removed from the drivable area nav_msgs/msg/OccupancyGrid object_clearance_map Cost map for obstacles (distance to the target obstacles to be avoided) nav_msgs/msg/OccupancyGrid clearance_map Cost map for drivable area (distance to the road boundaries) nav_msgs/msg/OccupancyGrid","title":"How to debug"},{"location":"planning/route_handler/","text":"route handler # route_handler is a library for calculating driving route on the lanelet map.","title":"route handler"},{"location":"planning/route_handler/#route-handler","text":"route_handler is a library for calculating driving route on the lanelet map.","title":"route handler"},{"location":"planning/surround_obstacle_checker/","text":"Surround Obstacle Checker # Purpose # surround_obstacle_checker is a module to prevent moving if any obstacles is near stopping ego vehicle. This module runs only when ego vehicle is stopping. Inner-workings / Algorithms # Flow chart # Algorithms # Check data # Check that surround_obstacle_checker receives no ground pointcloud, dynamic objects and current velocity data. Get distance to nearest object # Calculate distance between ego vehicle and the nearest object. In this function, it calculates the minimum distance between the polygon of ego vehicle and all points in pointclouds and the polygons of dynamic objects. Stop requirement # If it satisfies all following conditions, it plans stopping. Ego vehicle is stopped It satisfies any following conditions The distance to nearest obstacle satisfies following conditions If state is State::PASS , the distance is less than surround_check_distance If state is State::STOP , the distance is less than surround_check_recover_distance If it does not satisfies the condition in 1, elapsed time from the time it satisfies the condition in 1 is less than state_clear_time States # To prevent chattering, surround_obstacle_checker manages two states. As mentioned in stop condition section, it prevents chattering by changing threshold to find surround obstacle depending on the states. State::PASS : Stop planning is released State::STOP \uff1aWhile stop planning Inputs / Outputs # Input # Name Type Description ~/input/trajectory autoware_auto_planning_msgs::msg::Trajectory Reference trajectory /perception/obstacle_segmentation/pointcloud sensor_msgs::msg::PointCloud2 Pointcloud of obstacles which the ego-vehicle should stop or avoid /perception/object_recognition/objects autoware_auto_perception_msgs::msg::PredictedObjects Dynamic objects /localization/kinematic_state nav_msgs::msg::Odometry Current twist /tf tf2_msgs::msg::TFMessage TF /tf_static tf2_msgs::msg::TFMessage TF static Output # Name Type Description ~/output/trajectory autoware_auto_planning_msgs/Trajectory Modified trajectory ~/output/no_start_reason diagnostic_msgs::msg::DiagnosticStatus No start reason ~/output/stop_reasons autoware_planning_msgs::msg::StopReasonArray Stop reasons ~/debug/marker visualization_msgs::msg::MarkerArray Marker for visualization Parameters # Name Type Description Default value use_pointcloud bool Use pointcloud as obstacle check true use_dynamic_object bool Use dynamic object as obstacle check true surround_check_distance double If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status [m] 0.5 surround_check_recover_distance double If no object exists in this distance, transit to \"non-surrounding-obstacle\" status [m] 0.8 state_clear_time double Threshold to clear stop state [s] 2.0 stop_state_ego_speed double Threshold to check ego vehicle stopped [m/s] 0.1 Assumptions / Known limits # To perform stop planning, it is necessary to get obstacle pointclouds data. Hence, it does not plan stopping if the obstacle is in blind spot.","title":"Surround Obstacle Checker"},{"location":"planning/surround_obstacle_checker/#surround-obstacle-checker","text":"","title":"Surround Obstacle Checker"},{"location":"planning/surround_obstacle_checker/#purpose","text":"surround_obstacle_checker is a module to prevent moving if any obstacles is near stopping ego vehicle. This module runs only when ego vehicle is stopping.","title":"Purpose"},{"location":"planning/surround_obstacle_checker/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"planning/surround_obstacle_checker/#flow-chart","text":"","title":"Flow chart"},{"location":"planning/surround_obstacle_checker/#algorithms","text":"","title":"Algorithms"},{"location":"planning/surround_obstacle_checker/#check-data","text":"Check that surround_obstacle_checker receives no ground pointcloud, dynamic objects and current velocity data.","title":"Check data"},{"location":"planning/surround_obstacle_checker/#get-distance-to-nearest-object","text":"Calculate distance between ego vehicle and the nearest object. In this function, it calculates the minimum distance between the polygon of ego vehicle and all points in pointclouds and the polygons of dynamic objects.","title":"Get distance to nearest object"},{"location":"planning/surround_obstacle_checker/#stop-requirement","text":"If it satisfies all following conditions, it plans stopping. Ego vehicle is stopped It satisfies any following conditions The distance to nearest obstacle satisfies following conditions If state is State::PASS , the distance is less than surround_check_distance If state is State::STOP , the distance is less than surround_check_recover_distance If it does not satisfies the condition in 1, elapsed time from the time it satisfies the condition in 1 is less than state_clear_time","title":"Stop requirement"},{"location":"planning/surround_obstacle_checker/#states","text":"To prevent chattering, surround_obstacle_checker manages two states. As mentioned in stop condition section, it prevents chattering by changing threshold to find surround obstacle depending on the states. State::PASS : Stop planning is released State::STOP \uff1aWhile stop planning","title":"States"},{"location":"planning/surround_obstacle_checker/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"planning/surround_obstacle_checker/#input","text":"Name Type Description ~/input/trajectory autoware_auto_planning_msgs::msg::Trajectory Reference trajectory /perception/obstacle_segmentation/pointcloud sensor_msgs::msg::PointCloud2 Pointcloud of obstacles which the ego-vehicle should stop or avoid /perception/object_recognition/objects autoware_auto_perception_msgs::msg::PredictedObjects Dynamic objects /localization/kinematic_state nav_msgs::msg::Odometry Current twist /tf tf2_msgs::msg::TFMessage TF /tf_static tf2_msgs::msg::TFMessage TF static","title":"Input"},{"location":"planning/surround_obstacle_checker/#output","text":"Name Type Description ~/output/trajectory autoware_auto_planning_msgs/Trajectory Modified trajectory ~/output/no_start_reason diagnostic_msgs::msg::DiagnosticStatus No start reason ~/output/stop_reasons autoware_planning_msgs::msg::StopReasonArray Stop reasons ~/debug/marker visualization_msgs::msg::MarkerArray Marker for visualization","title":"Output"},{"location":"planning/surround_obstacle_checker/#parameters","text":"Name Type Description Default value use_pointcloud bool Use pointcloud as obstacle check true use_dynamic_object bool Use dynamic object as obstacle check true surround_check_distance double If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status [m] 0.5 surround_check_recover_distance double If no object exists in this distance, transit to \"non-surrounding-obstacle\" status [m] 0.8 state_clear_time double Threshold to clear stop state [s] 2.0 stop_state_ego_speed double Threshold to check ego vehicle stopped [m/s] 0.1","title":"Parameters"},{"location":"planning/surround_obstacle_checker/#assumptions-known-limits","text":"To perform stop planning, it is necessary to get obstacle pointclouds data. Hence, it does not plan stopping if the obstacle is in blind spot.","title":"Assumptions / Known limits"},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/","text":"Surround Obstacle Checker # Purpose # surround_obstacle_checker \u306f\u3001\u81ea\u8eca\u304c\u505c\u8eca\u4e2d\u3001\u81ea\u8eca\u306e\u5468\u56f2\u306b\u969c\u5bb3\u7269\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u306b\u767a\u9032\u3057\u306a\u3044\u3088\u3046\u306b\u505c\u6b62\u8a08\u753b\u3092\u884c\u3046\u30e2\u30b8\u30e5\u30fc\u30eb\u3067\u3042\u308b\u3002 Inner-workings / Algorithms # Flow chart # Algorithms # Check data # \u70b9\u7fa4\u3001\u52d5\u7684\u7269\u4f53\u3001\u81ea\u8eca\u901f\u5ea6\u306e\u30c7\u30fc\u30bf\u304c\u53d6\u5f97\u3067\u304d\u3066\u3044\u308b\u304b\u3069\u3046\u304b\u3092\u78ba\u8a8d\u3059\u308b\u3002 Get distance to nearest object # \u81ea\u8eca\u3068\u6700\u8fd1\u508d\u306e\u969c\u5bb3\u7269\u3068\u306e\u8ddd\u96e2\u3092\u8a08\u7b97\u3059\u308b\u3002 \u3053\u3053\u3067\u306f\u3001\u81ea\u8eca\u306e\u30dd\u30ea\u30b4\u30f3\u3092\u8a08\u7b97\u3057\u3001\u70b9\u7fa4\u306e\u5404\u70b9\u304a\u3088\u3073\u5404\u52d5\u7684\u7269\u4f53\u306e\u30dd\u30ea\u30b4\u30f3\u3068\u306e\u8ddd\u96e2\u3092\u305d\u308c\u305e\u308c\u8a08\u7b97\u3059\u308b\u3053\u3068\u3067\u6700\u8fd1\u508d\u306e\u969c\u5bb3\u7269\u3068\u306e\u8ddd\u96e2\u3092\u6c42\u3081\u308b\u3002 Stop condition # \u6b21\u306e\u6761\u4ef6\u3092\u3059\u3079\u3066\u6e80\u305f\u3059\u3068\u304d\u3001\u81ea\u8eca\u306f\u505c\u6b62\u8a08\u753b\u3092\u884c\u3046\u3002 \u81ea\u8eca\u304c\u505c\u8eca\u3057\u3066\u3044\u308b\u3053\u3068 \u6b21\u306e\u3046\u3061\u3044\u305a\u308c\u304b\u3092\u6e80\u305f\u3059\u3053\u3068 \u6700\u8fd1\u508d\u306e\u969c\u5bb3\u7269\u3068\u306e\u8ddd\u96e2\u304c\u6b21\u306e\u6761\u4ef6\u3092\u307f\u305f\u3059\u3053\u3068 State::PASS \u306e\u3068\u304d\u3001 surround_check_distance \u672a\u6e80\u3067\u3042\u308b State::STOP \u306e\u3068\u304d\u3001 surround_check_recover_distance \u4ee5\u4e0b\u3067\u3042\u308b 1 \u3092\u6e80\u305f\u3057\u3066\u3044\u306a\u3044\u3068\u304d\u30011 \u306e\u6761\u4ef6\u3092\u6e80\u305f\u3057\u305f\u6642\u523b\u304b\u3089\u306e\u7d4c\u904e\u6642\u9593\u304c state_clear_time \u4ee5\u4e0b\u3067\u3042\u308b\u3053\u3068 States # \u30c1\u30e3\u30bf\u30ea\u30f3\u30b0\u9632\u6b62\u306e\u305f\u3081\u3001 surround_obstacle_checker \u3067\u306f\u72b6\u614b\u3092\u7ba1\u7406\u3057\u3066\u3044\u308b\u3002 Stop condition \u306e\u9805\u3067\u8ff0\u3079\u305f\u3088\u3046\u306b\u3001\u72b6\u614b\u306b\u3088\u3063\u3066\u969c\u5bb3\u7269\u5224\u5b9a\u306e\u3057\u304d\u3044\u5024\u3092\u5909\u66f4\u3059\u308b\u3053\u3068\u3067\u30c1\u30e3\u30bf\u30ea\u30f3\u30b0\u3092\u9632\u6b62\u3057\u3066\u3044\u308b\u3002 State::PASS \uff1a\u505c\u6b62\u8a08\u753b\u89e3\u9664\u4e2d State::STOP \uff1a\u505c\u6b62\u8a08\u753b\u4e2d Inputs / Outputs # Input # Name Type Description ~/input/trajectory autoware_auto_planning_msgs::msg::Trajectory Reference trajectory /perception/obstacle_segmentation/pointcloud sensor_msgs::msg::PointCloud2 Pointcloud of obstacles which the ego-vehicle should stop or avoid /perception/object_recognition/objects autoware_auto_perception_msgs::msg::PredictedObjects Dynamic objects /localization/kinematic_state nav_msgs::msg::Odometry Current twist /tf tf2_msgs::msg::TFMessage TF /tf_static tf2_msgs::msg::TFMessage TF static Output # Name Type Description ~/output/trajectory autoware_auto_planning_msgs/Trajectory Modified trajectory ~/output/no_start_reason diagnostic_msgs::msg::DiagnosticStatus No start reason ~/output/stop_reasons autoware_planning_msgs::msg::StopReasonArray Stop reasons ~/debug/marker visualization_msgs::msg::MarkerArray Marker for visualization Parameters # Name Type Description Default value use_pointcloud bool Use pointcloud as obstacle check true use_dynamic_object bool Use dynamic object as obstacle check true surround_check_distance double If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status [m] 0.5 surround_check_recover_distance double If no object exists in this distance, transit to \"non-surrounding-obstacle\" status [m] 0.8 state_clear_time double Threshold to clear stop state [s] 2.0 stop_state_ego_speed double Threshold to check ego vehicle stopped [m/s] 0.1 Assumptions / Known limits # \u3053\u306e\u6a5f\u80fd\u304c\u52d5\u4f5c\u3059\u308b\u305f\u3081\u306b\u306f\u969c\u5bb3\u7269\u70b9\u7fa4\u306e\u89b3\u6e2c\u304c\u5fc5\u8981\u306a\u305f\u3081\u3001\u969c\u5bb3\u7269\u304c\u6b7b\u89d2\u306b\u5165\u3063\u3066\u3044\u308b\u5834\u5408\u306f\u505c\u6b62\u8a08\u753b\u3092\u884c\u308f\u306a\u3044\u3002","title":"Surround Obstacle Checker"},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/#surround-obstacle-checker","text":"","title":"Surround Obstacle Checker"},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/#purpose","text":"surround_obstacle_checker \u306f\u3001\u81ea\u8eca\u304c\u505c\u8eca\u4e2d\u3001\u81ea\u8eca\u306e\u5468\u56f2\u306b\u969c\u5bb3\u7269\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u306b\u767a\u9032\u3057\u306a\u3044\u3088\u3046\u306b\u505c\u6b62\u8a08\u753b\u3092\u884c\u3046\u30e2\u30b8\u30e5\u30fc\u30eb\u3067\u3042\u308b\u3002","title":"Purpose"},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/#flow-chart","text":"","title":"Flow chart"},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/#algorithms","text":"","title":"Algorithms"},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/#check-data","text":"\u70b9\u7fa4\u3001\u52d5\u7684\u7269\u4f53\u3001\u81ea\u8eca\u901f\u5ea6\u306e\u30c7\u30fc\u30bf\u304c\u53d6\u5f97\u3067\u304d\u3066\u3044\u308b\u304b\u3069\u3046\u304b\u3092\u78ba\u8a8d\u3059\u308b\u3002","title":"Check data"},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/#get-distance-to-nearest-object","text":"\u81ea\u8eca\u3068\u6700\u8fd1\u508d\u306e\u969c\u5bb3\u7269\u3068\u306e\u8ddd\u96e2\u3092\u8a08\u7b97\u3059\u308b\u3002 \u3053\u3053\u3067\u306f\u3001\u81ea\u8eca\u306e\u30dd\u30ea\u30b4\u30f3\u3092\u8a08\u7b97\u3057\u3001\u70b9\u7fa4\u306e\u5404\u70b9\u304a\u3088\u3073\u5404\u52d5\u7684\u7269\u4f53\u306e\u30dd\u30ea\u30b4\u30f3\u3068\u306e\u8ddd\u96e2\u3092\u305d\u308c\u305e\u308c\u8a08\u7b97\u3059\u308b\u3053\u3068\u3067\u6700\u8fd1\u508d\u306e\u969c\u5bb3\u7269\u3068\u306e\u8ddd\u96e2\u3092\u6c42\u3081\u308b\u3002","title":"Get distance to nearest object"},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/#stop-condition","text":"\u6b21\u306e\u6761\u4ef6\u3092\u3059\u3079\u3066\u6e80\u305f\u3059\u3068\u304d\u3001\u81ea\u8eca\u306f\u505c\u6b62\u8a08\u753b\u3092\u884c\u3046\u3002 \u81ea\u8eca\u304c\u505c\u8eca\u3057\u3066\u3044\u308b\u3053\u3068 \u6b21\u306e\u3046\u3061\u3044\u305a\u308c\u304b\u3092\u6e80\u305f\u3059\u3053\u3068 \u6700\u8fd1\u508d\u306e\u969c\u5bb3\u7269\u3068\u306e\u8ddd\u96e2\u304c\u6b21\u306e\u6761\u4ef6\u3092\u307f\u305f\u3059\u3053\u3068 State::PASS \u306e\u3068\u304d\u3001 surround_check_distance \u672a\u6e80\u3067\u3042\u308b State::STOP \u306e\u3068\u304d\u3001 surround_check_recover_distance \u4ee5\u4e0b\u3067\u3042\u308b 1 \u3092\u6e80\u305f\u3057\u3066\u3044\u306a\u3044\u3068\u304d\u30011 \u306e\u6761\u4ef6\u3092\u6e80\u305f\u3057\u305f\u6642\u523b\u304b\u3089\u306e\u7d4c\u904e\u6642\u9593\u304c state_clear_time \u4ee5\u4e0b\u3067\u3042\u308b\u3053\u3068","title":"Stop condition"},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/#states","text":"\u30c1\u30e3\u30bf\u30ea\u30f3\u30b0\u9632\u6b62\u306e\u305f\u3081\u3001 surround_obstacle_checker \u3067\u306f\u72b6\u614b\u3092\u7ba1\u7406\u3057\u3066\u3044\u308b\u3002 Stop condition \u306e\u9805\u3067\u8ff0\u3079\u305f\u3088\u3046\u306b\u3001\u72b6\u614b\u306b\u3088\u3063\u3066\u969c\u5bb3\u7269\u5224\u5b9a\u306e\u3057\u304d\u3044\u5024\u3092\u5909\u66f4\u3059\u308b\u3053\u3068\u3067\u30c1\u30e3\u30bf\u30ea\u30f3\u30b0\u3092\u9632\u6b62\u3057\u3066\u3044\u308b\u3002 State::PASS \uff1a\u505c\u6b62\u8a08\u753b\u89e3\u9664\u4e2d State::STOP \uff1a\u505c\u6b62\u8a08\u753b\u4e2d","title":"States"},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/#input","text":"Name Type Description ~/input/trajectory autoware_auto_planning_msgs::msg::Trajectory Reference trajectory /perception/obstacle_segmentation/pointcloud sensor_msgs::msg::PointCloud2 Pointcloud of obstacles which the ego-vehicle should stop or avoid /perception/object_recognition/objects autoware_auto_perception_msgs::msg::PredictedObjects Dynamic objects /localization/kinematic_state nav_msgs::msg::Odometry Current twist /tf tf2_msgs::msg::TFMessage TF /tf_static tf2_msgs::msg::TFMessage TF static","title":"Input"},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/#output","text":"Name Type Description ~/output/trajectory autoware_auto_planning_msgs/Trajectory Modified trajectory ~/output/no_start_reason diagnostic_msgs::msg::DiagnosticStatus No start reason ~/output/stop_reasons autoware_planning_msgs::msg::StopReasonArray Stop reasons ~/debug/marker visualization_msgs::msg::MarkerArray Marker for visualization","title":"Output"},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/#parameters","text":"Name Type Description Default value use_pointcloud bool Use pointcloud as obstacle check true use_dynamic_object bool Use dynamic object as obstacle check true surround_check_distance double If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status [m] 0.5 surround_check_recover_distance double If no object exists in this distance, transit to \"non-surrounding-obstacle\" status [m] 0.8 state_clear_time double Threshold to clear stop state [s] 2.0 stop_state_ego_speed double Threshold to check ego vehicle stopped [m/s] 0.1","title":"Parameters"},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/#assumptions-known-limits","text":"\u3053\u306e\u6a5f\u80fd\u304c\u52d5\u4f5c\u3059\u308b\u305f\u3081\u306b\u306f\u969c\u5bb3\u7269\u70b9\u7fa4\u306e\u89b3\u6e2c\u304c\u5fc5\u8981\u306a\u305f\u3081\u3001\u969c\u5bb3\u7269\u304c\u6b7b\u89d2\u306b\u5165\u3063\u3066\u3044\u308b\u5834\u5408\u306f\u505c\u6b62\u8a08\u753b\u3092\u884c\u308f\u306a\u3044\u3002","title":"Assumptions / Known limits"},{"location":"sensing/geo_pos_conv/","text":"geo_pos_conv # Purpose # The geo_pos_conv is a library to calculate the conversion between x, y positions on the plane rectangular coordinate and latitude/longitude on the earth . Inner-workings / Algorithms # Inputs / Outputs # Parameters # Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # (Optional) Future extensions / Unimplemented parts #","title":"geo_pos_conv"},{"location":"sensing/geo_pos_conv/#geo_pos_conv","text":"","title":"geo_pos_conv"},{"location":"sensing/geo_pos_conv/#purpose","text":"The geo_pos_conv is a library to calculate the conversion between x, y positions on the plane rectangular coordinate and latitude/longitude on the earth .","title":"Purpose"},{"location":"sensing/geo_pos_conv/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"sensing/geo_pos_conv/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"sensing/geo_pos_conv/#parameters","text":"","title":"Parameters"},{"location":"sensing/geo_pos_conv/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"sensing/geo_pos_conv/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"sensing/geo_pos_conv/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"sensing/geo_pos_conv/#optional-referencesexternal-links","text":"","title":"(Optional) References/External links"},{"location":"sensing/geo_pos_conv/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"sensing/gnss_poser/","text":"gnss_poser # Purpose # The gnss_poser is a node that subscribes gnss sensing messages and calculates vehicle pose with covariance. Inner-workings / Algorithms # Inputs / Outputs # Input # Name Type Description ~/input/fix sensor_msgs::msg::NavSatFix gnss status message ~/input/navpvt ublox_msgs::msg::NavPVT position, velocity and time solution (You can see detail description in reference document [1]) Output # Name Type Description ~/output/pose geometry_msgs::msg::PoseStamped vehicle pose calculated from gnss sensing data ~/output/gnss_pose_cov geometry_msgs::msg::PoseWithCovarianceStamped vehicle pose with covariance calculated from gnss sensing data ~/output/gnss_fixed autoware_debug_msgs::msg::BoolStamped gnss fix status Parameters # Core Parameters # Name Type Default Value Description base_frame string \"base_link\" frame d gnss_frame string \"gnss\" frame id gnss_base_frame string \"gnss_base_link\" frame id map_frame string \"map\" frame id use_ublox_receiver bool false flag to use ublox receiver plane_zone int 9 identification number of the plane rectangular coordinate systems (See, reference document [2]) Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # [1] https://github.com/KumarRobotics/ublox.git [2] https://www.gsi.go.jp/LAW/heimencho.html (Optional) Future extensions / Unimplemented parts #","title":"gnss_poser"},{"location":"sensing/gnss_poser/#gnss_poser","text":"","title":"gnss_poser"},{"location":"sensing/gnss_poser/#purpose","text":"The gnss_poser is a node that subscribes gnss sensing messages and calculates vehicle pose with covariance.","title":"Purpose"},{"location":"sensing/gnss_poser/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"sensing/gnss_poser/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"sensing/gnss_poser/#input","text":"Name Type Description ~/input/fix sensor_msgs::msg::NavSatFix gnss status message ~/input/navpvt ublox_msgs::msg::NavPVT position, velocity and time solution (You can see detail description in reference document [1])","title":"Input"},{"location":"sensing/gnss_poser/#output","text":"Name Type Description ~/output/pose geometry_msgs::msg::PoseStamped vehicle pose calculated from gnss sensing data ~/output/gnss_pose_cov geometry_msgs::msg::PoseWithCovarianceStamped vehicle pose with covariance calculated from gnss sensing data ~/output/gnss_fixed autoware_debug_msgs::msg::BoolStamped gnss fix status","title":"Output"},{"location":"sensing/gnss_poser/#parameters","text":"","title":"Parameters"},{"location":"sensing/gnss_poser/#core-parameters","text":"Name Type Default Value Description base_frame string \"base_link\" frame d gnss_frame string \"gnss\" frame id gnss_base_frame string \"gnss_base_link\" frame id map_frame string \"map\" frame id use_ublox_receiver bool false flag to use ublox receiver plane_zone int 9 identification number of the plane rectangular coordinate systems (See, reference document [2])","title":"Core Parameters"},{"location":"sensing/gnss_poser/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"sensing/gnss_poser/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"sensing/gnss_poser/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"sensing/gnss_poser/#optional-referencesexternal-links","text":"[1] https://github.com/KumarRobotics/ublox.git [2] https://www.gsi.go.jp/LAW/heimencho.html","title":"(Optional) References/External links"},{"location":"sensing/gnss_poser/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"sensing/image_transport_decompressor/","text":"image_transport_decompressor # Purpose # The image_transport_decompressor is a node that decompresses images. Inner-workings / Algorithms # Inputs / Outputs # Input # Name Type Description ~/input/compressed_image sensor_msgs::msg::CompressedImage compressed image Output # Name Type Description ~/output/raw_image sensor_msgs::msg::Image decompressed image Parameters # Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # (Optional) Future extensions / Unimplemented parts #","title":"image_transport_decompressor"},{"location":"sensing/image_transport_decompressor/#image_transport_decompressor","text":"","title":"image_transport_decompressor"},{"location":"sensing/image_transport_decompressor/#purpose","text":"The image_transport_decompressor is a node that decompresses images.","title":"Purpose"},{"location":"sensing/image_transport_decompressor/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"sensing/image_transport_decompressor/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"sensing/image_transport_decompressor/#input","text":"Name Type Description ~/input/compressed_image sensor_msgs::msg::CompressedImage compressed image","title":"Input"},{"location":"sensing/image_transport_decompressor/#output","text":"Name Type Description ~/output/raw_image sensor_msgs::msg::Image decompressed image","title":"Output"},{"location":"sensing/image_transport_decompressor/#parameters","text":"","title":"Parameters"},{"location":"sensing/image_transport_decompressor/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"sensing/image_transport_decompressor/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"sensing/image_transport_decompressor/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"sensing/image_transport_decompressor/#optional-referencesexternal-links","text":"","title":"(Optional) References/External links"},{"location":"sensing/image_transport_decompressor/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"sensing/imu_corrector/","text":"imu_corrector # Purpose # imu_corrector_node is a node that correct imu data. Correct yaw rate offset by reading the parameter. Correct yaw rate standard deviation by reading the parameter. Use the value estimated by deviation_estimator as the parameters for this node. Inputs / Outputs # Input # Name Type Description ~input sensor_msgs::msg::Imu raw imu data Output # Name Type Description ~output sensor_msgs::msg::Imu corrected imu data Parameters # Core Parameters # Name Type Description angular_velocity_offset_z double yaw rate offset [rad/s] angular_velocity_stddev_zz double yaw rate standard deviation [rad/s] Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # (Optional) Future extensions / Unimplemented parts #","title":"imu_corrector"},{"location":"sensing/imu_corrector/#imu_corrector","text":"","title":"imu_corrector"},{"location":"sensing/imu_corrector/#purpose","text":"imu_corrector_node is a node that correct imu data. Correct yaw rate offset by reading the parameter. Correct yaw rate standard deviation by reading the parameter. Use the value estimated by deviation_estimator as the parameters for this node.","title":"Purpose"},{"location":"sensing/imu_corrector/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"sensing/imu_corrector/#input","text":"Name Type Description ~input sensor_msgs::msg::Imu raw imu data","title":"Input"},{"location":"sensing/imu_corrector/#output","text":"Name Type Description ~output sensor_msgs::msg::Imu corrected imu data","title":"Output"},{"location":"sensing/imu_corrector/#parameters","text":"","title":"Parameters"},{"location":"sensing/imu_corrector/#core-parameters","text":"Name Type Description angular_velocity_offset_z double yaw rate offset [rad/s] angular_velocity_stddev_zz double yaw rate standard deviation [rad/s]","title":"Core Parameters"},{"location":"sensing/imu_corrector/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"sensing/imu_corrector/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"sensing/imu_corrector/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"sensing/imu_corrector/#optional-referencesexternal-links","text":"","title":"(Optional) References/External links"},{"location":"sensing/imu_corrector/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"sensing/laserscan_to_occupancy_grid_map/","text":"laserscan_to_occupancy_grid_map # Purpose # This package outputs the probability of having an obstacle as occupancy grid map. Inner-workings / Algorithms # The basic idea is to take a 2D laserscan and ray trace it to create a time-series processed occupancy grid map. the node take a laserscan and make an occupancy grid map with one frame. ray trace is done by Bresenham's line algorithm. Optionally, obstacle point clouds and raw point clouds can be received and reflected in the occupancy grid map. The reason is that laserscan only uses the most foreground point in the polar coordinate system, so it throws away a lot of information. As a result, the occupancy grid map is almost an UNKNOWN cell. Therefore, the obstacle point cloud and the raw point cloud are used to reflect what is judged to be the ground and what is judged to be an obstacle in the occupancy grid map. The black and red dots represent raw point clouds, and the red dots represent obstacle point clouds. In other words, the black points are determined as the ground, and the red point cloud is the points determined as obstacles. The gray cells are represented as UNKNOWN cells. Using the previous occupancy grid map, update the existence probability using a binary Bayesian filter (1). Also, the unobserved cells are time-decayed like the system noise of the Kalman filter (2). \\hat{P_{o}} = \\frac{(P_{o} * P_{z})}{(P_{o} * P_{z} + (1 - P_{o}) * \\bar{P_{z}})} \\tag{1} \\hat{P_{o}} = \\frac{(P_{o} + 0.5 * \\frac{1}{ratio})}{(\\frac{1}{ratio} + 1)} \\tag{2} Inputs / Outputs # Input # Name Type Description ~/input/laserscan sensor_msgs::LaserScan laserscan ~/input/obstacle_pointcloud sensor_msgs::PointCloud2 obstacle pointcloud ~/input/raw_pointcloud sensor_msgs::PointCloud2 The overall point cloud used to input the obstacle point cloud Output # Name Type Description ~/output/occupancy_grid_map nav_msgs::OccupancyGrid occupancy grid map Parameters # Node Parameters # Name Type Description map_frame string map frame base_link_frame string base_link frame input_obstacle_pointcloud bool whether to use the optional obstacle point cloud? If this is true, ~/input/obstacle_pointcloud topics will be received. input_obstacle_and_raw_pointcloud bool whether to use the optional obstacle and raw point cloud? If this is true, ~/input/obstacle_pointcloud and ~/input/raw_pointcloud topics will be received. use_height_filter bool whether to height filter for ~/input/obstacle_pointcloud and ~/input/raw_pointcloud ? By default, the height is set to -1~2m. map_length double The length of the map. -100 if it is 50~50[m] map_resolution double The map cell resolution [m] Assumptions / Known limits # In several places we have modified the external code written in BSD3 license. occupancy_grid_map.hpp cost_value.hpp occupancy_grid_map.cpp (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # Bresenham's_line_algorithm https://en.wikipedia.org/wiki/Bresenham%27s_line_algorithm https://web.archive.org/web/20080528040104/http://www.research.ibm.com/journal/sj/041/ibmsjIVRIC.pdf (Optional) Future extensions / Unimplemented parts # The update probability of the binary Bayesian filter is currently hard-coded and requires a code change to be modified. Since there is no special support for moving objects, the probability of existence is not increased for fast objects.","title":"laserscan_to_occupancy_grid_map"},{"location":"sensing/laserscan_to_occupancy_grid_map/#laserscan_to_occupancy_grid_map","text":"","title":"laserscan_to_occupancy_grid_map"},{"location":"sensing/laserscan_to_occupancy_grid_map/#purpose","text":"This package outputs the probability of having an obstacle as occupancy grid map.","title":"Purpose"},{"location":"sensing/laserscan_to_occupancy_grid_map/#inner-workings-algorithms","text":"The basic idea is to take a 2D laserscan and ray trace it to create a time-series processed occupancy grid map. the node take a laserscan and make an occupancy grid map with one frame. ray trace is done by Bresenham's line algorithm. Optionally, obstacle point clouds and raw point clouds can be received and reflected in the occupancy grid map. The reason is that laserscan only uses the most foreground point in the polar coordinate system, so it throws away a lot of information. As a result, the occupancy grid map is almost an UNKNOWN cell. Therefore, the obstacle point cloud and the raw point cloud are used to reflect what is judged to be the ground and what is judged to be an obstacle in the occupancy grid map. The black and red dots represent raw point clouds, and the red dots represent obstacle point clouds. In other words, the black points are determined as the ground, and the red point cloud is the points determined as obstacles. The gray cells are represented as UNKNOWN cells. Using the previous occupancy grid map, update the existence probability using a binary Bayesian filter (1). Also, the unobserved cells are time-decayed like the system noise of the Kalman filter (2). \\hat{P_{o}} = \\frac{(P_{o} * P_{z})}{(P_{o} * P_{z} + (1 - P_{o}) * \\bar{P_{z}})} \\tag{1} \\hat{P_{o}} = \\frac{(P_{o} + 0.5 * \\frac{1}{ratio})}{(\\frac{1}{ratio} + 1)} \\tag{2}","title":"Inner-workings / Algorithms"},{"location":"sensing/laserscan_to_occupancy_grid_map/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"sensing/laserscan_to_occupancy_grid_map/#input","text":"Name Type Description ~/input/laserscan sensor_msgs::LaserScan laserscan ~/input/obstacle_pointcloud sensor_msgs::PointCloud2 obstacle pointcloud ~/input/raw_pointcloud sensor_msgs::PointCloud2 The overall point cloud used to input the obstacle point cloud","title":"Input"},{"location":"sensing/laserscan_to_occupancy_grid_map/#output","text":"Name Type Description ~/output/occupancy_grid_map nav_msgs::OccupancyGrid occupancy grid map","title":"Output"},{"location":"sensing/laserscan_to_occupancy_grid_map/#parameters","text":"","title":"Parameters"},{"location":"sensing/laserscan_to_occupancy_grid_map/#node-parameters","text":"Name Type Description map_frame string map frame base_link_frame string base_link frame input_obstacle_pointcloud bool whether to use the optional obstacle point cloud? If this is true, ~/input/obstacle_pointcloud topics will be received. input_obstacle_and_raw_pointcloud bool whether to use the optional obstacle and raw point cloud? If this is true, ~/input/obstacle_pointcloud and ~/input/raw_pointcloud topics will be received. use_height_filter bool whether to height filter for ~/input/obstacle_pointcloud and ~/input/raw_pointcloud ? By default, the height is set to -1~2m. map_length double The length of the map. -100 if it is 50~50[m] map_resolution double The map cell resolution [m]","title":"Node Parameters"},{"location":"sensing/laserscan_to_occupancy_grid_map/#assumptions-known-limits","text":"In several places we have modified the external code written in BSD3 license. occupancy_grid_map.hpp cost_value.hpp occupancy_grid_map.cpp","title":"Assumptions / Known limits"},{"location":"sensing/laserscan_to_occupancy_grid_map/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"sensing/laserscan_to_occupancy_grid_map/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"sensing/laserscan_to_occupancy_grid_map/#optional-referencesexternal-links","text":"Bresenham's_line_algorithm https://en.wikipedia.org/wiki/Bresenham%27s_line_algorithm https://web.archive.org/web/20080528040104/http://www.research.ibm.com/journal/sj/041/ibmsjIVRIC.pdf","title":"(Optional) References/External links"},{"location":"sensing/laserscan_to_occupancy_grid_map/#optional-future-extensions-unimplemented-parts","text":"The update probability of the binary Bayesian filter is currently hard-coded and requires a code change to be modified. Since there is no special support for moving objects, the probability of existence is not increased for fast objects.","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"sensing/livox/livox_tag_filter/","text":"livox_tag_filter # Purpose # The livox_tag_filter is a node that removes noise from pointcloud by using the following tags: Point property based on spatial position Point property based on intensity Return number Inner-workings / Algorithms # Inputs / Outputs # Input # Name Type Description ~/input sensor_msgs::msg::PointCloud2 reference points Output # Name Type Description ~/output sensor_msgs::msg::PointCloud2 filtered points Parameters # Node Parameters # Name Type Description ignore_tags vector ignored tags (See the following table) Tag Parameters # Bit Description Options 0~1 Point property based on spatial position 00: Normal 01: High confidence level of the noise 10: Moderate confidence level of the noise 11: Low confidence level of the noise 2~3 Point property based on intensity 00: Normal 01: High confidence level of the noise 10: Moderate confidence level of the noise 11: Reserved 4~5 Return number 00: return 0 01: return 1 10: return 2 11: return 3 6~7 Reserved You can download more detail description about the livox from external link [1]. Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # [1] https://www.livoxtech.com/downloads (Optional) Future extensions / Unimplemented parts #","title":"livox_tag_filter"},{"location":"sensing/livox/livox_tag_filter/#livox_tag_filter","text":"","title":"livox_tag_filter"},{"location":"sensing/livox/livox_tag_filter/#purpose","text":"The livox_tag_filter is a node that removes noise from pointcloud by using the following tags: Point property based on spatial position Point property based on intensity Return number","title":"Purpose"},{"location":"sensing/livox/livox_tag_filter/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"sensing/livox/livox_tag_filter/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"sensing/livox/livox_tag_filter/#input","text":"Name Type Description ~/input sensor_msgs::msg::PointCloud2 reference points","title":"Input"},{"location":"sensing/livox/livox_tag_filter/#output","text":"Name Type Description ~/output sensor_msgs::msg::PointCloud2 filtered points","title":"Output"},{"location":"sensing/livox/livox_tag_filter/#parameters","text":"","title":"Parameters"},{"location":"sensing/livox/livox_tag_filter/#node-parameters","text":"Name Type Description ignore_tags vector ignored tags (See the following table)","title":"Node Parameters"},{"location":"sensing/livox/livox_tag_filter/#tag-parameters","text":"Bit Description Options 0~1 Point property based on spatial position 00: Normal 01: High confidence level of the noise 10: Moderate confidence level of the noise 11: Low confidence level of the noise 2~3 Point property based on intensity 00: Normal 01: High confidence level of the noise 10: Moderate confidence level of the noise 11: Reserved 4~5 Return number 00: return 0 01: return 1 10: return 2 11: return 3 6~7 Reserved You can download more detail description about the livox from external link [1].","title":"Tag Parameters"},{"location":"sensing/livox/livox_tag_filter/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"sensing/livox/livox_tag_filter/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"sensing/livox/livox_tag_filter/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"sensing/livox/livox_tag_filter/#optional-referencesexternal-links","text":"[1] https://www.livoxtech.com/downloads","title":"(Optional) References/External links"},{"location":"sensing/livox/livox_tag_filter/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"sensing/pointcloud_preprocessor/","text":"pointcloud_preprocessor # Purpose # The pointcloud_preprocessor is a package that includes the following filters: removing outlier points cropping concatenating pointclouds correcting distortion downsampling Inner-workings / Algorithms # Detail description of each filter's algorithm is in the following links. Filter Name Description Detail concatenate_data subscribe multiple pointclouds and concatenate them into a pointcloud link crop_box_filter remove points within a given box link distortion_corrector compensate pointcloud distortion caused by ego vehicle's movement during 1 scan link downsample_filter downsampling input pointcloud link outlier_filter remove points caused by hardware problems, rain drops and small insects as a noise link passthrough_filter remove points on the outside of a range in given field (e.g. x, y, z, intensity) link pointcloud_accumulator accumulate pointclouds for a given amount of time link vector_map_filter remove points on the outside of lane by using vector map link Inputs / Outputs # Input # Name Type Description ~/input/points sensor_msgs::msg::PointCloud2 reference points ~/input/indices pcl_msgs::msg::Indices reference indices Output # Name Type Description ~/output/points sensor_msgs::msg::PointCloud2 filtered points Parameters # Node Parameters # Name Type Default Value Description input_frame string \" \" input frame id output_frame string \" \" output frame id max_queue_size int 5 max queue size of input/output topics use_indices bool false flag to use pointcloud indices latched_indices bool false flag to latch pointcloud indices approximate_sync bool false flag to use approximate sync option Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # (Optional) Future extensions / Unimplemented parts #","title":"pointcloud_preprocessor"},{"location":"sensing/pointcloud_preprocessor/#pointcloud_preprocessor","text":"","title":"pointcloud_preprocessor"},{"location":"sensing/pointcloud_preprocessor/#purpose","text":"The pointcloud_preprocessor is a package that includes the following filters: removing outlier points cropping concatenating pointclouds correcting distortion downsampling","title":"Purpose"},{"location":"sensing/pointcloud_preprocessor/#inner-workings-algorithms","text":"Detail description of each filter's algorithm is in the following links. Filter Name Description Detail concatenate_data subscribe multiple pointclouds and concatenate them into a pointcloud link crop_box_filter remove points within a given box link distortion_corrector compensate pointcloud distortion caused by ego vehicle's movement during 1 scan link downsample_filter downsampling input pointcloud link outlier_filter remove points caused by hardware problems, rain drops and small insects as a noise link passthrough_filter remove points on the outside of a range in given field (e.g. x, y, z, intensity) link pointcloud_accumulator accumulate pointclouds for a given amount of time link vector_map_filter remove points on the outside of lane by using vector map link","title":"Inner-workings / Algorithms"},{"location":"sensing/pointcloud_preprocessor/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"sensing/pointcloud_preprocessor/#input","text":"Name Type Description ~/input/points sensor_msgs::msg::PointCloud2 reference points ~/input/indices pcl_msgs::msg::Indices reference indices","title":"Input"},{"location":"sensing/pointcloud_preprocessor/#output","text":"Name Type Description ~/output/points sensor_msgs::msg::PointCloud2 filtered points","title":"Output"},{"location":"sensing/pointcloud_preprocessor/#parameters","text":"","title":"Parameters"},{"location":"sensing/pointcloud_preprocessor/#node-parameters","text":"Name Type Default Value Description input_frame string \" \" input frame id output_frame string \" \" output frame id max_queue_size int 5 max queue size of input/output topics use_indices bool false flag to use pointcloud indices latched_indices bool false flag to latch pointcloud indices approximate_sync bool false flag to use approximate sync option","title":"Node Parameters"},{"location":"sensing/pointcloud_preprocessor/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"sensing/pointcloud_preprocessor/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"sensing/pointcloud_preprocessor/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"sensing/pointcloud_preprocessor/#optional-referencesexternal-links","text":"","title":"(Optional) References/External links"},{"location":"sensing/pointcloud_preprocessor/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"sensing/pointcloud_preprocessor/docs/concatenate-data/","text":"concatenate_data # Purpose # The concatenate_data is a node that concatenates multiple pointclouds acquired by multiple LiDARs into a pointcloud. Inner-workings / Algorithms # Inputs / Outputs # Input # Name Type Description ~/input/points sensor_msgs::msg::Pointcloud2 reference points ~/input/twist autoware_auto_vehicle_msgs::msg::VelocityReport vehicle velocity Output # Name Type Description ~/output/points sensor_msgs::msg::Pointcloud2 filtered points Parameters # Name Type Default Value Description input_frame string \" \" input frame id output_frame string \" \" output frame id max_queue_size int 5 max queue size of input/output topics Core Parameters # Name Type Default Value Description timeout_sec double 0.1 tolerance of time to publish next pointcloud [s] When this time limit is exceeded, the filter concatenates and publishes pointcloud, even if not all the point clouds are subscribed. Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # (Optional) Future extensions / Unimplemented parts #","title":"concatenate_data"},{"location":"sensing/pointcloud_preprocessor/docs/concatenate-data/#concatenate_data","text":"","title":"concatenate_data"},{"location":"sensing/pointcloud_preprocessor/docs/concatenate-data/#purpose","text":"The concatenate_data is a node that concatenates multiple pointclouds acquired by multiple LiDARs into a pointcloud.","title":"Purpose"},{"location":"sensing/pointcloud_preprocessor/docs/concatenate-data/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"sensing/pointcloud_preprocessor/docs/concatenate-data/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"sensing/pointcloud_preprocessor/docs/concatenate-data/#input","text":"Name Type Description ~/input/points sensor_msgs::msg::Pointcloud2 reference points ~/input/twist autoware_auto_vehicle_msgs::msg::VelocityReport vehicle velocity","title":"Input"},{"location":"sensing/pointcloud_preprocessor/docs/concatenate-data/#output","text":"Name Type Description ~/output/points sensor_msgs::msg::Pointcloud2 filtered points","title":"Output"},{"location":"sensing/pointcloud_preprocessor/docs/concatenate-data/#parameters","text":"Name Type Default Value Description input_frame string \" \" input frame id output_frame string \" \" output frame id max_queue_size int 5 max queue size of input/output topics","title":"Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/concatenate-data/#core-parameters","text":"Name Type Default Value Description timeout_sec double 0.1 tolerance of time to publish next pointcloud [s] When this time limit is exceeded, the filter concatenates and publishes pointcloud, even if not all the point clouds are subscribed.","title":"Core Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/concatenate-data/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"sensing/pointcloud_preprocessor/docs/concatenate-data/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"sensing/pointcloud_preprocessor/docs/concatenate-data/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"sensing/pointcloud_preprocessor/docs/concatenate-data/#optional-referencesexternal-links","text":"","title":"(Optional) References/External links"},{"location":"sensing/pointcloud_preprocessor/docs/concatenate-data/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"sensing/pointcloud_preprocessor/docs/crop-box-filter/","text":"box_crop_filter # Purpose # The box_crop_filter is a node that removes points with in a given box region. This filter is used to remove the points that hit the vehicle itself. Inner-workings / Algorithms # Inputs / Outputs # Name Type Description ~/input/points sensor_msgs::msg::PointCloud2 reference points Output # Name Type Description ~/output/points sensor_msgs::msg::PointCloud2 filtered points Parameters # Core Parameters # Name Type Default Value Description min_x double -1.0 x-coordinate minimum value for crop range max_x double 1.0 x-coordinate maximum value for crop range min_y double -1.0 y-coordinate minimum value for crop range max_y double 1.0 y-coordinate maximum value for crop range min_z double -1.0 z-coordinate minimum value for crop range max_z double 1.0 z-coordinate maximum value for crop range Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # (Optional) Future extensions / Unimplemented parts #","title":"box_crop_filter"},{"location":"sensing/pointcloud_preprocessor/docs/crop-box-filter/#box_crop_filter","text":"","title":"box_crop_filter"},{"location":"sensing/pointcloud_preprocessor/docs/crop-box-filter/#purpose","text":"The box_crop_filter is a node that removes points with in a given box region. This filter is used to remove the points that hit the vehicle itself.","title":"Purpose"},{"location":"sensing/pointcloud_preprocessor/docs/crop-box-filter/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"sensing/pointcloud_preprocessor/docs/crop-box-filter/#inputs-outputs","text":"Name Type Description ~/input/points sensor_msgs::msg::PointCloud2 reference points","title":"Inputs / Outputs"},{"location":"sensing/pointcloud_preprocessor/docs/crop-box-filter/#output","text":"Name Type Description ~/output/points sensor_msgs::msg::PointCloud2 filtered points","title":"Output"},{"location":"sensing/pointcloud_preprocessor/docs/crop-box-filter/#parameters","text":"","title":"Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/crop-box-filter/#core-parameters","text":"Name Type Default Value Description min_x double -1.0 x-coordinate minimum value for crop range max_x double 1.0 x-coordinate maximum value for crop range min_y double -1.0 y-coordinate minimum value for crop range max_y double 1.0 y-coordinate maximum value for crop range min_z double -1.0 z-coordinate minimum value for crop range max_z double 1.0 z-coordinate maximum value for crop range","title":"Core Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/crop-box-filter/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"sensing/pointcloud_preprocessor/docs/crop-box-filter/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"sensing/pointcloud_preprocessor/docs/crop-box-filter/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"sensing/pointcloud_preprocessor/docs/crop-box-filter/#optional-referencesexternal-links","text":"","title":"(Optional) References/External links"},{"location":"sensing/pointcloud_preprocessor/docs/crop-box-filter/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"sensing/pointcloud_preprocessor/docs/distortion-corrector/","text":"distortion_corrector # Purpose # The distortion_corrector is a node that compensates pointcloud distortion caused by ego vehicle's movement during 1 scan. Inner-workings / Algorithms # Inputs / Outputs # Input # Name Type Description ~/input/points sensor_msgs::msg::PointCloud2 reference points ~/input/velocity_report autoware_auto_vehicle_msgs::msg::VelocityReport vehicle velocity Output # Name Type Description ~/output/points sensor_msgs::msg::PointCloud2 filtered points Parameters # Core Parameters # Name Type Default Value Description timestamp_field_name string \"time_stamp\" time stamp field name Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # (Optional) Future extensions / Unimplemented parts #","title":"distortion_corrector"},{"location":"sensing/pointcloud_preprocessor/docs/distortion-corrector/#distortion_corrector","text":"","title":"distortion_corrector"},{"location":"sensing/pointcloud_preprocessor/docs/distortion-corrector/#purpose","text":"The distortion_corrector is a node that compensates pointcloud distortion caused by ego vehicle's movement during 1 scan.","title":"Purpose"},{"location":"sensing/pointcloud_preprocessor/docs/distortion-corrector/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"sensing/pointcloud_preprocessor/docs/distortion-corrector/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"sensing/pointcloud_preprocessor/docs/distortion-corrector/#input","text":"Name Type Description ~/input/points sensor_msgs::msg::PointCloud2 reference points ~/input/velocity_report autoware_auto_vehicle_msgs::msg::VelocityReport vehicle velocity","title":"Input"},{"location":"sensing/pointcloud_preprocessor/docs/distortion-corrector/#output","text":"Name Type Description ~/output/points sensor_msgs::msg::PointCloud2 filtered points","title":"Output"},{"location":"sensing/pointcloud_preprocessor/docs/distortion-corrector/#parameters","text":"","title":"Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/distortion-corrector/#core-parameters","text":"Name Type Default Value Description timestamp_field_name string \"time_stamp\" time stamp field name","title":"Core Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/distortion-corrector/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"sensing/pointcloud_preprocessor/docs/distortion-corrector/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"sensing/pointcloud_preprocessor/docs/distortion-corrector/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"sensing/pointcloud_preprocessor/docs/distortion-corrector/#optional-referencesexternal-links","text":"","title":"(Optional) References/External links"},{"location":"sensing/pointcloud_preprocessor/docs/distortion-corrector/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/","text":"downsample_filter # Purpose # The downsample_filter is a node that reduces the number of points. Inner-workings / Algorithms # Approximate Downsample Filter # WIP Random Downsample Filter # WIP Voxel Grid Downsample Filter # WIP Inputs / Outputs # Name Type Description ~/input/points sensor_msgs::msg::PointCloud2 reference points Output # Name Type Description ~/output/points sensor_msgs::msg::PointCloud2 filtered points Parameters # Node Parameters # Name Type Default Value Description voxel_size_x double 0.3 voxel size x [m] voxel_size_y double 0.3 voxel size y [m] voxel_size_z double 0.1 voxel size z [m] sample_num int 1500 number of indices to be sampled Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # (Optional) Future extensions / Unimplemented parts #","title":"downsample_filter"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#downsample_filter","text":"","title":"downsample_filter"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#purpose","text":"The downsample_filter is a node that reduces the number of points.","title":"Purpose"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#approximate-downsample-filter","text":"WIP","title":"Approximate Downsample Filter"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#random-downsample-filter","text":"WIP","title":"Random Downsample Filter"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#voxel-grid-downsample-filter","text":"WIP","title":"Voxel Grid Downsample Filter"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#inputs-outputs","text":"Name Type Description ~/input/points sensor_msgs::msg::PointCloud2 reference points","title":"Inputs / Outputs"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#output","text":"Name Type Description ~/output/points sensor_msgs::msg::PointCloud2 filtered points","title":"Output"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#parameters","text":"","title":"Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#node-parameters","text":"Name Type Default Value Description voxel_size_x double 0.3 voxel size x [m] voxel_size_y double 0.3 voxel size y [m] voxel_size_z double 0.1 voxel size z [m] sample_num int 1500 number of indices to be sampled","title":"Node Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#optional-referencesexternal-links","text":"","title":"(Optional) References/External links"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"sensing/pointcloud_preprocessor/docs/outlier-filter/","text":"outlier_filter # Purpose # This node is an outlier filter based on a occupancy grid map. Depending on the implementation of occupancy grid map, it can be called an outlier filter in time series, since the occupancy grid map expresses the occupancy probabilities in time series. Inner-workings / Algorithms # Dual Return Outlier Filter # WIP Occupancy GridMap Outlier Filter # Use the occupancy grid map to separate point clouds into those with low occupancy probability and those with high occupancy probability. The point clouds that belong to the low occupancy probability are not necessarily outliers. In particular, the top of the moving object tends to belong to the low occupancy probability. Therefore, if use_radius_search_2d_filter is true, then apply an radius search 2d outlier filter to the point cloud that is determined to have a low occupancy probability. For each low occupancy probability point, determine the outlier from the radius ( radius_search_2d_filter/search_radius ) and the number of point clouds. In this case, the point cloud to be referenced is not only low occupancy probability points, but all point cloud including high occupancy probability points. The number of point clouds can be multiplied by radius_search_2d_filter/min_points_and_distance_ratio and distance from base link. However, the minimum and maximum number of point clouds is limited. The following video is a sample. Yellow points are high occupancy probability, green points are low occupancy probability which is not an outlier, and red points are outliers. At around 0:15 and 1:16 in the first video, a bird crosses the road, but it is considered as an outlier. movie1 movie2 Radius Search 2d Outlier Filter [1] # WIP Ring Outlier Filter # WIP Voxel Grid Outlier Filter # WIP Inputs / Outputs # Input # Name Type Description ~/input/pointcloud sensor_msgs/PointCloud2 Obstacle point cloud with ground removed. ~/input/occupancy_grid_map nav_msgs/OccupancyGrid A map in which the probability of the presence of an obstacle is occupancy probability map Output # Name Type Description ~/output/pointcloud sensor_msgs/PointCloud2 Point cloud with outliers removed. trajectory ~/output/debug/outlier/pointcloud sensor_msgs/PointCloud2 Point clouds removed as outliers. ~/output/debug/low_confidence/pointcloud sensor_msgs/PointCloud2 Point clouds that had a low probability of occupancy in the occupancy grid map. However, it is not considered as an outlier. ~/output/debug/high_confidence/pointcloud sensor_msgs/PointCloud2 Point clouds that had a high probability of occupancy in the occupancy grid map. trajectory Parameters # Name Type Description map_frame string map frame id base_link_frame string base link frame id cost_threshold int Cost threshold of occupancy grid map (0~100). 100 means 100% probability that there is an obstacle, close to 50 means that it is indistinguishable whether it is an obstacle or free space, 0 means that there is no obstacle. enable_debugger bool Whether to output the point cloud for debugging. use_radius_search_2d_filter bool Whether or not to apply density-based outlier filters to objects that are judged to have low probability of occupancy on the occupancy grid map. radius_search_2d_filter/search_radius float Radius when calculating the density radius_search_2d_filter/min_points_and_distance_ratio float Threshold value of the number of point clouds per radius when the distance from baselink is 1m, because the number of point clouds varies with the distance from baselink. radius_search_2d_filter/min_points int Minimum number of point clouds per radius radius_search_2d_filter/max_points int Maximum number of point clouds per radius Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # [1] https://pcl.readthedocs.io/projects/tutorials/en/latest/remove_outliers.html (Optional) Future extensions / Unimplemented parts #","title":"outlier_filter"},{"location":"sensing/pointcloud_preprocessor/docs/outlier-filter/#outlier_filter","text":"","title":"outlier_filter"},{"location":"sensing/pointcloud_preprocessor/docs/outlier-filter/#purpose","text":"This node is an outlier filter based on a occupancy grid map. Depending on the implementation of occupancy grid map, it can be called an outlier filter in time series, since the occupancy grid map expresses the occupancy probabilities in time series.","title":"Purpose"},{"location":"sensing/pointcloud_preprocessor/docs/outlier-filter/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"sensing/pointcloud_preprocessor/docs/outlier-filter/#dual-return-outlier-filter","text":"WIP","title":"Dual Return Outlier Filter"},{"location":"sensing/pointcloud_preprocessor/docs/outlier-filter/#occupancy-gridmap-outlier-filter","text":"Use the occupancy grid map to separate point clouds into those with low occupancy probability and those with high occupancy probability. The point clouds that belong to the low occupancy probability are not necessarily outliers. In particular, the top of the moving object tends to belong to the low occupancy probability. Therefore, if use_radius_search_2d_filter is true, then apply an radius search 2d outlier filter to the point cloud that is determined to have a low occupancy probability. For each low occupancy probability point, determine the outlier from the radius ( radius_search_2d_filter/search_radius ) and the number of point clouds. In this case, the point cloud to be referenced is not only low occupancy probability points, but all point cloud including high occupancy probability points. The number of point clouds can be multiplied by radius_search_2d_filter/min_points_and_distance_ratio and distance from base link. However, the minimum and maximum number of point clouds is limited. The following video is a sample. Yellow points are high occupancy probability, green points are low occupancy probability which is not an outlier, and red points are outliers. At around 0:15 and 1:16 in the first video, a bird crosses the road, but it is considered as an outlier. movie1 movie2","title":"Occupancy GridMap Outlier Filter"},{"location":"sensing/pointcloud_preprocessor/docs/outlier-filter/#radius-search-2d-outlier-filter-1","text":"WIP","title":"Radius Search 2d Outlier Filter [1]"},{"location":"sensing/pointcloud_preprocessor/docs/outlier-filter/#ring-outlier-filter","text":"WIP","title":"Ring Outlier Filter"},{"location":"sensing/pointcloud_preprocessor/docs/outlier-filter/#voxel-grid-outlier-filter","text":"WIP","title":"Voxel Grid Outlier Filter"},{"location":"sensing/pointcloud_preprocessor/docs/outlier-filter/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"sensing/pointcloud_preprocessor/docs/outlier-filter/#input","text":"Name Type Description ~/input/pointcloud sensor_msgs/PointCloud2 Obstacle point cloud with ground removed. ~/input/occupancy_grid_map nav_msgs/OccupancyGrid A map in which the probability of the presence of an obstacle is occupancy probability map","title":"Input"},{"location":"sensing/pointcloud_preprocessor/docs/outlier-filter/#output","text":"Name Type Description ~/output/pointcloud sensor_msgs/PointCloud2 Point cloud with outliers removed. trajectory ~/output/debug/outlier/pointcloud sensor_msgs/PointCloud2 Point clouds removed as outliers. ~/output/debug/low_confidence/pointcloud sensor_msgs/PointCloud2 Point clouds that had a low probability of occupancy in the occupancy grid map. However, it is not considered as an outlier. ~/output/debug/high_confidence/pointcloud sensor_msgs/PointCloud2 Point clouds that had a high probability of occupancy in the occupancy grid map. trajectory","title":"Output"},{"location":"sensing/pointcloud_preprocessor/docs/outlier-filter/#parameters","text":"Name Type Description map_frame string map frame id base_link_frame string base link frame id cost_threshold int Cost threshold of occupancy grid map (0~100). 100 means 100% probability that there is an obstacle, close to 50 means that it is indistinguishable whether it is an obstacle or free space, 0 means that there is no obstacle. enable_debugger bool Whether to output the point cloud for debugging. use_radius_search_2d_filter bool Whether or not to apply density-based outlier filters to objects that are judged to have low probability of occupancy on the occupancy grid map. radius_search_2d_filter/search_radius float Radius when calculating the density radius_search_2d_filter/min_points_and_distance_ratio float Threshold value of the number of point clouds per radius when the distance from baselink is 1m, because the number of point clouds varies with the distance from baselink. radius_search_2d_filter/min_points int Minimum number of point clouds per radius radius_search_2d_filter/max_points int Maximum number of point clouds per radius","title":"Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/outlier-filter/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"sensing/pointcloud_preprocessor/docs/outlier-filter/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"sensing/pointcloud_preprocessor/docs/outlier-filter/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"sensing/pointcloud_preprocessor/docs/outlier-filter/#optional-referencesexternal-links","text":"[1] https://pcl.readthedocs.io/projects/tutorials/en/latest/remove_outliers.html","title":"(Optional) References/External links"},{"location":"sensing/pointcloud_preprocessor/docs/outlier-filter/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"sensing/pointcloud_preprocessor/docs/passthrough-filter/","text":"passthrough_filter # Purpose # The passthrough_filter is a node that removes points on the outside of a range in a given field (e.g. x, y, z, intensity, ring, etc). Inner-workings / Algorithms # Inputs / Outputs # Input # Name Type Description ~/input/points sensor_msgs::msg::PointCloud2 reference points ~/input/indices pcl_msgs::msg::Indices reference indices Output # Name Type Description ~/output/points sensor_msgs::msg::PointCloud2 filtered points Parameters # Core Parameters # Name Type Default Value Description filter_limit_min int 0 minimum allowed field value filter_limit_max int 127 maximum allowed field value filter_field_name string \"ring\" filtering field name keep_organized bool false flag to keep indices structure filter_limit_negative bool false flag to return whether the data is inside limit or not Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # (Optional) Future extensions / Unimplemented parts #","title":"passthrough_filter"},{"location":"sensing/pointcloud_preprocessor/docs/passthrough-filter/#passthrough_filter","text":"","title":"passthrough_filter"},{"location":"sensing/pointcloud_preprocessor/docs/passthrough-filter/#purpose","text":"The passthrough_filter is a node that removes points on the outside of a range in a given field (e.g. x, y, z, intensity, ring, etc).","title":"Purpose"},{"location":"sensing/pointcloud_preprocessor/docs/passthrough-filter/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"sensing/pointcloud_preprocessor/docs/passthrough-filter/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"sensing/pointcloud_preprocessor/docs/passthrough-filter/#input","text":"Name Type Description ~/input/points sensor_msgs::msg::PointCloud2 reference points ~/input/indices pcl_msgs::msg::Indices reference indices","title":"Input"},{"location":"sensing/pointcloud_preprocessor/docs/passthrough-filter/#output","text":"Name Type Description ~/output/points sensor_msgs::msg::PointCloud2 filtered points","title":"Output"},{"location":"sensing/pointcloud_preprocessor/docs/passthrough-filter/#parameters","text":"","title":"Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/passthrough-filter/#core-parameters","text":"Name Type Default Value Description filter_limit_min int 0 minimum allowed field value filter_limit_max int 127 maximum allowed field value filter_field_name string \"ring\" filtering field name keep_organized bool false flag to keep indices structure filter_limit_negative bool false flag to return whether the data is inside limit or not","title":"Core Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/passthrough-filter/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"sensing/pointcloud_preprocessor/docs/passthrough-filter/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"sensing/pointcloud_preprocessor/docs/passthrough-filter/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"sensing/pointcloud_preprocessor/docs/passthrough-filter/#optional-referencesexternal-links","text":"","title":"(Optional) References/External links"},{"location":"sensing/pointcloud_preprocessor/docs/passthrough-filter/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"sensing/pointcloud_preprocessor/docs/pointcloud-accumulator/","text":"pointcloud_accumulator # Purpose # The pointcloud_accumulator is a node that accumulates pointclouds for a given amount of time. Inner-workings / Algorithms # Inputs / Outputs # Input # Name Type Description ~/input/points sensor_msgs::msg::PointCloud2 reference points Output # Name Type Description ~/output/points sensor_msgs::msg::PointCloud2 filtered points Parameters # Core Parameters # Name Type Default Value Description accumulation_time_sec double 2.0 accumulation period [s] pointcloud_buffer_size int 50 buffer size Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # (Optional) Future extensions / Unimplemented parts #","title":"pointcloud_accumulator"},{"location":"sensing/pointcloud_preprocessor/docs/pointcloud-accumulator/#pointcloud_accumulator","text":"","title":"pointcloud_accumulator"},{"location":"sensing/pointcloud_preprocessor/docs/pointcloud-accumulator/#purpose","text":"The pointcloud_accumulator is a node that accumulates pointclouds for a given amount of time.","title":"Purpose"},{"location":"sensing/pointcloud_preprocessor/docs/pointcloud-accumulator/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"sensing/pointcloud_preprocessor/docs/pointcloud-accumulator/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"sensing/pointcloud_preprocessor/docs/pointcloud-accumulator/#input","text":"Name Type Description ~/input/points sensor_msgs::msg::PointCloud2 reference points","title":"Input"},{"location":"sensing/pointcloud_preprocessor/docs/pointcloud-accumulator/#output","text":"Name Type Description ~/output/points sensor_msgs::msg::PointCloud2 filtered points","title":"Output"},{"location":"sensing/pointcloud_preprocessor/docs/pointcloud-accumulator/#parameters","text":"","title":"Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/pointcloud-accumulator/#core-parameters","text":"Name Type Default Value Description accumulation_time_sec double 2.0 accumulation period [s] pointcloud_buffer_size int 50 buffer size","title":"Core Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/pointcloud-accumulator/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"sensing/pointcloud_preprocessor/docs/pointcloud-accumulator/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"sensing/pointcloud_preprocessor/docs/pointcloud-accumulator/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"sensing/pointcloud_preprocessor/docs/pointcloud-accumulator/#optional-referencesexternal-links","text":"","title":"(Optional) References/External links"},{"location":"sensing/pointcloud_preprocessor/docs/pointcloud-accumulator/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-filter/","text":"vector_map_filter # Purpose # The vector_map_filter is a node that removes points on the outside of lane by using vector map. Inner-workings / Algorithms # Inputs / Outputs # Input # Name Type Description ~/input/points sensor_msgs::msg::PointCloud2 reference points ~/input/vector_map autoware_auto_mapping_msgs::msg::HADMapBin vector map Output # Name Type Description ~/output/points sensor_msgs::msg::PointCloud2 filtered points Parameters # Core Parameters # Name Type Default Value Description voxel_size_x double 0.04 voxel size voxel_size_y double 0.04 voxel size Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # (Optional) Future extensions / Unimplemented parts #","title":"vector_map_filter"},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-filter/#vector_map_filter","text":"","title":"vector_map_filter"},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-filter/#purpose","text":"The vector_map_filter is a node that removes points on the outside of lane by using vector map.","title":"Purpose"},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-filter/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-filter/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-filter/#input","text":"Name Type Description ~/input/points sensor_msgs::msg::PointCloud2 reference points ~/input/vector_map autoware_auto_mapping_msgs::msg::HADMapBin vector map","title":"Input"},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-filter/#output","text":"Name Type Description ~/output/points sensor_msgs::msg::PointCloud2 filtered points","title":"Output"},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-filter/#parameters","text":"","title":"Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-filter/#core-parameters","text":"Name Type Default Value Description voxel_size_x double 0.04 voxel size voxel_size_y double 0.04 voxel size","title":"Core Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-filter/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-filter/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-filter/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-filter/#optional-referencesexternal-links","text":"","title":"(Optional) References/External links"},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-filter/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"sensing/tier4_pcl_extensions/","text":"tier4_pcl_extensions # Purpose # The tier4_pcl_extensions is a pcl extension library. The voxel grid filter in this package works with a different algorithm than the original one. Inner-workings / Algorithms # Original Algorithm [1] # create a 3D voxel grid over the input pointcloud data calculate centroid in each voxel all the points are approximated with their centroid Extended Algorithm # create a 3D voxel grid over the input pointcloud data calculate centroid in each voxel all the points are approximated with the closest point to their centroid Inputs / Outputs # Parameters # Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # [1] https://pointclouds.org/documentation/tutorials/voxel_grid.html (Optional) Future extensions / Unimplemented parts #","title":"tier4_pcl_extensions"},{"location":"sensing/tier4_pcl_extensions/#tier4_pcl_extensions","text":"","title":"tier4_pcl_extensions"},{"location":"sensing/tier4_pcl_extensions/#purpose","text":"The tier4_pcl_extensions is a pcl extension library. The voxel grid filter in this package works with a different algorithm than the original one.","title":"Purpose"},{"location":"sensing/tier4_pcl_extensions/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"sensing/tier4_pcl_extensions/#original-algorithm-1","text":"create a 3D voxel grid over the input pointcloud data calculate centroid in each voxel all the points are approximated with their centroid","title":"Original Algorithm [1]"},{"location":"sensing/tier4_pcl_extensions/#extended-algorithm","text":"create a 3D voxel grid over the input pointcloud data calculate centroid in each voxel all the points are approximated with the closest point to their centroid","title":"Extended Algorithm"},{"location":"sensing/tier4_pcl_extensions/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"sensing/tier4_pcl_extensions/#parameters","text":"","title":"Parameters"},{"location":"sensing/tier4_pcl_extensions/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"sensing/tier4_pcl_extensions/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"sensing/tier4_pcl_extensions/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"sensing/tier4_pcl_extensions/#optional-referencesexternal-links","text":"[1] https://pointclouds.org/documentation/tutorials/voxel_grid.html","title":"(Optional) References/External links"},{"location":"sensing/tier4_pcl_extensions/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"simulator/dummy_perception_publisher/","text":"dummy_perception_publisher # Purpose # This node publishes the result of the dummy detection with the type of perception. Inner-workings / Algorithms # Inputs / Outputs # Input # Name Type Description /tf tf2_msgs/TFMessage TF (self-pose) input/object dummy_perception_publisher::msg::Object dummy detection objects Output # Name Type Description output/dynamic_object autoware_perception_msgs::msg::DetectedObjectsWithFeature Publishes objects output/points_raw sensor_msgs::msg::PointCloud2 point cloud of objects Parameters # Name Type Default Value Explanation visible_range double 100.0 sensor visible range [m] detection_successful_rate double 0.8 sensor detection rate. (min) 0.0 - 1.0(max) enable_ray_tracing bool true if True, use ray tracking use_object_recognition bool true if True, publish objects topic Node Parameters # None. Core Parameters # None. Assumptions / Known limits # TBD.","title":"dummy_perception_publisher"},{"location":"simulator/dummy_perception_publisher/#dummy_perception_publisher","text":"","title":"dummy_perception_publisher"},{"location":"simulator/dummy_perception_publisher/#purpose","text":"This node publishes the result of the dummy detection with the type of perception.","title":"Purpose"},{"location":"simulator/dummy_perception_publisher/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"simulator/dummy_perception_publisher/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"simulator/dummy_perception_publisher/#input","text":"Name Type Description /tf tf2_msgs/TFMessage TF (self-pose) input/object dummy_perception_publisher::msg::Object dummy detection objects","title":"Input"},{"location":"simulator/dummy_perception_publisher/#output","text":"Name Type Description output/dynamic_object autoware_perception_msgs::msg::DetectedObjectsWithFeature Publishes objects output/points_raw sensor_msgs::msg::PointCloud2 point cloud of objects","title":"Output"},{"location":"simulator/dummy_perception_publisher/#parameters","text":"Name Type Default Value Explanation visible_range double 100.0 sensor visible range [m] detection_successful_rate double 0.8 sensor detection rate. (min) 0.0 - 1.0(max) enable_ray_tracing bool true if True, use ray tracking use_object_recognition bool true if True, publish objects topic","title":"Parameters"},{"location":"simulator/dummy_perception_publisher/#node-parameters","text":"None.","title":"Node Parameters"},{"location":"simulator/dummy_perception_publisher/#core-parameters","text":"None.","title":"Core Parameters"},{"location":"simulator/dummy_perception_publisher/#assumptions-known-limits","text":"TBD.","title":"Assumptions / Known limits"},{"location":"simulator/fault_injection/","text":"fault_injection # Purpose # This package is used to convert pseudo system faults from PSim to Diagnostics and notify Autoware. The component diagram is as follows: Test # source install/setup.bash cd fault_injection launch_test test/test_fault_injection_node.test.py Inner-workings / Algorithms # Inputs / Outputs # Input # Name Type Description ~/input/simulation_events autoware_simulation_msgs::msg::SimulationEvents simulation events Output # None. Parameters # None. Node Parameters # None. Core Parameters # None. Assumptions / Known limits # TBD.","title":"fault_injection"},{"location":"simulator/fault_injection/#fault_injection","text":"","title":"fault_injection"},{"location":"simulator/fault_injection/#purpose","text":"This package is used to convert pseudo system faults from PSim to Diagnostics and notify Autoware. The component diagram is as follows:","title":"Purpose"},{"location":"simulator/fault_injection/#test","text":"source install/setup.bash cd fault_injection launch_test test/test_fault_injection_node.test.py","title":"Test"},{"location":"simulator/fault_injection/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"simulator/fault_injection/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"simulator/fault_injection/#input","text":"Name Type Description ~/input/simulation_events autoware_simulation_msgs::msg::SimulationEvents simulation events","title":"Input"},{"location":"simulator/fault_injection/#output","text":"None.","title":"Output"},{"location":"simulator/fault_injection/#parameters","text":"None.","title":"Parameters"},{"location":"simulator/fault_injection/#node-parameters","text":"None.","title":"Node Parameters"},{"location":"simulator/fault_injection/#core-parameters","text":"None.","title":"Core Parameters"},{"location":"simulator/fault_injection/#assumptions-known-limits","text":"TBD.","title":"Assumptions / Known limits"},{"location":"system/autoware_error_monitor/","text":"autoware_error_monitor # Purpose # Autoware Error Monitor has two main functions. It is to judge the system hazard level from the aggregated diagnostic information of each module of Autoware. It enables automatic recovery from the emergency state. Inner-workings / Algorithms # State Transition # updateEmergencyHoldingCondition Flow Chart # Inputs / Outputs # Input # Name Type Description /diagnostics_agg diagnostic_msgs::msg::DiagnosticArray Diagnostic information aggregated based diagnostic_aggregator setting is used to /autoware/state autoware_auto_system_msgs::msg::AutowareState Required to ignore error during Route, Planning and Finalizing. /control/current_gate_mode autoware_control_msgs::msg::GateMode Required to select the appropriate module from autonomous_driving or external_control /vehicle/control_mode autoware_auto_vehicle_msgs::msg::ControlModeReport Required to not hold emergency during manual driving Output # Name Type Description /system/emergency/hazard_status autoware_auto_system_msgs::msg::HazardStatusStamped HazardStatus contains system hazard level, emergency hold status and failure details /diagnostics_err diagnostic_msgs::msg::DiagnosticArray This has the same contents as HazardStatus. This is used for visualization Parameters # Node Parameters # Name Type Default Value Explanation ignore_missing_diagnostics bool false If this parameter is turned off, it will be ignored if required modules have not been received. add_leaf_diagnostics bool true Required to use children diagnostics. diag_timeout_sec double 1.0 (sec) If required diagnostic is not received for a diag_timeout_sec , the diagnostic state become STALE state. data_ready_timeout double 30.0 If input topics required for autoware_error_monitor are not available for data_ready_timeout seconds, autoware_state will translate to emergency state. Core Parameters # Name Type Default Value Explanation hazard_recovery_timeout double 5.0 The vehicle can recovery to normal driving if emergencies disappear during hazard_recovery_timeout . use_emergency_hold bool false If it is false, the vehicle will return to normal as soon as emergencies disappear. use_emergency_hold_in_manual_driving bool false If this parameter is turned off, emergencies will be ignored during manual driving. emergency_hazard_level int 2 If hazard_level is more than emergency_hazard_level, autoware state will translate to emergency state YAML format for autoware_error_monitor # The parameter key should be filled with the hierarchical diagnostics output by diagnostic_aggregator. Parameters prefixed with required_modules.autonomous_driving are for autonomous driving. Parameters with the required_modules.remote_control prefix are for remote control. If the value is default , the default value will be set. Key Type Default Value Explanation required_modules.autonomous_driving.DIAGNOSTIC_NAME.sf_at string \"none\" Diagnostic level where it becomes Safe Fault. Available options are \"none\" , \"warn\" , \"error\" . required_modules.autonomous_driving.DIAGNOSTIC_NAME.lf_at string \"warn\" Diagnostic level where it becomes Latent Fault. Available options are \"none\" , \"warn\" , \"error\" . required_modules.autonomous_driving.DIAGNOSTIC_NAME.spf_at string \"error\" Diagnostic level where it becomes Single Point Fault. Available options are \"none\" , \"warn\" , \"error\" . required_modules.autonomous_driving.DIAGNOSTIC_NAME.auto_recovery string \"true\" Determines whether the system will automatically recover when it recovers from an error. required_modules.remote_control.DIAGNOSTIC_NAME.sf_at string \"none\" Diagnostic level where it becomes Safe Fault. Available options are \"none\" , \"warn\" , \"error\" . required_modules.remote_control.DIAGNOSTIC_NAME.lf_at string \"warn\" Diagnostic level where it becomes Latent Fault. Available options are \"none\" , \"warn\" , \"error\" . required_modules.remote_control.DIAGNOSTIC_NAME.spf_at string \"error\" Diagnostic level where it becomes Single Point Fault. Available options are \"none\" , \"warn\" , \"error\" . required_modules.remote_control.DIAGNOSTIC_NAME.auto_recovery string \"true\" Determines whether the system will automatically recover when it recovers from an error. Assumptions / Known limits # TBD.","title":"autoware_error_monitor"},{"location":"system/autoware_error_monitor/#autoware_error_monitor","text":"","title":"autoware_error_monitor"},{"location":"system/autoware_error_monitor/#purpose","text":"Autoware Error Monitor has two main functions. It is to judge the system hazard level from the aggregated diagnostic information of each module of Autoware. It enables automatic recovery from the emergency state.","title":"Purpose"},{"location":"system/autoware_error_monitor/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"system/autoware_error_monitor/#state-transition","text":"","title":"State Transition"},{"location":"system/autoware_error_monitor/#updateemergencyholdingcondition-flow-chart","text":"","title":"updateEmergencyHoldingCondition Flow Chart"},{"location":"system/autoware_error_monitor/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"system/autoware_error_monitor/#input","text":"Name Type Description /diagnostics_agg diagnostic_msgs::msg::DiagnosticArray Diagnostic information aggregated based diagnostic_aggregator setting is used to /autoware/state autoware_auto_system_msgs::msg::AutowareState Required to ignore error during Route, Planning and Finalizing. /control/current_gate_mode autoware_control_msgs::msg::GateMode Required to select the appropriate module from autonomous_driving or external_control /vehicle/control_mode autoware_auto_vehicle_msgs::msg::ControlModeReport Required to not hold emergency during manual driving","title":"Input"},{"location":"system/autoware_error_monitor/#output","text":"Name Type Description /system/emergency/hazard_status autoware_auto_system_msgs::msg::HazardStatusStamped HazardStatus contains system hazard level, emergency hold status and failure details /diagnostics_err diagnostic_msgs::msg::DiagnosticArray This has the same contents as HazardStatus. This is used for visualization","title":"Output"},{"location":"system/autoware_error_monitor/#parameters","text":"","title":"Parameters"},{"location":"system/autoware_error_monitor/#node-parameters","text":"Name Type Default Value Explanation ignore_missing_diagnostics bool false If this parameter is turned off, it will be ignored if required modules have not been received. add_leaf_diagnostics bool true Required to use children diagnostics. diag_timeout_sec double 1.0 (sec) If required diagnostic is not received for a diag_timeout_sec , the diagnostic state become STALE state. data_ready_timeout double 30.0 If input topics required for autoware_error_monitor are not available for data_ready_timeout seconds, autoware_state will translate to emergency state.","title":"Node Parameters"},{"location":"system/autoware_error_monitor/#core-parameters","text":"Name Type Default Value Explanation hazard_recovery_timeout double 5.0 The vehicle can recovery to normal driving if emergencies disappear during hazard_recovery_timeout . use_emergency_hold bool false If it is false, the vehicle will return to normal as soon as emergencies disappear. use_emergency_hold_in_manual_driving bool false If this parameter is turned off, emergencies will be ignored during manual driving. emergency_hazard_level int 2 If hazard_level is more than emergency_hazard_level, autoware state will translate to emergency state","title":"Core Parameters"},{"location":"system/autoware_error_monitor/#yaml-format-for-autoware_error_monitor","text":"The parameter key should be filled with the hierarchical diagnostics output by diagnostic_aggregator. Parameters prefixed with required_modules.autonomous_driving are for autonomous driving. Parameters with the required_modules.remote_control prefix are for remote control. If the value is default , the default value will be set. Key Type Default Value Explanation required_modules.autonomous_driving.DIAGNOSTIC_NAME.sf_at string \"none\" Diagnostic level where it becomes Safe Fault. Available options are \"none\" , \"warn\" , \"error\" . required_modules.autonomous_driving.DIAGNOSTIC_NAME.lf_at string \"warn\" Diagnostic level where it becomes Latent Fault. Available options are \"none\" , \"warn\" , \"error\" . required_modules.autonomous_driving.DIAGNOSTIC_NAME.spf_at string \"error\" Diagnostic level where it becomes Single Point Fault. Available options are \"none\" , \"warn\" , \"error\" . required_modules.autonomous_driving.DIAGNOSTIC_NAME.auto_recovery string \"true\" Determines whether the system will automatically recover when it recovers from an error. required_modules.remote_control.DIAGNOSTIC_NAME.sf_at string \"none\" Diagnostic level where it becomes Safe Fault. Available options are \"none\" , \"warn\" , \"error\" . required_modules.remote_control.DIAGNOSTIC_NAME.lf_at string \"warn\" Diagnostic level where it becomes Latent Fault. Available options are \"none\" , \"warn\" , \"error\" . required_modules.remote_control.DIAGNOSTIC_NAME.spf_at string \"error\" Diagnostic level where it becomes Single Point Fault. Available options are \"none\" , \"warn\" , \"error\" . required_modules.remote_control.DIAGNOSTIC_NAME.auto_recovery string \"true\" Determines whether the system will automatically recover when it recovers from an error.","title":"YAML format for autoware_error_monitor"},{"location":"system/autoware_error_monitor/#assumptions-known-limits","text":"TBD.","title":"Assumptions / Known limits"},{"location":"system/autoware_state_monitor/","text":"autoware_state_monitor # Purpose # This node manages AutowareState transitions. Inner-workings / Algorithms # Inputs / Outputs # Input # Name Type Description /planning/mission_planning/route autoware_auto_planning_msgs::msg::HADMapRoute Subscribe route /localization/kinematic_state nav_msgs::msg::Odometry Used to decide whether vehicle is stopped or not /vehicle/state_report autoware_auto_vehicle_msgs::msg::ControlModeReport Used to check vehicle mode: autonomous or manual. Output # Name Type Description /autoware/engage autoware_auto_vehicle_msgs::msg::Engage publish disengage flag on AutowareState transition /autoware/state autoware_auto_system_msgs::msg::AutowareState publish AutowareState Parameters # Node Parameters # Name Type Default Value Explanation update_rate int 10 Timer callback period. Core Parameters # Name Type Default Value Explanation th_arrived_distance_m double 1.0 threshold distance to check if vehicle has arrived at the route's endpoint th_stopped_time_sec double 1.0 threshold time to check if vehicle is stopped th_stopped_velocity_mps double 0.01 threshold velocity to check if vehicle is stopped disengage_on_route bool true send disengage flag or not when the route is subscribed disengage_on_goal bool true send disengage flag or not when the vehicle is arrived goal Assumptions / Known limits # TBD.","title":"autoware_state_monitor"},{"location":"system/autoware_state_monitor/#autoware_state_monitor","text":"","title":"autoware_state_monitor"},{"location":"system/autoware_state_monitor/#purpose","text":"This node manages AutowareState transitions.","title":"Purpose"},{"location":"system/autoware_state_monitor/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"system/autoware_state_monitor/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"system/autoware_state_monitor/#input","text":"Name Type Description /planning/mission_planning/route autoware_auto_planning_msgs::msg::HADMapRoute Subscribe route /localization/kinematic_state nav_msgs::msg::Odometry Used to decide whether vehicle is stopped or not /vehicle/state_report autoware_auto_vehicle_msgs::msg::ControlModeReport Used to check vehicle mode: autonomous or manual.","title":"Input"},{"location":"system/autoware_state_monitor/#output","text":"Name Type Description /autoware/engage autoware_auto_vehicle_msgs::msg::Engage publish disengage flag on AutowareState transition /autoware/state autoware_auto_system_msgs::msg::AutowareState publish AutowareState","title":"Output"},{"location":"system/autoware_state_monitor/#parameters","text":"","title":"Parameters"},{"location":"system/autoware_state_monitor/#node-parameters","text":"Name Type Default Value Explanation update_rate int 10 Timer callback period.","title":"Node Parameters"},{"location":"system/autoware_state_monitor/#core-parameters","text":"Name Type Default Value Explanation th_arrived_distance_m double 1.0 threshold distance to check if vehicle has arrived at the route's endpoint th_stopped_time_sec double 1.0 threshold time to check if vehicle is stopped th_stopped_velocity_mps double 0.01 threshold velocity to check if vehicle is stopped disengage_on_route bool true send disengage flag or not when the route is subscribed disengage_on_goal bool true send disengage flag or not when the vehicle is arrived goal","title":"Core Parameters"},{"location":"system/autoware_state_monitor/#assumptions-known-limits","text":"TBD.","title":"Assumptions / Known limits"},{"location":"system/autoware_version/","text":"autoware_version # This package provides a command line tool to know the architecture version. This feature is temporary and will be removed when the interface is unified in the future. How to use # ros2 run autoware_version print arch { full,type,version }","title":"autoware_version"},{"location":"system/autoware_version/#autoware_version","text":"This package provides a command line tool to know the architecture version. This feature is temporary and will be removed when the interface is unified in the future.","title":"autoware_version"},{"location":"system/autoware_version/#how-to-use","text":"ros2 run autoware_version print arch { full,type,version }","title":"How to use"},{"location":"system/dummy_diag_publisher/","text":"dummy_diag_publisher # Purpose # This package outputs a dummy diagnostic data for debugging and developing. Inputs / Outputs # Outputs # Name Type Description /diagnostics diagnostic_msgs::msgs::DiagnosticArray Diagnostics outputs Parameters # Node Parameters # Name Type Default Value Explanation update_rate int 10 Timer callback period [Hz] diag_name string diag_name Diag_name set by dummy diag publisher is_active bool true Force update or not Assumptions / Known limits # TBD. Usage # ros2 launch dummy_diag_publisher dummy_diag_publisher.launch.xml","title":"dummy_diag_publisher"},{"location":"system/dummy_diag_publisher/#dummy_diag_publisher","text":"","title":"dummy_diag_publisher"},{"location":"system/dummy_diag_publisher/#purpose","text":"This package outputs a dummy diagnostic data for debugging and developing.","title":"Purpose"},{"location":"system/dummy_diag_publisher/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"system/dummy_diag_publisher/#outputs","text":"Name Type Description /diagnostics diagnostic_msgs::msgs::DiagnosticArray Diagnostics outputs","title":"Outputs"},{"location":"system/dummy_diag_publisher/#parameters","text":"","title":"Parameters"},{"location":"system/dummy_diag_publisher/#node-parameters","text":"Name Type Default Value Explanation update_rate int 10 Timer callback period [Hz] diag_name string diag_name Diag_name set by dummy diag publisher is_active bool true Force update or not","title":"Node Parameters"},{"location":"system/dummy_diag_publisher/#assumptions-known-limits","text":"TBD.","title":"Assumptions / Known limits"},{"location":"system/dummy_diag_publisher/#usage","text":"ros2 launch dummy_diag_publisher dummy_diag_publisher.launch.xml","title":"Usage"},{"location":"system/dummy_infrastructure/","text":"dummy_infrastructure # This is a debug node for infrastructure communication. Usage # ros2 launch dummy_infrastructure dummy_infrastructure.launch.xml ros2 run rqt_reconfigure rqt_reconfigure Inputs / Outputs # Inputs # Name Type Description ~/input/command_array autoware_v2x_msgs::msg::InfrastructureCommandArray Infrastructure command Outputs # Name Type Description ~/output/state_array autoware_v2x_msgs::msg::VirtualTrafficLightStateArray Virtual traffic light array Parameters # Node Parameters # Name Type Default Value Explanation update_rate int 10 Timer callback period [Hz] use_first_command bool true Consider instrument id or not instrument_id string `` Used as command id approval bool false set approval filed to ros param is_finalized bool false Stop at stop_line if finalization isn't completed Assumptions / Known limits # TBD.","title":"dummy_infrastructure"},{"location":"system/dummy_infrastructure/#dummy_infrastructure","text":"This is a debug node for infrastructure communication.","title":"dummy_infrastructure"},{"location":"system/dummy_infrastructure/#usage","text":"ros2 launch dummy_infrastructure dummy_infrastructure.launch.xml ros2 run rqt_reconfigure rqt_reconfigure","title":"Usage"},{"location":"system/dummy_infrastructure/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"system/dummy_infrastructure/#inputs","text":"Name Type Description ~/input/command_array autoware_v2x_msgs::msg::InfrastructureCommandArray Infrastructure command","title":"Inputs"},{"location":"system/dummy_infrastructure/#outputs","text":"Name Type Description ~/output/state_array autoware_v2x_msgs::msg::VirtualTrafficLightStateArray Virtual traffic light array","title":"Outputs"},{"location":"system/dummy_infrastructure/#parameters","text":"","title":"Parameters"},{"location":"system/dummy_infrastructure/#node-parameters","text":"Name Type Default Value Explanation update_rate int 10 Timer callback period [Hz] use_first_command bool true Consider instrument id or not instrument_id string `` Used as command id approval bool false set approval filed to ros param is_finalized bool false Stop at stop_line if finalization isn't completed","title":"Node Parameters"},{"location":"system/dummy_infrastructure/#assumptions-known-limits","text":"TBD.","title":"Assumptions / Known limits"},{"location":"system/emergency_handler/","text":"emergency_handler # Purpose # Emergency Handler is a node to select proper MRM from from system failure state contained in HazardStatus. Inner-workings / Algorithms # State Transitions # Inputs / Outputs # Input # Name Type Description /system/emergency/hazard_status autoware_auto_system_msgs::msg::HazardStatusStamped Used to select proper MRM from system failure state contained in HazardStatus /control/vehicle_cmd autoware_auto_control_msgs::msg::AckermannControlCommand Used as reference when generate Emergency Control Command /localization/kinematic_state nav_msgs::msg::Odometry Used to decide whether vehicle is stopped or not /vehicle/status/control_mode autoware_auto_vehicle_msgs::msg::ControlModeReport Used to check vehicle mode: autonomous or manual. Output # Name Type Description /system/emergency/control_cmd autoware_auto_control_msgs::msg::AckermannControlCommand Required to execute proper MRM /system/emergency/shift_cmd autoware_auto_vehicle_msgs::msg::GearCommand Required to execute proper MRM (send gear cmd) /system/emergency/hazard_cmd autoware_auto_vehicle_msgs::msg::HazardLightsCommand Required to execute proper MRM (send turn signal cmd) /system/emergency/emergency_state autoware_auto_system_msgs::msg::EmergencyStateStamped Used to inform the emergency situation of the vehicle Parameters # Node Parameters # Name Type Default Value Explanation update_rate int 10 Timer callback period. Core Parameters # Name Type Default Value Explanation timeout_hazard_status double 0.5 If the input hazard_status topic cannot be received for more than timeout_hazard_status , vehicle will make an emergency stop. use_parking_after_stopped bool false If this parameter is true, it will publish PARKING shift command. turning_hazard_on.emergency bool true If this parameter is true, hazard lamps will be turned on during emergency state. Assumptions / Known limits # TBD.","title":"emergency_handler"},{"location":"system/emergency_handler/#emergency_handler","text":"","title":"emergency_handler"},{"location":"system/emergency_handler/#purpose","text":"Emergency Handler is a node to select proper MRM from from system failure state contained in HazardStatus.","title":"Purpose"},{"location":"system/emergency_handler/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"system/emergency_handler/#state-transitions","text":"","title":"State Transitions"},{"location":"system/emergency_handler/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"system/emergency_handler/#input","text":"Name Type Description /system/emergency/hazard_status autoware_auto_system_msgs::msg::HazardStatusStamped Used to select proper MRM from system failure state contained in HazardStatus /control/vehicle_cmd autoware_auto_control_msgs::msg::AckermannControlCommand Used as reference when generate Emergency Control Command /localization/kinematic_state nav_msgs::msg::Odometry Used to decide whether vehicle is stopped or not /vehicle/status/control_mode autoware_auto_vehicle_msgs::msg::ControlModeReport Used to check vehicle mode: autonomous or manual.","title":"Input"},{"location":"system/emergency_handler/#output","text":"Name Type Description /system/emergency/control_cmd autoware_auto_control_msgs::msg::AckermannControlCommand Required to execute proper MRM /system/emergency/shift_cmd autoware_auto_vehicle_msgs::msg::GearCommand Required to execute proper MRM (send gear cmd) /system/emergency/hazard_cmd autoware_auto_vehicle_msgs::msg::HazardLightsCommand Required to execute proper MRM (send turn signal cmd) /system/emergency/emergency_state autoware_auto_system_msgs::msg::EmergencyStateStamped Used to inform the emergency situation of the vehicle","title":"Output"},{"location":"system/emergency_handler/#parameters","text":"","title":"Parameters"},{"location":"system/emergency_handler/#node-parameters","text":"Name Type Default Value Explanation update_rate int 10 Timer callback period.","title":"Node Parameters"},{"location":"system/emergency_handler/#core-parameters","text":"Name Type Default Value Explanation timeout_hazard_status double 0.5 If the input hazard_status topic cannot be received for more than timeout_hazard_status , vehicle will make an emergency stop. use_parking_after_stopped bool false If this parameter is true, it will publish PARKING shift command. turning_hazard_on.emergency bool true If this parameter is true, hazard lamps will be turned on during emergency state.","title":"Core Parameters"},{"location":"system/emergency_handler/#assumptions-known-limits","text":"TBD.","title":"Assumptions / Known limits"},{"location":"system/system_monitor/","text":"System Monitor for Autoware # Further improvement of system monitor functionality for Autoware. Description # This package provides the following nodes for monitoring system: CPU Monitor HDD Monitor Memory Monitor Network Monitor NTP Monitor Process Monitor GPU Monitor Supported architecture # x86_64 arm64v8/aarch64 Operation confirmed platform # PC system intel core i7 NVIDIA Jetson AGX Xavier Raspberry Pi4 Model B How to use # Use colcon build and launch in the same way as other packages. colcon build source install/setup.bash ros2 launch system_monitor system_monitor.launch.xml CPU and GPU monitoring method differs depending on platform. CMake automatically chooses source to be built according to build environment. If you build this package on intel platform, CPU monitor and GPU monitor which run on intel platform are built. ROS topics published by system monitor # Every topic is published in 1 minute interval. CPU Monitor HDD Monitor Mem Monitor Net Monitor NTP Monitor Process Monitor GPU Monitor [Usage] \u2713\uff1aSupported, -\uff1aNot supported Node Message Intel arm64(tegra) arm64(raspi) Notes CPU Monitor CPU Temperature \u2713 \u2713 \u2713 CPU Usage \u2713 \u2713 \u2713 CPU Load Average \u2713 \u2713 \u2713 CPU Thermal Throttling \u2713 - \u2713 CPU Frequency \u2713 \u2713 \u2713 Notification of frequency only, normally error not generated. HDD Monitor HDD Temperature \u2713 \u2713 \u2713 HDD Usage \u2713 \u2713 \u2713 Memory Monitor Memory Usage \u2713 \u2713 \u2713 Net Monitor Network Usage \u2713 \u2713 \u2713 NTP Monitor NTP Offset \u2713 \u2713 \u2713 Process Monitor Tasks Summary \u2713 \u2713 \u2713 High-load Proc[0-9] \u2713 \u2713 \u2713 High-mem Proc[0-9] \u2713 \u2713 \u2713 GPU Monitor GPU Temperature \u2713 \u2713 - GPU Usage \u2713 \u2713 - GPU Memory Usage \u2713 - - GPU Thermal Throttling \u2713 - - GPU Frequency - \u2713 - ROS parameters # See ROS parameters . Notes # CPU monitor for intel platform # Thermal throttling event can be monitored by reading contents of MSR(Model Specific Register), and accessing MSR is only allowed for root by default, so this package provides the following approach to minimize security risks as much as possible: Provide a small program named 'msr_reader' which accesses MSR and sends thermal throttling status to CPU monitor by using socket programming. Run 'msr_reader' as a specific user instead of root. CPU monitor is able to know the status as an unprivileged user since thermal throttling status is sent by socket communication. Instructions before starting # Create a user to run 'msr_reader'. sudo adduser <username> Load kernel module 'msr' into your target system. The path '/dev/cpu/CPUNUM/msr' appears. sudo modprobe msr Allow user to access MSR with read-only privilege using the Access Control List (ACL). sudo setfacl -m u:<username>:r /dev/cpu/*/msr Assign capability to 'msr_reader' since msr kernel module requires rawio capability. sudo setcap cap_sys_rawio = ep install/system_monitor/lib/system_monitor/msr_reader Run 'msr_reader' as the user you created, and run system_monitor as a generic user. su <username> install/system_monitor/lib/system_monitor/msr_reader See also # msr_reader HDD Monitor # Generally, S.M.A.R.T. information is used to monitor HDD temperature, and normally accessing disk device node is allowed for root user or disk group. As with the CPU monitor, this package provides an approach to minimize security risks as much as possible: Provide a small program named 'hdd_reader' which accesses S.M.A.R.T. information and sends HDD temperature to HDD monitor by using socket programming. Run 'hdd_reader' as a specific user. HDD monitor is able to know HDD temperature as an unprivileged user since HDD temperature is sent by socket communication. Instructions before starting # Create a user to run 'hdd_reader'. sudo adduser <username> Add user to the disk group. sudo usermod -a -G disk <username> Assign capabilities to 'hdd_reader' since SCSI kernel module requires rawio capability to send ATA PASS-THROUGH (12) command and NVMe kernel module requires admin capability to send Admin Command. sudo setcap 'cap_sys_rawio=ep cap_sys_admin=ep' install/system_monitor/lib/system_monitor/hdd_reader Run 'hdd_reader' as the user you created, and run system_monitor as a generic user. su <username> install/system_monitor/lib/system_monitor/hdd_reader See also # hdd_reader GPU Monitor for intel platform # Currently GPU monitor for intel platform only supports NVIDIA GPU whose information can be accessed by NVML API. Also you need to install CUDA libraries. For installation instructions for CUDA 10.0, see NVIDIA CUDA Installation Guide for Linux . UML diagrams # See Class diagrams . See Sequence diagrams .","title":"System Monitor for Autoware"},{"location":"system/system_monitor/#system-monitor-for-autoware","text":"Further improvement of system monitor functionality for Autoware.","title":"System Monitor for Autoware"},{"location":"system/system_monitor/#description","text":"This package provides the following nodes for monitoring system: CPU Monitor HDD Monitor Memory Monitor Network Monitor NTP Monitor Process Monitor GPU Monitor","title":"Description"},{"location":"system/system_monitor/#supported-architecture","text":"x86_64 arm64v8/aarch64","title":"Supported architecture"},{"location":"system/system_monitor/#operation-confirmed-platform","text":"PC system intel core i7 NVIDIA Jetson AGX Xavier Raspberry Pi4 Model B","title":"Operation confirmed platform"},{"location":"system/system_monitor/#how-to-use","text":"Use colcon build and launch in the same way as other packages. colcon build source install/setup.bash ros2 launch system_monitor system_monitor.launch.xml CPU and GPU monitoring method differs depending on platform. CMake automatically chooses source to be built according to build environment. If you build this package on intel platform, CPU monitor and GPU monitor which run on intel platform are built.","title":"How to use"},{"location":"system/system_monitor/#ros-topics-published-by-system-monitor","text":"Every topic is published in 1 minute interval. CPU Monitor HDD Monitor Mem Monitor Net Monitor NTP Monitor Process Monitor GPU Monitor [Usage] \u2713\uff1aSupported, -\uff1aNot supported Node Message Intel arm64(tegra) arm64(raspi) Notes CPU Monitor CPU Temperature \u2713 \u2713 \u2713 CPU Usage \u2713 \u2713 \u2713 CPU Load Average \u2713 \u2713 \u2713 CPU Thermal Throttling \u2713 - \u2713 CPU Frequency \u2713 \u2713 \u2713 Notification of frequency only, normally error not generated. HDD Monitor HDD Temperature \u2713 \u2713 \u2713 HDD Usage \u2713 \u2713 \u2713 Memory Monitor Memory Usage \u2713 \u2713 \u2713 Net Monitor Network Usage \u2713 \u2713 \u2713 NTP Monitor NTP Offset \u2713 \u2713 \u2713 Process Monitor Tasks Summary \u2713 \u2713 \u2713 High-load Proc[0-9] \u2713 \u2713 \u2713 High-mem Proc[0-9] \u2713 \u2713 \u2713 GPU Monitor GPU Temperature \u2713 \u2713 - GPU Usage \u2713 \u2713 - GPU Memory Usage \u2713 - - GPU Thermal Throttling \u2713 - - GPU Frequency - \u2713 -","title":"ROS topics published by system monitor"},{"location":"system/system_monitor/#ros-parameters","text":"See ROS parameters .","title":"ROS parameters"},{"location":"system/system_monitor/#notes","text":"","title":"Notes"},{"location":"system/system_monitor/#cpu-monitor-for-intel-platform","text":"Thermal throttling event can be monitored by reading contents of MSR(Model Specific Register), and accessing MSR is only allowed for root by default, so this package provides the following approach to minimize security risks as much as possible: Provide a small program named 'msr_reader' which accesses MSR and sends thermal throttling status to CPU monitor by using socket programming. Run 'msr_reader' as a specific user instead of root. CPU monitor is able to know the status as an unprivileged user since thermal throttling status is sent by socket communication.","title":"CPU monitor for intel platform"},{"location":"system/system_monitor/#instructions-before-starting","text":"Create a user to run 'msr_reader'. sudo adduser <username> Load kernel module 'msr' into your target system. The path '/dev/cpu/CPUNUM/msr' appears. sudo modprobe msr Allow user to access MSR with read-only privilege using the Access Control List (ACL). sudo setfacl -m u:<username>:r /dev/cpu/*/msr Assign capability to 'msr_reader' since msr kernel module requires rawio capability. sudo setcap cap_sys_rawio = ep install/system_monitor/lib/system_monitor/msr_reader Run 'msr_reader' as the user you created, and run system_monitor as a generic user. su <username> install/system_monitor/lib/system_monitor/msr_reader","title":"Instructions before starting"},{"location":"system/system_monitor/#see-also","text":"msr_reader","title":"See also"},{"location":"system/system_monitor/#hdd-monitor","text":"Generally, S.M.A.R.T. information is used to monitor HDD temperature, and normally accessing disk device node is allowed for root user or disk group. As with the CPU monitor, this package provides an approach to minimize security risks as much as possible: Provide a small program named 'hdd_reader' which accesses S.M.A.R.T. information and sends HDD temperature to HDD monitor by using socket programming. Run 'hdd_reader' as a specific user. HDD monitor is able to know HDD temperature as an unprivileged user since HDD temperature is sent by socket communication.","title":"HDD Monitor"},{"location":"system/system_monitor/#instructions-before-starting_1","text":"Create a user to run 'hdd_reader'. sudo adduser <username> Add user to the disk group. sudo usermod -a -G disk <username> Assign capabilities to 'hdd_reader' since SCSI kernel module requires rawio capability to send ATA PASS-THROUGH (12) command and NVMe kernel module requires admin capability to send Admin Command. sudo setcap 'cap_sys_rawio=ep cap_sys_admin=ep' install/system_monitor/lib/system_monitor/hdd_reader Run 'hdd_reader' as the user you created, and run system_monitor as a generic user. su <username> install/system_monitor/lib/system_monitor/hdd_reader","title":"Instructions before starting"},{"location":"system/system_monitor/#see-also_1","text":"hdd_reader","title":"See also"},{"location":"system/system_monitor/#gpu-monitor-for-intel-platform","text":"Currently GPU monitor for intel platform only supports NVIDIA GPU whose information can be accessed by NVML API. Also you need to install CUDA libraries. For installation instructions for CUDA 10.0, see NVIDIA CUDA Installation Guide for Linux .","title":"GPU Monitor for intel platform"},{"location":"system/system_monitor/#uml-diagrams","text":"See Class diagrams . See Sequence diagrams .","title":"UML diagrams"},{"location":"system/system_monitor/docs/class_diagrams/","text":"Class diagrams # CPU Monitor # HDD Monitor # Memory Monitor # Net Monitor # NTP Monitor # Process Monitor # GPU Monitor #","title":"Class diagrams"},{"location":"system/system_monitor/docs/class_diagrams/#class-diagrams","text":"","title":"Class diagrams"},{"location":"system/system_monitor/docs/class_diagrams/#cpu-monitor","text":"","title":"CPU Monitor"},{"location":"system/system_monitor/docs/class_diagrams/#hdd-monitor","text":"","title":"HDD Monitor"},{"location":"system/system_monitor/docs/class_diagrams/#memory-monitor","text":"","title":"Memory Monitor"},{"location":"system/system_monitor/docs/class_diagrams/#net-monitor","text":"","title":"Net Monitor"},{"location":"system/system_monitor/docs/class_diagrams/#ntp-monitor","text":"","title":"NTP Monitor"},{"location":"system/system_monitor/docs/class_diagrams/#process-monitor","text":"","title":"Process Monitor"},{"location":"system/system_monitor/docs/class_diagrams/#gpu-monitor","text":"","title":"GPU Monitor"},{"location":"system/system_monitor/docs/hdd_reader/","text":"hdd_reader # Name # hdd_reader - Read S.M.A.R.T. information for monitoring HDD temperature Synopsis # hdd_reader [OPTION] Description # Read S.M.A.R.T. information for monitoring HDD temperature. This runs as a daemon process and listens to a TCP/IP port (7635 by default). Options: -h, --help Display help -p, --port # Port number to listen to Exit status: Returns 0 if OK; non-zero otherwise. Notes # The 'hdd_reader' accesses minimal data enough to get Model number, Serial number, and HDD temperature. This is an approach to limit its functionality, however, the functionality can be expanded for further improvements and considerations in the future. [ATA] # Purpose Name Length Model number, Serial number IDENTIFY DEVICE data 256 words(512 bytes) HDD temperature SMART READ DATA 256 words(512 bytes) For details please see the documents below. ATA Command Set - 4 (ACS-4) ATA/ATAPI Command Set - 3 (ACS-3) SMART Attribute Overview SMART Attribute Annex [NVMe] # Purpose Name Length Model number, Serial number Identify Controller data structure 4096 bytes HDD temperature SMART / Health Information 1 Dword(4 bytes) For details please see the documents below. NVM Express 1.2b Operation confirmed drives # SAMSUNG MZVLB1T0HALR (SSD) Western Digital My Passport (Portable HDD)","title":"hdd_reader"},{"location":"system/system_monitor/docs/hdd_reader/#hdd_reader","text":"","title":"hdd_reader"},{"location":"system/system_monitor/docs/hdd_reader/#name","text":"hdd_reader - Read S.M.A.R.T. information for monitoring HDD temperature","title":"Name"},{"location":"system/system_monitor/docs/hdd_reader/#synopsis","text":"hdd_reader [OPTION]","title":"Synopsis"},{"location":"system/system_monitor/docs/hdd_reader/#description","text":"Read S.M.A.R.T. information for monitoring HDD temperature. This runs as a daemon process and listens to a TCP/IP port (7635 by default). Options: -h, --help Display help -p, --port # Port number to listen to Exit status: Returns 0 if OK; non-zero otherwise.","title":"Description"},{"location":"system/system_monitor/docs/hdd_reader/#notes","text":"The 'hdd_reader' accesses minimal data enough to get Model number, Serial number, and HDD temperature. This is an approach to limit its functionality, however, the functionality can be expanded for further improvements and considerations in the future.","title":"Notes"},{"location":"system/system_monitor/docs/hdd_reader/#ata","text":"Purpose Name Length Model number, Serial number IDENTIFY DEVICE data 256 words(512 bytes) HDD temperature SMART READ DATA 256 words(512 bytes) For details please see the documents below. ATA Command Set - 4 (ACS-4) ATA/ATAPI Command Set - 3 (ACS-3) SMART Attribute Overview SMART Attribute Annex","title":"[ATA]"},{"location":"system/system_monitor/docs/hdd_reader/#nvme","text":"Purpose Name Length Model number, Serial number Identify Controller data structure 4096 bytes HDD temperature SMART / Health Information 1 Dword(4 bytes) For details please see the documents below. NVM Express 1.2b","title":"[NVMe]"},{"location":"system/system_monitor/docs/hdd_reader/#operation-confirmed-drives","text":"SAMSUNG MZVLB1T0HALR (SSD) Western Digital My Passport (Portable HDD)","title":"Operation confirmed drives"},{"location":"system/system_monitor/docs/msr_reader/","text":"msr_reader # Name # msr_reader - Read MSR register for monitoring thermal throttling event Synopsis # msr_reader [OPTION] Description # Read MSR register for monitoring thermal throttling event. This runs as a daemon process and listens to a TCP/IP port (7634 by default). Options: -h, --help Display help -p, --port # Port number to listen to Exit status: Returns 0 if OK; non-zero otherwise. Notes # The 'msr_reader' accesses minimal data enough to get thermal throttling event. This is an approach to limit its functionality, however, the functionality can be expanded for further improvements and considerations in the future. Register Address Name Length 1B1H IA32_PACKAGE_THERM_STATUS 64bit For details please see the documents below. Intel\u00ae 64 and IA-32 ArchitecturesSoftware Developer\u2019s Manual Operation confirmed platform # PC system intel core i7","title":"msr_reader"},{"location":"system/system_monitor/docs/msr_reader/#msr_reader","text":"","title":"msr_reader"},{"location":"system/system_monitor/docs/msr_reader/#name","text":"msr_reader - Read MSR register for monitoring thermal throttling event","title":"Name"},{"location":"system/system_monitor/docs/msr_reader/#synopsis","text":"msr_reader [OPTION]","title":"Synopsis"},{"location":"system/system_monitor/docs/msr_reader/#description","text":"Read MSR register for monitoring thermal throttling event. This runs as a daemon process and listens to a TCP/IP port (7634 by default). Options: -h, --help Display help -p, --port # Port number to listen to Exit status: Returns 0 if OK; non-zero otherwise.","title":"Description"},{"location":"system/system_monitor/docs/msr_reader/#notes","text":"The 'msr_reader' accesses minimal data enough to get thermal throttling event. This is an approach to limit its functionality, however, the functionality can be expanded for further improvements and considerations in the future. Register Address Name Length 1B1H IA32_PACKAGE_THERM_STATUS 64bit For details please see the documents below. Intel\u00ae 64 and IA-32 ArchitecturesSoftware Developer\u2019s Manual","title":"Notes"},{"location":"system/system_monitor/docs/msr_reader/#operation-confirmed-platform","text":"PC system intel core i7","title":"Operation confirmed platform"},{"location":"system/system_monitor/docs/ros_parameters/","text":"ROS parameters # CPU Monitor # cpu_monitor: Name Type Unit Default Notes temp_warn float DegC 90.0 Generates warning when CPU temperature reaches a specified value or higher. temp_error float DegC 95.0 Generates error when CPU temperature reaches a specified value or higher. usage_warn float %(1e-2) 0.90 Generates warning when CPU usage reaches a specified value or higher. usage_error float %(1e-2) 1.00 Generates error when CPU usage reaches a specified value or higher. load1_warn float %(1e-2) 0.90 Generates warning when load average 1min reaches a specified value or higher. load5_warn float %(1e-2) 0.80 Generates warning when load average 5min reaches a specified value or higher. msr_reader_port int n/a 7634 Port number to connect to msr_reader. HDD Monitor # hdd_monitor: disks: Name Type Unit Default Notes name string n/a none The disk name to monitor temperature. (e.g. /dev/sda) temp_error float DegC 55.0 Generates warning when HDD temperature reaches a specified value or higher. temp_error float DegC 70.0 Generates error when HDD temperature reaches a specified value or higher. hdd_monitor: Name Type Unit Default Notes hdd_reader_port int n/a 7635 Port number to connect to hdd_reader. usage_warn float %(1e-2) 0.95 Generates warning when disk usage reaches a specified value or higher. usage_error float %(1e-2) 0.99 Generates error when disk usage reaches a specified value or higher. Memory Monitor # mem_monitor: Name Type Unit Default Notes usage_warn float %(1e-2) 0.95 Generates warning when physical memory usage reaches a specified value or higher. usage_error float %(1e-2) 0.99 Generates error when physical memory usage reaches a specified value or higher. Net Monitor # net_monitor: Name Type Unit Default Notes devices list[string] n/a none The name of network interface to monitor. (e.g. eth0, * for all network interfaces) usage_warn float %(1e-2) 0.95 Generates warning when network usage reaches a specified value or higher. NTP Monitor # ntp_monitor: Name Type Unit Default Notes server string n/a ntp.ubuntu.com The name of NTP server to synchronize date and time. (e.g. ntp.nict.jp for Japan) offset_warn float sec 0.1 Generates warning when NTP offset reaches a specified value or higher. (default is 100ms) offset_error float sec 5.0 Generates warning when NTP offset reaches a specified value or higher. (default is 5sec) Process Monitor # process_monitor: Name Type Unit Default Notes num_of_procs int n/a 5 The number of processes to generate High-load Proc[0-9] and High-mem Proc[0-9]. GPU Monitor # gpu_monitor: Name Type Unit Default Notes temp_warn float DegC 90.0 Generates warning when GPU temperature reaches a specified value or higher. temp_error float DegC 95.0 Generates error when GPU temperature reaches a specified value or higher. gpu_usage_warn float %(1e-2) 0.90 Generates warning when GPU usage reaches a specified value or higher. gpu_usage_error float %(1e-2) 1.00 Generates error when GPU usage reaches a specified value or higher. memory_usage_warn float %(1e-2) 0.90 Generates warning when GPU memory usage reaches a specified value or higher. memory_usage_error float %(1e-2) 1.00 Generates error when GPU memory usage reaches a specified value or higher.","title":"ROS parameters"},{"location":"system/system_monitor/docs/ros_parameters/#ros-parameters","text":"","title":"ROS parameters"},{"location":"system/system_monitor/docs/ros_parameters/#cpu-monitor","text":"cpu_monitor: Name Type Unit Default Notes temp_warn float DegC 90.0 Generates warning when CPU temperature reaches a specified value or higher. temp_error float DegC 95.0 Generates error when CPU temperature reaches a specified value or higher. usage_warn float %(1e-2) 0.90 Generates warning when CPU usage reaches a specified value or higher. usage_error float %(1e-2) 1.00 Generates error when CPU usage reaches a specified value or higher. load1_warn float %(1e-2) 0.90 Generates warning when load average 1min reaches a specified value or higher. load5_warn float %(1e-2) 0.80 Generates warning when load average 5min reaches a specified value or higher. msr_reader_port int n/a 7634 Port number to connect to msr_reader.","title":"CPU Monitor"},{"location":"system/system_monitor/docs/ros_parameters/#hdd-monitor","text":"hdd_monitor: disks: Name Type Unit Default Notes name string n/a none The disk name to monitor temperature. (e.g. /dev/sda) temp_error float DegC 55.0 Generates warning when HDD temperature reaches a specified value or higher. temp_error float DegC 70.0 Generates error when HDD temperature reaches a specified value or higher. hdd_monitor: Name Type Unit Default Notes hdd_reader_port int n/a 7635 Port number to connect to hdd_reader. usage_warn float %(1e-2) 0.95 Generates warning when disk usage reaches a specified value or higher. usage_error float %(1e-2) 0.99 Generates error when disk usage reaches a specified value or higher.","title":"HDD Monitor"},{"location":"system/system_monitor/docs/ros_parameters/#memory-monitor","text":"mem_monitor: Name Type Unit Default Notes usage_warn float %(1e-2) 0.95 Generates warning when physical memory usage reaches a specified value or higher. usage_error float %(1e-2) 0.99 Generates error when physical memory usage reaches a specified value or higher.","title":"Memory Monitor"},{"location":"system/system_monitor/docs/ros_parameters/#net-monitor","text":"net_monitor: Name Type Unit Default Notes devices list[string] n/a none The name of network interface to monitor. (e.g. eth0, * for all network interfaces) usage_warn float %(1e-2) 0.95 Generates warning when network usage reaches a specified value or higher.","title":"Net Monitor"},{"location":"system/system_monitor/docs/ros_parameters/#ntp-monitor","text":"ntp_monitor: Name Type Unit Default Notes server string n/a ntp.ubuntu.com The name of NTP server to synchronize date and time. (e.g. ntp.nict.jp for Japan) offset_warn float sec 0.1 Generates warning when NTP offset reaches a specified value or higher. (default is 100ms) offset_error float sec 5.0 Generates warning when NTP offset reaches a specified value or higher. (default is 5sec)","title":"NTP Monitor"},{"location":"system/system_monitor/docs/ros_parameters/#process-monitor","text":"process_monitor: Name Type Unit Default Notes num_of_procs int n/a 5 The number of processes to generate High-load Proc[0-9] and High-mem Proc[0-9].","title":"Process Monitor"},{"location":"system/system_monitor/docs/ros_parameters/#gpu-monitor","text":"gpu_monitor: Name Type Unit Default Notes temp_warn float DegC 90.0 Generates warning when GPU temperature reaches a specified value or higher. temp_error float DegC 95.0 Generates error when GPU temperature reaches a specified value or higher. gpu_usage_warn float %(1e-2) 0.90 Generates warning when GPU usage reaches a specified value or higher. gpu_usage_error float %(1e-2) 1.00 Generates error when GPU usage reaches a specified value or higher. memory_usage_warn float %(1e-2) 0.90 Generates warning when GPU memory usage reaches a specified value or higher. memory_usage_error float %(1e-2) 1.00 Generates error when GPU memory usage reaches a specified value or higher.","title":"GPU Monitor"},{"location":"system/system_monitor/docs/seq_diagrams/","text":"Sequence diagrams # CPU Monitor # HDD Monitor # Memory Monitor # Net Monitor # NTP Monitor # Process Monitor # GPU Monitor #","title":"Sequence diagrams"},{"location":"system/system_monitor/docs/seq_diagrams/#sequence-diagrams","text":"","title":"Sequence diagrams"},{"location":"system/system_monitor/docs/seq_diagrams/#cpu-monitor","text":"","title":"CPU Monitor"},{"location":"system/system_monitor/docs/seq_diagrams/#hdd-monitor","text":"","title":"HDD Monitor"},{"location":"system/system_monitor/docs/seq_diagrams/#memory-monitor","text":"","title":"Memory Monitor"},{"location":"system/system_monitor/docs/seq_diagrams/#net-monitor","text":"","title":"Net Monitor"},{"location":"system/system_monitor/docs/seq_diagrams/#ntp-monitor","text":"","title":"NTP Monitor"},{"location":"system/system_monitor/docs/seq_diagrams/#process-monitor","text":"","title":"Process Monitor"},{"location":"system/system_monitor/docs/seq_diagrams/#gpu-monitor","text":"","title":"GPU Monitor"},{"location":"system/system_monitor/docs/topics_cpu_monitor/","text":"ROS topics: CPU Monitor # CPU Temperature # /diagnostics/cpu_monitor: CPU Temperature [summary] level message OK OK WARN warm ERROR hot [values] key (example) value (example) Package id 0, Core [0-9], thermal_zone[0-9] 50.0 DegC *key: thermal_zone[0-9] for ARM architecture. CPU Usage # /diagnostics/cpu_monitor: CPU Usage [summary] level message OK OK WARN high load ERROR very high Lload [values] key value (example) CPU [all,0-9]: status OK / high load / very high load CPU [all,0-9]: usr 2.00% CPU [all,0-9]: nice 0.00% CPU [all,0-9]: sys 1.00% CPU [all,0-9]: idle 97.00% CPU Load Average # /diagnostics/cpu_monitor: CPU Load Average [summary] level message OK OK WARN high load [values] key value (example) 1min 14.50% 5min 14.55% 15min 9.67% CPU Thermal Throttling # Intel and raspi platform only. Tegra platform not supported. /diagnostics/cpu_monitor: CPU Thermal Throttling [summary] level message OK OK ERROR throttling [values for intel platform] key value (example) CPU [0-9]: Pkg Thermal Status OK / throttling [values for raspi platform] key value (example) status All clear / Currently throttled / Soft temperature limit active CPU Frequency # /diagnostics/cpu_monitor: CPU Frequency [summary] level message OK OK [values] key value (example) CPU [0-9]: clock 2879MHz","title":"ROS topics: CPU Monitor"},{"location":"system/system_monitor/docs/topics_cpu_monitor/#ros-topics-cpu-monitor","text":"","title":"ROS topics: CPU Monitor"},{"location":"system/system_monitor/docs/topics_cpu_monitor/#cpu-temperature","text":"/diagnostics/cpu_monitor: CPU Temperature [summary] level message OK OK WARN warm ERROR hot [values] key (example) value (example) Package id 0, Core [0-9], thermal_zone[0-9] 50.0 DegC *key: thermal_zone[0-9] for ARM architecture.","title":"CPU Temperature"},{"location":"system/system_monitor/docs/topics_cpu_monitor/#cpu-usage","text":"/diagnostics/cpu_monitor: CPU Usage [summary] level message OK OK WARN high load ERROR very high Lload [values] key value (example) CPU [all,0-9]: status OK / high load / very high load CPU [all,0-9]: usr 2.00% CPU [all,0-9]: nice 0.00% CPU [all,0-9]: sys 1.00% CPU [all,0-9]: idle 97.00%","title":"CPU Usage"},{"location":"system/system_monitor/docs/topics_cpu_monitor/#cpu-load-average","text":"/diagnostics/cpu_monitor: CPU Load Average [summary] level message OK OK WARN high load [values] key value (example) 1min 14.50% 5min 14.55% 15min 9.67%","title":"CPU Load Average"},{"location":"system/system_monitor/docs/topics_cpu_monitor/#cpu-thermal-throttling","text":"Intel and raspi platform only. Tegra platform not supported. /diagnostics/cpu_monitor: CPU Thermal Throttling [summary] level message OK OK ERROR throttling [values for intel platform] key value (example) CPU [0-9]: Pkg Thermal Status OK / throttling [values for raspi platform] key value (example) status All clear / Currently throttled / Soft temperature limit active","title":"CPU Thermal Throttling"},{"location":"system/system_monitor/docs/topics_cpu_monitor/#cpu-frequency","text":"/diagnostics/cpu_monitor: CPU Frequency [summary] level message OK OK [values] key value (example) CPU [0-9]: clock 2879MHz","title":"CPU Frequency"},{"location":"system/system_monitor/docs/topics_gpu_monitor/","text":"ROS topics: GPU Monitor # Intel and tegra platform only. Raspi platform not supported. GPU Temperature # /diagnostics/gpu_monitor: GPU Temperature [summary] level message OK OK WARN warm ERROR hot [values] key (example) value (example) GeForce GTX 1650, thermal_zone[0-9] 46.0 DegC *key: thermal_zone[0-9] for ARM architecture. GPU Usage # /diagnostics/gpu_monitor: GPU Usage [summary] level message OK OK WARN high load ERROR very high load [values] key value (example) GPU [0-9]: status OK / high load / very high load GPU [0-9]: name GeForce GTX 1650, gpu.[0-9] GPU [0-9]: usage 19.0% *key: gpu.[0-9] for ARM architecture. GPU Memory Usage # Intel platform only. There is no separate gpu memory in tegra. Both cpu and gpu uses cpu memory. /diagnostics/gpu_monitor: GPU Memory Usage [summary] level message OK OK WARN high load ERROR very high load [values] key value (example) GPU [0-9]: status OK / high load / very high load GPU [0-9]: name GeForce GTX 1650 GPU [0-9]: usage 13.0% GPU [0-9]: total 3G GPU [0-9]: used 1G GPU [0-9]: free 2G GPU Thermal Throttling # Intel platform only. Tegra platform not supported. /diagnostics/gpu_monitor: GPU Thermal Throttling [summary] level message OK OK ERROR throttling [values] key value (example) GPU [0-9]: status OK / throttling GPU [0-9]: name GeForce GTX 1650 GPU [0-9]: graphics clock 1020 MHz GPU [0-9]: reasons GpuIdle / SwThermalSlowdown etc. GPU Frequency # Tegra platform only. /diagnostics/gpu_monitor: GPU Frequency [summary] level message OK OK [values] key (example) value (example) GPU 17000000.gv11b: clock 318 MHz","title":"ROS topics: GPU Monitor"},{"location":"system/system_monitor/docs/topics_gpu_monitor/#ros-topics-gpu-monitor","text":"Intel and tegra platform only. Raspi platform not supported.","title":"ROS topics: GPU Monitor"},{"location":"system/system_monitor/docs/topics_gpu_monitor/#gpu-temperature","text":"/diagnostics/gpu_monitor: GPU Temperature [summary] level message OK OK WARN warm ERROR hot [values] key (example) value (example) GeForce GTX 1650, thermal_zone[0-9] 46.0 DegC *key: thermal_zone[0-9] for ARM architecture.","title":"GPU Temperature"},{"location":"system/system_monitor/docs/topics_gpu_monitor/#gpu-usage","text":"/diagnostics/gpu_monitor: GPU Usage [summary] level message OK OK WARN high load ERROR very high load [values] key value (example) GPU [0-9]: status OK / high load / very high load GPU [0-9]: name GeForce GTX 1650, gpu.[0-9] GPU [0-9]: usage 19.0% *key: gpu.[0-9] for ARM architecture.","title":"GPU Usage"},{"location":"system/system_monitor/docs/topics_gpu_monitor/#gpu-memory-usage","text":"Intel platform only. There is no separate gpu memory in tegra. Both cpu and gpu uses cpu memory. /diagnostics/gpu_monitor: GPU Memory Usage [summary] level message OK OK WARN high load ERROR very high load [values] key value (example) GPU [0-9]: status OK / high load / very high load GPU [0-9]: name GeForce GTX 1650 GPU [0-9]: usage 13.0% GPU [0-9]: total 3G GPU [0-9]: used 1G GPU [0-9]: free 2G","title":"GPU Memory Usage"},{"location":"system/system_monitor/docs/topics_gpu_monitor/#gpu-thermal-throttling","text":"Intel platform only. Tegra platform not supported. /diagnostics/gpu_monitor: GPU Thermal Throttling [summary] level message OK OK ERROR throttling [values] key value (example) GPU [0-9]: status OK / throttling GPU [0-9]: name GeForce GTX 1650 GPU [0-9]: graphics clock 1020 MHz GPU [0-9]: reasons GpuIdle / SwThermalSlowdown etc.","title":"GPU Thermal Throttling"},{"location":"system/system_monitor/docs/topics_gpu_monitor/#gpu-frequency","text":"Tegra platform only. /diagnostics/gpu_monitor: GPU Frequency [summary] level message OK OK [values] key (example) value (example) GPU 17000000.gv11b: clock 318 MHz","title":"GPU Frequency"},{"location":"system/system_monitor/docs/topics_hdd_monitor/","text":"ROS topics: CPU Monitor # HDD Temperature # /diagnostics/hdd_monitor: HDD Temperature [summary] level message OK OK WARN hot ERROR critical hot [values] key value (example) HDD [0-9]: status OK / hot / critical hot HDD [0-9]: name /dev/nvme0 HDD [0-9]: model SAMSUNG MZVLB1T0HBLR-000L7 HDD [0-9]: serial S4EMNF0M820682 HDD [0-9]: temperature 37.0 DegC HDD Usage # /diagnostics/hdd_monitor: HDD Usage [summary] level message OK OK WARN low disk space ERROR very low disk space [values] key value (example) HDD [0-9]: status OK / low disk space / very low disk space HDD [0-9]: filesystem /dev/nvme0n1p4 HDD [0-9]: size 264G HDD [0-9]: used 172G HDD [0-9]: avail 749G HDD [0-9]: use 69% HDD [0-9]: mounted on /","title":"ROS topics: CPU Monitor"},{"location":"system/system_monitor/docs/topics_hdd_monitor/#ros-topics-cpu-monitor","text":"","title":"ROS topics: CPU Monitor"},{"location":"system/system_monitor/docs/topics_hdd_monitor/#hdd-temperature","text":"/diagnostics/hdd_monitor: HDD Temperature [summary] level message OK OK WARN hot ERROR critical hot [values] key value (example) HDD [0-9]: status OK / hot / critical hot HDD [0-9]: name /dev/nvme0 HDD [0-9]: model SAMSUNG MZVLB1T0HBLR-000L7 HDD [0-9]: serial S4EMNF0M820682 HDD [0-9]: temperature 37.0 DegC","title":"HDD Temperature"},{"location":"system/system_monitor/docs/topics_hdd_monitor/#hdd-usage","text":"/diagnostics/hdd_monitor: HDD Usage [summary] level message OK OK WARN low disk space ERROR very low disk space [values] key value (example) HDD [0-9]: status OK / low disk space / very low disk space HDD [0-9]: filesystem /dev/nvme0n1p4 HDD [0-9]: size 264G HDD [0-9]: used 172G HDD [0-9]: avail 749G HDD [0-9]: use 69% HDD [0-9]: mounted on /","title":"HDD Usage"},{"location":"system/system_monitor/docs/topics_mem_monitor/","text":"ROS topics: Memory Monitor # Memory Usage # /diagnostics/mem_monitor: Memory Usage [summary] level message OK OK WARN high load ERROR very high load [values] key value (example) Mem: usage 18.99% Mem: total 31G Mem: used 5.9G Mem: free 15G Swap: total 2.0G Swap: used 0B Swap: free 2.0G Total: total 33G Total: used 5.9G Total: free 17G","title":"ROS topics: Memory Monitor"},{"location":"system/system_monitor/docs/topics_mem_monitor/#ros-topics-memory-monitor","text":"","title":"ROS topics: Memory Monitor"},{"location":"system/system_monitor/docs/topics_mem_monitor/#memory-usage","text":"/diagnostics/mem_monitor: Memory Usage [summary] level message OK OK WARN high load ERROR very high load [values] key value (example) Mem: usage 18.99% Mem: total 31G Mem: used 5.9G Mem: free 15G Swap: total 2.0G Swap: used 0B Swap: free 2.0G Total: total 33G Total: used 5.9G Total: free 17G","title":"Memory Usage"},{"location":"system/system_monitor/docs/topics_net_monitor/","text":"ROS topics: Net Monitor # Network Usage # /diagnostics/cpu_monitor: Network Usage [summary] level message OK OK WARN high load ERROR down [values] key value (example) Network [0-9]: status OK / high load / down Network [0-9]: interface name wlp82s0 Network [0-9]: rx_usage 0.00% Network [0-9]: tx_usage 0.00% Network [0-9]: rx_traffic 0.00 MB/s Network [0-9]: tx_traffic 0.00 MB/s Network [0-9]: capacity 400.0 MB/s Network [0-9]: mtu 1500 Network [0-9]: rx_bytes 58455228 Network [0-9]: rx_errors 0 Network [0-9]: tx_bytes 11069136 Network [0-9]: tx_errors 0 Network [0-9]: collisions 0","title":"ROS topics: Net Monitor"},{"location":"system/system_monitor/docs/topics_net_monitor/#ros-topics-net-monitor","text":"","title":"ROS topics: Net Monitor"},{"location":"system/system_monitor/docs/topics_net_monitor/#network-usage","text":"/diagnostics/cpu_monitor: Network Usage [summary] level message OK OK WARN high load ERROR down [values] key value (example) Network [0-9]: status OK / high load / down Network [0-9]: interface name wlp82s0 Network [0-9]: rx_usage 0.00% Network [0-9]: tx_usage 0.00% Network [0-9]: rx_traffic 0.00 MB/s Network [0-9]: tx_traffic 0.00 MB/s Network [0-9]: capacity 400.0 MB/s Network [0-9]: mtu 1500 Network [0-9]: rx_bytes 58455228 Network [0-9]: rx_errors 0 Network [0-9]: tx_bytes 11069136 Network [0-9]: tx_errors 0 Network [0-9]: collisions 0","title":"Network Usage"},{"location":"system/system_monitor/docs/topics_ntp_monitor/","text":"ROS topics: NTP Monitor # NTP Offset # /diagnostics/ntp_monitor: NTP Offset [summary] level message OK OK WARN high ERROR too high [values] key value (example) NTP Offset -0.013181 sec NTP Delay 0.053880 sec","title":"ROS topics: NTP Monitor"},{"location":"system/system_monitor/docs/topics_ntp_monitor/#ros-topics-ntp-monitor","text":"","title":"ROS topics: NTP Monitor"},{"location":"system/system_monitor/docs/topics_ntp_monitor/#ntp-offset","text":"/diagnostics/ntp_monitor: NTP Offset [summary] level message OK OK WARN high ERROR too high [values] key value (example) NTP Offset -0.013181 sec NTP Delay 0.053880 sec","title":"NTP Offset"},{"location":"system/system_monitor/docs/topics_process_monitor/","text":"ROS topics: Process Monitor # Tasks Summary # /diagnostics/process_monitor: Tasks Summary [summary] level message OK OK [values] key value (example) total 409 running 2 sleeping 321 stopped 0 zombie 0 High-load Proc[0-9] # /diagnostics/process_monitor: High-load Proc[0-9] [summary] level message OK OK [values] key value (example) COMMAND /usr/lib/firefox/firefox %CPU 37.5 %MEM 2.1 PID 14062 USER autoware PR 20 NI 0 VIRT 3461152 RES 669052 SHR 481208 S S TIME+ 23:57.49 High-mem Proc[0-9] # /diagnostics/process_monitor: High-mem Proc[0-9] [summary] level message OK OK [values] key value (example) COMMAND /snap/multipass/1784/usr/bin/qemu-system-x86_64 %CPU 0 %MEM 2.5 PID 1565 USER root PR 20 NI 0 VIRT 3722320 RES 812432 SHR 20340 S S TIME+ 0:22.84","title":"ROS topics: Process Monitor"},{"location":"system/system_monitor/docs/topics_process_monitor/#ros-topics-process-monitor","text":"","title":"ROS topics: Process Monitor"},{"location":"system/system_monitor/docs/topics_process_monitor/#tasks-summary","text":"/diagnostics/process_monitor: Tasks Summary [summary] level message OK OK [values] key value (example) total 409 running 2 sleeping 321 stopped 0 zombie 0","title":"Tasks Summary"},{"location":"system/system_monitor/docs/topics_process_monitor/#high-load-proc0-9","text":"/diagnostics/process_monitor: High-load Proc[0-9] [summary] level message OK OK [values] key value (example) COMMAND /usr/lib/firefox/firefox %CPU 37.5 %MEM 2.1 PID 14062 USER autoware PR 20 NI 0 VIRT 3461152 RES 669052 SHR 481208 S S TIME+ 23:57.49","title":"High-load Proc[0-9]"},{"location":"system/system_monitor/docs/topics_process_monitor/#high-mem-proc0-9","text":"/diagnostics/process_monitor: High-mem Proc[0-9] [summary] level message OK OK [values] key value (example) COMMAND /snap/multipass/1784/usr/bin/qemu-system-x86_64 %CPU 0 %MEM 2.5 PID 1565 USER root PR 20 NI 0 VIRT 3722320 RES 812432 SHR 20340 S S TIME+ 0:22.84","title":"High-mem Proc[0-9]"},{"location":"system/topic_state_monitor/Readme/","text":"topic_state_monitor # Purpose # This node monitors input topic for abnormalities such as timeout and low frequency. The result of topic status is published as diagnostics. Inner-workings / Algorithms # The types of topic status and corresponding diagnostic status are following. Topic status Diagnostic status Description OK OK The topic has no abnormalities NotReceived ERROR The topic has not been received yet WarnRate WARN The frequency of the topic is dropped ErrorRate ERROR The frequency of the topic is significantly dropped Timeout ERROR The topic subscription is stopped for a certain time Inputs / Outputs # Input # Name Type Description any name any type Subscribe target topic to monitor Output # Name Type Description /diagnostics diagnostic_msgs/DiagnosticArray Diagnostics outputs Parameters # Node Parameters # Name Type Default Value Description update_rate double 10.0 Timer callback period [Hz] window_size int 10 Window size of target topic for calculating frequency Core Parameters # Name Type Default Value Description topic string - Name of target topic topic_type string - Type of target topic transient_local bool false QoS policy of topic subscription (Transient Local/Volatile) best_effort bool false QoS policy of topic subscription (Best Effort/Reliable) diag_name string - Name used for the diagnostics to publish warn_rate double 0.5 If the topic rate is lower than this value, the topic status becomes WarnRate error_rate double 0.1 If the topic rate is lower than this value, the topic status becomes ErrorRate timeout double 1.0 If the topic subscription is stopped for more than this time [s], the topic status becomes Timeout Assumptions / Known limits # TBD.","title":"topic_state_monitor"},{"location":"system/topic_state_monitor/Readme/#topic_state_monitor","text":"","title":"topic_state_monitor"},{"location":"system/topic_state_monitor/Readme/#purpose","text":"This node monitors input topic for abnormalities such as timeout and low frequency. The result of topic status is published as diagnostics.","title":"Purpose"},{"location":"system/topic_state_monitor/Readme/#inner-workings-algorithms","text":"The types of topic status and corresponding diagnostic status are following. Topic status Diagnostic status Description OK OK The topic has no abnormalities NotReceived ERROR The topic has not been received yet WarnRate WARN The frequency of the topic is dropped ErrorRate ERROR The frequency of the topic is significantly dropped Timeout ERROR The topic subscription is stopped for a certain time","title":"Inner-workings / Algorithms"},{"location":"system/topic_state_monitor/Readme/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"system/topic_state_monitor/Readme/#input","text":"Name Type Description any name any type Subscribe target topic to monitor","title":"Input"},{"location":"system/topic_state_monitor/Readme/#output","text":"Name Type Description /diagnostics diagnostic_msgs/DiagnosticArray Diagnostics outputs","title":"Output"},{"location":"system/topic_state_monitor/Readme/#parameters","text":"","title":"Parameters"},{"location":"system/topic_state_monitor/Readme/#node-parameters","text":"Name Type Default Value Description update_rate double 10.0 Timer callback period [Hz] window_size int 10 Window size of target topic for calculating frequency","title":"Node Parameters"},{"location":"system/topic_state_monitor/Readme/#core-parameters","text":"Name Type Default Value Description topic string - Name of target topic topic_type string - Type of target topic transient_local bool false QoS policy of topic subscription (Transient Local/Volatile) best_effort bool false QoS policy of topic subscription (Best Effort/Reliable) diag_name string - Name used for the diagnostics to publish warn_rate double 0.5 If the topic rate is lower than this value, the topic status becomes WarnRate error_rate double 0.1 If the topic rate is lower than this value, the topic status becomes ErrorRate timeout double 1.0 If the topic subscription is stopped for more than this time [s], the topic status becomes Timeout","title":"Core Parameters"},{"location":"system/topic_state_monitor/Readme/#assumptions-known-limits","text":"TBD.","title":"Assumptions / Known limits"},{"location":"system/velodyne_monitor/Readme/","text":"velodyne_monitor # Purpose # This node monitors the status of Velodyne LiDARs. The result of the status is published as diagnostics. Inner-workings / Algorithms # The status of Velodyne LiDAR can be retrieved from http://[ip_address]/cgi/{info, settings, status, diag}.json . The types of abnormal status and corresponding diagnostics status are following. Abnormal status Diagnostic status No abnormality OK Top board temperature is too cold ERROR Top board temperature is cold WARN Top board temperature is too hot ERROR Top board temperature is hot WARN Bottom board temperature is too cold ERROR Bottom board temperature is cold WARN Bottom board temperature is too hot ERROR Bottom board temperature is hot WARN Rpm(Rotations per minute) of the motor is too low ERROR Rpm(Rotations per minute) of the motor is low WARN Connection error (cannot get Velodyne LiDAR status) ERROR Inputs / Outputs # Input # None Output # Name Type Description /diagnostics diagnostic_msgs/DiagnosticArray Diagnostics outputs Parameters # Node Parameters # Name Type Default Value Description timeout double 0.5 Timeout for HTTP request to get Velodyne LiDAR status [s] Core Parameters # Name Type Default Value Description ip_address string \"192.168.1.201\" IP address of target Velodyne LiDAR temp_cold_warn double -5.0 If the temperature of Velodyne LiDAR is lower than this value, the diagnostics status becomes WARN [\u00b0C] temp_cold_error double -10.0 If the temperature of Velodyne LiDAR is lower than this value, the diagnostics status becomes ERROR [\u00b0C] temp_hot_warn double 75.0 If the temperature of Velodyne LiDAR is higher than this value, the diagnostics status becomes WARN [\u00b0C] temp_hot_error double 80.0 If the temperature of Velodyne LiDAR is higher than this value, the diagnostics status becomes ERROR [\u00b0C] rpm_ratio_warn double 0.80 If the rpm rate of the motor (= current rpm / default rpm) is lower than this value, the diagnostics status becomes WARN rpm_ratio_error double 0.70 If the rpm rate of the motor (= current rpm / default rpm) is lower than this value, the diagnostics status becomes ERROR Assumptions / Known limits # TBD.","title":"velodyne_monitor"},{"location":"system/velodyne_monitor/Readme/#velodyne_monitor","text":"","title":"velodyne_monitor"},{"location":"system/velodyne_monitor/Readme/#purpose","text":"This node monitors the status of Velodyne LiDARs. The result of the status is published as diagnostics.","title":"Purpose"},{"location":"system/velodyne_monitor/Readme/#inner-workings-algorithms","text":"The status of Velodyne LiDAR can be retrieved from http://[ip_address]/cgi/{info, settings, status, diag}.json . The types of abnormal status and corresponding diagnostics status are following. Abnormal status Diagnostic status No abnormality OK Top board temperature is too cold ERROR Top board temperature is cold WARN Top board temperature is too hot ERROR Top board temperature is hot WARN Bottom board temperature is too cold ERROR Bottom board temperature is cold WARN Bottom board temperature is too hot ERROR Bottom board temperature is hot WARN Rpm(Rotations per minute) of the motor is too low ERROR Rpm(Rotations per minute) of the motor is low WARN Connection error (cannot get Velodyne LiDAR status) ERROR","title":"Inner-workings / Algorithms"},{"location":"system/velodyne_monitor/Readme/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"system/velodyne_monitor/Readme/#input","text":"None","title":"Input"},{"location":"system/velodyne_monitor/Readme/#output","text":"Name Type Description /diagnostics diagnostic_msgs/DiagnosticArray Diagnostics outputs","title":"Output"},{"location":"system/velodyne_monitor/Readme/#parameters","text":"","title":"Parameters"},{"location":"system/velodyne_monitor/Readme/#node-parameters","text":"Name Type Default Value Description timeout double 0.5 Timeout for HTTP request to get Velodyne LiDAR status [s]","title":"Node Parameters"},{"location":"system/velodyne_monitor/Readme/#core-parameters","text":"Name Type Default Value Description ip_address string \"192.168.1.201\" IP address of target Velodyne LiDAR temp_cold_warn double -5.0 If the temperature of Velodyne LiDAR is lower than this value, the diagnostics status becomes WARN [\u00b0C] temp_cold_error double -10.0 If the temperature of Velodyne LiDAR is lower than this value, the diagnostics status becomes ERROR [\u00b0C] temp_hot_warn double 75.0 If the temperature of Velodyne LiDAR is higher than this value, the diagnostics status becomes WARN [\u00b0C] temp_hot_error double 80.0 If the temperature of Velodyne LiDAR is higher than this value, the diagnostics status becomes ERROR [\u00b0C] rpm_ratio_warn double 0.80 If the rpm rate of the motor (= current rpm / default rpm) is lower than this value, the diagnostics status becomes WARN rpm_ratio_error double 0.70 If the rpm rate of the motor (= current rpm / default rpm) is lower than this value, the diagnostics status becomes ERROR","title":"Core Parameters"},{"location":"system/velodyne_monitor/Readme/#assumptions-known-limits","text":"TBD.","title":"Assumptions / Known limits"},{"location":"vehicle/external_cmd_converter/","text":"external_cmd_converter # external_cmd_converter is a node that converts desired mechanical input to acceleration and velocity by using accel/brake map. Input topics # Name Type Description ~/in/external_control_cmd autoware_external_api_msgs::msg::ControlCommand target throttle/brake/steering_angle/steering_angle_velocity is necessary to calculate desired control command. ~/input/shift_cmd\" autoware_auto_vehicle_msgs::GearCommand current command of gear. ~/input/emergency_stop autoware_external_api_msgs::msg::Heartbeat emergency heart beat for external command. ~/input/current_gate_mode autoware_control_msgs::msg::GateMode topic for gate mode. ~/input/odometry navigation_msgs::Odometry twist topic in odometry is used. Output topics # Name Type Description ~/out/control_cmd autoware_control_msgs::msg::AckermannControlCommand ackermann control command converted from selected external command Parameters # Parameter Type Description timer_rate double timer's update rate wait_for_first_topic double if time out check is done after receiving first topic control_command_timeout double time out check for control command emergency_stop_timeout double time out check for emergency stop command Limitation # tbd.","title":"external_cmd_converter"},{"location":"vehicle/external_cmd_converter/#external_cmd_converter","text":"external_cmd_converter is a node that converts desired mechanical input to acceleration and velocity by using accel/brake map.","title":"external_cmd_converter"},{"location":"vehicle/external_cmd_converter/#input-topics","text":"Name Type Description ~/in/external_control_cmd autoware_external_api_msgs::msg::ControlCommand target throttle/brake/steering_angle/steering_angle_velocity is necessary to calculate desired control command. ~/input/shift_cmd\" autoware_auto_vehicle_msgs::GearCommand current command of gear. ~/input/emergency_stop autoware_external_api_msgs::msg::Heartbeat emergency heart beat for external command. ~/input/current_gate_mode autoware_control_msgs::msg::GateMode topic for gate mode. ~/input/odometry navigation_msgs::Odometry twist topic in odometry is used.","title":"Input topics"},{"location":"vehicle/external_cmd_converter/#output-topics","text":"Name Type Description ~/out/control_cmd autoware_control_msgs::msg::AckermannControlCommand ackermann control command converted from selected external command","title":"Output topics"},{"location":"vehicle/external_cmd_converter/#parameters","text":"Parameter Type Description timer_rate double timer's update rate wait_for_first_topic double if time out check is done after receiving first topic control_command_timeout double time out check for control command emergency_stop_timeout double time out check for emergency stop command","title":"Parameters"},{"location":"vehicle/external_cmd_converter/#limitation","text":"tbd.","title":"Limitation"},{"location":"vehicle/raw_vehicle_cmd_converter/","text":"raw_vehicle_cmd_converter # raw_vehicle_command_converter is a node that converts desired acceleration and velocity to mechanical input by using feed forward + feed back control (optional). Input topics # Name Type Description ~/input/control_cmd autoware_auto_control_msgs::msg::AckermannControlCommand target velocity/acceleration/steering_angle/steering_angle_velocity is necessary to calculate actuation command. ~/input/steering\" autoware_auto_vehicle_msgs::SteeringReport current status of steering used for steering feed back control ~/input/twist navigation_msgs::Odometry twist topic in odometry is used. Output topics # Name Type Description ~/output/actuation_cmd autoware_vehicle_msgs::msg::ActuationCommandStamped actuation command for vehicle to apply mechanical input Parameters # Parameter Type Description update_rate double timer's update rate th_max_message_delay_sec double threshold time of input messages' maximum delay th_arrived_distance_m double threshold distance to check if vehicle has arrived at the trajectory's endpoint th_stopped_time_sec double threshold time to check if vehicle is stopped th_stopped_velocity_mps double threshold velocity to check if vehicle is stopped Limitation # The current feed back implementation is only applied to steering control.","title":"raw_vehicle_cmd_converter"},{"location":"vehicle/raw_vehicle_cmd_converter/#raw_vehicle_cmd_converter","text":"raw_vehicle_command_converter is a node that converts desired acceleration and velocity to mechanical input by using feed forward + feed back control (optional).","title":"raw_vehicle_cmd_converter"},{"location":"vehicle/raw_vehicle_cmd_converter/#input-topics","text":"Name Type Description ~/input/control_cmd autoware_auto_control_msgs::msg::AckermannControlCommand target velocity/acceleration/steering_angle/steering_angle_velocity is necessary to calculate actuation command. ~/input/steering\" autoware_auto_vehicle_msgs::SteeringReport current status of steering used for steering feed back control ~/input/twist navigation_msgs::Odometry twist topic in odometry is used.","title":"Input topics"},{"location":"vehicle/raw_vehicle_cmd_converter/#output-topics","text":"Name Type Description ~/output/actuation_cmd autoware_vehicle_msgs::msg::ActuationCommandStamped actuation command for vehicle to apply mechanical input","title":"Output topics"},{"location":"vehicle/raw_vehicle_cmd_converter/#parameters","text":"Parameter Type Description update_rate double timer's update rate th_max_message_delay_sec double threshold time of input messages' maximum delay th_arrived_distance_m double threshold distance to check if vehicle has arrived at the trajectory's endpoint th_stopped_time_sec double threshold time to check if vehicle is stopped th_stopped_velocity_mps double threshold velocity to check if vehicle is stopped","title":"Parameters"},{"location":"vehicle/raw_vehicle_cmd_converter/#limitation","text":"The current feed back implementation is only applied to steering control.","title":"Limitation"},{"location":"vehicle/vehicle_info_util/Readme/","text":"Vehicle Info Util # Purpose # This package is to get vehicle info parameters. Description # Assumptions / Known limits # TBD.","title":"Vehicle Info Util"},{"location":"vehicle/vehicle_info_util/Readme/#vehicle-info-util","text":"","title":"Vehicle Info Util"},{"location":"vehicle/vehicle_info_util/Readme/#purpose","text":"This package is to get vehicle info parameters.","title":"Purpose"},{"location":"vehicle/vehicle_info_util/Readme/#description","text":"","title":"Description"},{"location":"vehicle/vehicle_info_util/Readme/#assumptions-known-limits","text":"TBD.","title":"Assumptions / Known limits"}]}